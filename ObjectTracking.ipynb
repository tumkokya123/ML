{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ZZeYHU47f8KnppzGFye0FcrVHhkgCEP2","authorship_tag":"ABX9TyOEaZhifTi3wYRpM+W4oLCC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Main**"],"metadata":{"id":"dAidpoGwligW"}},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9Iy0VEHnaT9","executionInfo":{"status":"ok","timestamp":1749970986038,"user_tz":-330,"elapsed":131027,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"85614f22-3385-4453-a3af-75066e54c26a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.155-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.155-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.155 ultralytics-thop-2.0.14\n"]}]},{"cell_type":"code","source":["!pip install deep_sort_realtime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xe_QqVuQncbK","executionInfo":{"status":"ok","timestamp":1749971015925,"user_tz":-330,"elapsed":5902,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"d3a007df-ee7d-44b6-fd36-6b9a89b8f42c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deep_sort_realtime\n","  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deep_sort_realtime) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from deep_sort_realtime) (1.15.3)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from deep_sort_realtime) (4.11.0.86)\n","Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deep_sort_realtime\n","Successfully installed deep_sort_realtime-1.3.2\n"]}]},{"cell_type":"code","source":["!pip install opencv-python numpy torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"isDguGK4n8wn","executionInfo":{"status":"ok","timestamp":1749971039484,"user_tz":-330,"elapsed":22503,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"fb92575b-c85b-4ce5-a2c0-518d83c03c79"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TG7j5umWlaLO","executionInfo":{"status":"ok","timestamp":1749971054213,"user_tz":-330,"elapsed":5025,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"d72d04a9-bf6a-4cd7-9098-40092f560b52"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/2025/KratiTech/UPSIDA/Smart Surveillance /Netra/Ghaziabad/Cam 1\n"]}],"source":["%cd '/content/drive/MyDrive/2025/KratiTech/UPSIDA/Smart Surveillance /Netra/Ghaziabad/Cam 1'"]},{"cell_type":"code","source":["from ultralytics  import YOLO\n","from deep_sort_realtime.deepsort_tracker import DeepSort\n","import cv2\n","import numpy as np\n","import os"],"metadata":{"id":"zPu7h7hsoD7K","executionInfo":{"status":"ok","timestamp":1749971064458,"user_tz":-330,"elapsed":6073,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cc785d5-55c2-4e07-9ec7-4a4671fecab5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}]},{"cell_type":"code","source":["video_path = 'samples/9th June/5am/combined_video.mp4'\n","output_video_path = \"samples/9th June/5am/tracked_video.mp4\"\n","\n","if not os.path.exists(video_path):\n","    print(f\"Error: Video file not found at {video_path}\")\n","else:\n","    print(f\"Loading video from: {video_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5T7tfuAjodcB","executionInfo":{"status":"ok","timestamp":1749971064780,"user_tz":-330,"elapsed":319,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"93f0d269-30db-4c8d-8922-d7d75fa84862"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading video from: samples/9th June/5am/combined_video.mp4\n"]}]},{"cell_type":"code","source":["print(\"Loading AI model...\")\n","model = YOLO('yolov8n.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10n6sDnfoMOx","executionInfo":{"status":"ok","timestamp":1749971069498,"user_tz":-330,"elapsed":2284,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"66bee468-99f1-4ff3-908b-2788cf563dd6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading AI model...\n"]}]},{"cell_type":"code","source":["cctv_feed = cv2.VideoCapture(video_path)\n","\n","if not cctv_feed.isOpened():\n","  print(\"Error: Could not open video.\")\n","else:\n","  print(\"Video loaded successfully. Processing frames...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"znIMqC9coNVQ","executionInfo":{"status":"ok","timestamp":1749971069950,"user_tz":-330,"elapsed":457,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"679f9399-f805-4bd8-a4e0-4baf309e94fa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Video loaded successfully. Processing frames...\n"]}]},{"cell_type":"code","source":["tracker = DeepSort(max_age=30, n_init=5)"],"metadata":{"id":"y1bI7XOcotqk","executionInfo":{"status":"ok","timestamp":1749971078877,"user_tz":-330,"elapsed":7601,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["frame_width = int(cctv_feed.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cctv_feed.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = int(cctv_feed.get(cv2.CAP_PROP_FPS))\n","\n","print(fps)\n","\n","frame_count = 0\n","frames_to_skip = int(fps)\n","frame_count = 0\n","t=1\n","\n","tracked_summary = {}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYSMplv4xFGD","executionInfo":{"status":"ok","timestamp":1749971079013,"user_tz":-330,"elapsed":132,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"13bbb9a0-ea91-4f89-e6ad-8af8078be95e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["14\n"]}]},{"cell_type":"code","source":["fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","if not out_video.isOpened():\n","  print(f\"Error: Could not initialize VideoWriter for output file: {output_video_path}\")\n","  cctv_feed.release()\n","\n","print(f\"Processing video... Output will be saved to {output_video_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdtMoVtFzCnB","executionInfo":{"status":"ok","timestamp":1749971081009,"user_tz":-330,"elapsed":30,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"3e41d8a1-6be9-4c6c-b6a2-d4edf6734373"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing video... Output will be saved to samples/9th June/5am/tracked_video.mp4\n"]}]},{"cell_type":"code","source":["while cctv_feed.isOpened():\n","\n","    ret, frame = cctv_feed.read()\n","\n","    if not ret:\n","        break  # Stop when video ends\n","\n","    print(\"Logic frame count :\",frame_count)\n","    print(\"Analyzing image for objects...\")\n","    flag = False\n","\n","    # Run YOLO detection on the current frame\n","    results = model(frame)\n","\n","    # Store detected persons\n","    detections = []\n","    class_names = results[0].names\n","    i=1\n","    print(\"Processing detections...\")\n","\n","    for r in results[0].boxes.data:\n","      x1, y1, x2, y2, conf, cls = r\n","\n","      detections.append(([x1, y1, x2, y2], conf,int(cls)))\n","\n","    print(detections)\n","\n","    # Pass detections to DeepSORT tracker\n","    tracked_objects = tracker.update_tracks(detections, frame=frame)\n","\n","    for obj in tracked_objects:\n","      if obj.is_confirmed():\n","          s=\"object\"+str(i)\n","          i+=1\n","\n","          if t not in tracked_summary:\n","              tracked_summary[t] = {}\n","\n","          tracked_summary[t][s] = {\"class\":class_names[obj.det_class],\"track_id\":obj.track_id,\"confidence_score\":obj.det_conf,\"xyxy\":obj.to_tlbr()}\n","          print(f\"Class {class_names[obj.det_class]} : ID {obj.track_id}: Confidence {obj.det_conf} : Bounding Box {obj.to_tlbr()}\")\n","          bbox = obj.to_tlbr()  # Convert bbox to (top-left, bottom-right)\n","          id = obj.track_id\n","          cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n","          cv2.putText(frame, f\"ID {id}\", (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n","\n","    out_video.write(frame)\n","    t+=1\n","    frame_count += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQ2s9TsFpNDw","executionInfo":{"status":"ok","timestamp":1749971229512,"user_tz":-330,"elapsed":145689,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"9559e370-6e7e-4a4f-99d1-4125fe5a2a2a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Logic frame count : 0\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 399.1ms\n","Speed: 8.0ms preprocess, 399.1ms inference, 31.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1722.6960), tensor(946.8572), tensor(1862.5911), tensor(1307.5427)], tensor(0.7159), 0), ([tensor(1852.9315), tensor(974.7105), tensor(1921.6256), tensor(1075.0234)], tensor(0.4063), 0), ([tensor(0.0728), tensor(558.1110), tensor(752.7139), tensor(934.6522)], tensor(0.3463), 6)]\n","Logic frame count : 1\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 boat, 285.9ms\n","Speed: 4.7ms preprocess, 285.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1727.4556), tensor(941.9409), tensor(1858.5703), tensor(1309.0864)], tensor(0.6741), 0), ([tensor(0.6456), tensor(559.2654), tensor(749.8453), tensor(939.2368)], tensor(0.4019), 6), ([tensor(1729.9719), tensor(1154.4902), tensor(1904.4612), tensor(1440.)], tensor(0.3098), 1), ([tensor(1309.2372), tensor(765.6091), tensor(1532.7335), tensor(1031.9547)], tensor(0.2745), 8), ([tensor(1856.3848), tensor(972.6091), tensor(1922.3916), tensor(1076.0544)], tensor(0.2727), 0), ([tensor(1807.3474), tensor(973.8390), tensor(1928.0784), tensor(1179.8734)], tensor(0.2709), 0)]\n","Logic frame count : 2\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 1 boat, 1 umbrella, 301.4ms\n","Speed: 11.1ms preprocess, 301.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1725.7141), tensor(940.2660), tensor(1855.6643), tensor(1309.4948)], tensor(0.7406), 0), ([tensor(0.3920), tensor(557.1018), tensor(746.9131), tensor(939.7594)], tensor(0.3915), 6), ([tensor(1815.4956), tensor(974.1207), tensor(1927.3081), tensor(1175.0200)], tensor(0.2860), 0), ([tensor(1308.9182), tensor(766.5258), tensor(1531.7473), tensor(1031.9683)], tensor(0.2770), 8), ([tensor(1847.6810), tensor(973.7063), tensor(1920.8739), tensor(1084.1787)], tensor(0.2665), 0), ([tensor(1570.2957), tensor(733.8021), tensor(1697.3269), tensor(810.4198)], tensor(0.2617), 25)]\n","Logic frame count : 3\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 317.6ms\n","Speed: 4.5ms preprocess, 317.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1723.4578), tensor(939.2521), tensor(1856.6028), tensor(1310.3474)], tensor(0.7497), 0), ([tensor(0.), tensor(561.3954), tensor(746.9114), tensor(939.1314)], tensor(0.3752), 6), ([tensor(1814.9829), tensor(973.9453), tensor(1928.1548), tensor(1178.8411)], tensor(0.2880), 0), ([tensor(1727.9716), tensor(1149.7760), tensor(1904.1857), tensor(1440.)], tensor(0.2800), 1), ([tensor(1570.0833), tensor(733.4355), tensor(1695.2122), tensor(810.3345)], tensor(0.2646), 25), ([tensor(1851.8881), tensor(973.0079), tensor(1921.2750), tensor(1088.9613)], tensor(0.2613), 0)]\n","Logic frame count : 4\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 umbrella, 135.3ms\n","Speed: 4.5ms preprocess, 135.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1725.8149), tensor(938.6860), tensor(1858.3589), tensor(1308.0759)], tensor(0.7236), 0), ([tensor(0.), tensor(558.4312), tensor(772.0507), tensor(941.4885)], tensor(0.3377), 6), ([tensor(1859.1379), tensor(972.2014), tensor(1921.6213), tensor(1065.5840)], tensor(0.2674), 0), ([tensor(1570.1919), tensor(733.2924), tensor(1694.6250), tensor(810.2499)], tensor(0.2574), 25), ([tensor(1729.1582), tensor(1150.7981), tensor(1902.0286), tensor(1440.)], tensor(0.2562), 1)]\n","Class person : ID 1: Confidence 0.7236363291740417 : Bounding Box [     1722.3      938.46      3585.7      2247.2]\n","Class person : ID 2: Confidence 0.2673659324645996 : Bounding Box [     1859.1       972.4      3775.7        2045]\n","Class train : ID 3: Confidence 0.3377344608306885 : Bounding Box [     3.6177      559.11      761.45      1500.2]\n","Logic frame count : 5\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 bicycles, 1 train, 1 umbrella, 154.4ms\n","Speed: 4.6ms preprocess, 154.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1725.1396), tensor(937.3925), tensor(1861.3667), tensor(1308.9498)], tensor(0.6965), 0), ([tensor(0.), tensor(556.3595), tensor(748.2017), tensor(939.8029)], tensor(0.4099), 6), ([tensor(1728.1194), tensor(1077.6252), tensor(2032.9417), tensor(1440.)], tensor(0.2865), 1), ([tensor(1729.4226), tensor(1151.9047), tensor(1905.8918), tensor(1440.)], tensor(0.2805), 1), ([tensor(1570.3661), tensor(732.9594), tensor(1695.8568), tensor(810.3007)], tensor(0.2564), 25)]\n","Class person : ID 1: Confidence 0.6964578628540039 : Bounding Box [     1723.5      937.35        3587      2246.2]\n","Class bicycle : ID 2: Confidence 0.28645315766334534 : Bounding Box [     1584.1        1048      3946.3      2384.6]\n","Class train : ID 3: Confidence 0.4098939597606659 : Bounding Box [    -1.3403      557.18      755.49      1497.6]\n","Logic frame count : 6\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 142.2ms\n","Speed: 9.8ms preprocess, 142.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1724.3888), tensor(937.6923), tensor(1864.0858), tensor(1308.8092)], tensor(0.7199), 0), ([tensor(0.0846), tensor(556.9532), tensor(742.3984), tensor(940.0956)], tensor(0.3886), 6), ([tensor(1570.2390), tensor(733.9471), tensor(1696.3840), tensor(810.4381)], tensor(0.2709), 25), ([tensor(1729.9696), tensor(1160.0967), tensor(1901.3622), tensor(1440.)], tensor(0.2566), 1)]\n","Class person : ID 1: Confidence 0.7199354767799377 : Bounding Box [     1724.4      937.23      3587.8      2246.1]\n","Class bicycle : ID 2: Confidence None : Bounding Box [     1520.3      1065.2      3987.5      2461.2]\n","Class train : ID 3: Confidence 0.38860443234443665 : Bounding Box [    -4.8588      556.91      751.07      1497.3]\n","Class umbrella : ID 7: Confidence 0.27088820934295654 : Bounding Box [     1569.8      733.66      3266.9        1544]\n","Logic frame count : 7\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 1 umbrella, 135.9ms\n","Speed: 3.2ms preprocess, 135.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1723.6304), tensor(939.5087), tensor(1874.0801), tensor(1308.2563)], tensor(0.7413), 0), ([tensor(0.5433), tensor(558.1079), tensor(747.3138), tensor(941.2474)], tensor(0.3804), 6), ([tensor(1570.3584), tensor(734.6522), tensor(1697.2173), tensor(810.2201)], tensor(0.2823), 25), ([tensor(1305.8147), tensor(791.4365), tensor(1529.5759), tensor(1032.9059)], tensor(0.2706), 8)]\n","Class person : ID 1: Confidence 0.7413342595100403 : Bounding Box [     1727.5      938.49      3591.1      2246.9]\n","Class bicycle : ID 2: Confidence None : Bounding Box [     1456.4      1082.4      4028.7      2537.8]\n","Class train : ID 3: Confidence 0.3803648352622986 : Bounding Box [    -4.2716      557.63      751.66      1498.7]\n","Class umbrella : ID 7: Confidence 0.2823488414287567 : Bounding Box [     1570.4      734.38      3267.2      1544.7]\n","Logic frame count : 8\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 2 boats, 137.9ms\n","Speed: 3.7ms preprocess, 137.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1717.8462), tensor(938.9608), tensor(1877.3135), tensor(1307.0358)], tensor(0.6900), 0), ([tensor(0.6797), tensor(557.6447), tensor(758.0489), tensor(939.6942)], tensor(0.3901), 6), ([tensor(1305.2668), tensor(787.5605), tensor(1530.1284), tensor(1032.2943)], tensor(0.2988), 8), ([tensor(1729.3035), tensor(1015.2258), tensor(2033.9438), tensor(1438.8225)], tensor(0.2703), 8)]\n","Class person : ID 1: Confidence 0.6900205612182617 : Bounding Box [       1726      938.62      3589.3      2246.1]\n","Class bicycle : ID 2: Confidence None : Bounding Box [     1392.5      1099.5      4069.9      2614.5]\n","Class train : ID 3: Confidence 0.39008209109306335 : Bounding Box [  -0.073595      557.59      755.43      1497.9]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1570.6      734.59      3267.4      1544.8]\n","Logic frame count : 9\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 180.7ms\n","Speed: 5.8ms preprocess, 180.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1706.0203), tensor(939.0286), tensor(1876.9551), tensor(1309.3875)], tensor(0.7544), 0), ([tensor(0.4974), tensor(555.4159), tensor(751.7808), tensor(939.1967)], tensor(0.4503), 6), ([tensor(1570.6279), tensor(734.6478), tensor(1697.3862), tensor(810.4774)], tensor(0.2576), 25)]\n","Class person : ID 1: Confidence 0.7544078826904297 : Bounding Box [     1715.9      938.74      3581.8      2247.4]\n","Class bicycle : ID 2: Confidence None : Bounding Box [     1328.6      1116.7      4111.1      2691.1]\n","Class train : ID 3: Confidence 0.45028337836265564 : Bounding Box [   -0.47398      556.08       754.3      1495.7]\n","Class umbrella : ID 7: Confidence 0.2575945258140564 : Bounding Box [     1570.7      734.67      3267.9      1545.1]\n","Logic frame count : 10\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 226.3ms\n","Speed: 4.3ms preprocess, 226.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1700.8601), tensor(938.8569), tensor(1873.9941), tensor(1295.2849)], tensor(0.6733), 0), ([tensor(0.2794), tensor(556.3798), tensor(742.6006), tensor(938.4316)], tensor(0.4800), 6), ([tensor(1570.5674), tensor(734.4719), tensor(1697.5229), tensor(810.4819)], tensor(0.2706), 25), ([tensor(1727.5591), tensor(1158.5591), tensor(1902.1631), tensor(1440.)], tensor(0.2581), 1)]\n","Class person : ID 1: Confidence 0.673297107219696 : Bounding Box [     1713.6      938.69      3568.7      2238.4]\n","Class bicycle : ID 2: Confidence None : Bounding Box [     1264.7      1133.8      4152.3      2767.7]\n","Class train : ID 3: Confidence 0.4799640476703644 : Bounding Box [    -3.2664      556.17      750.03        1495]\n","Class umbrella : ID 7: Confidence 0.27060800790786743 : Bounding Box [     1570.7      734.59        3268      1545.1]\n","Logic frame count : 11\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 boat, 1 umbrella, 213.7ms\n","Speed: 5.5ms preprocess, 213.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1694.9402), tensor(938.5879), tensor(1862.7625), tensor(1294.8445)], tensor(0.7289), 0), ([tensor(0.2396), tensor(557.3177), tensor(749.0467), tensor(939.4062)], tensor(0.4585), 6), ([tensor(1727.2271), tensor(1159.5339), tensor(1906.2373), tensor(1440.)], tensor(0.2823), 1), ([tensor(1570.4604), tensor(734.5504), tensor(1697.9233), tensor(810.4669)], tensor(0.2652), 25), ([tensor(1300.3125), tensor(780.2010), tensor(1529.0728), tensor(1031.9337)], tensor(0.2546), 8)]\n","Class person : ID 1: Confidence 0.7289037108421326 : Bounding Box [       1705      938.51      3556.1      2234.5]\n","Class bicycle : ID 2: Confidence None : Bounding Box [     1200.8        1151      4193.5      2844.3]\n","Class train : ID 3: Confidence 0.4585087299346924 : Bounding Box [    -2.4093      556.85       750.8      1496.1]\n","Class umbrella : ID 7: Confidence 0.2651703655719757 : Bounding Box [     1570.8      734.61      3268.1      1545.1]\n","Logic frame count : 12\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 230.1ms\n","Speed: 6.4ms preprocess, 230.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1694.9834), tensor(938.8909), tensor(1856.3088), tensor(1295.0725)], tensor(0.7277), 0), ([tensor(0.), tensor(559.0687), tensor(738.2411), tensor(941.2889)], tensor(0.4591), 6), ([tensor(1854.3752), tensor(973.4513), tensor(1923.2273), tensor(1089.8422)], tensor(0.2688), 0)]\n","Class person : ID 1: Confidence 0.7277493476867676 : Bounding Box [     1699.6      938.65      3549.6      2233.5]\n","Class person : ID 2: Confidence 0.2687870264053345 : Bounding Box [       1801      987.61        3812      2125.7]\n","Class train : ID 3: Confidence 0.45909008383750916 : Bounding Box [     -5.732      558.28      747.23      1498.9]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1570.9      734.73      3268.3      1545.2]\n","Logic frame count : 13\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 217.5ms\n","Speed: 5.3ms preprocess, 217.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1696.5160), tensor(938.5828), tensor(1860.9034), tensor(1296.6233)], tensor(0.7026), 0), ([tensor(0.), tensor(556.0919), tensor(739.4208), tensor(942.1940)], tensor(0.4463), 6), ([tensor(1822.3569), tensor(973.9567), tensor(1928.0566), tensor(1179.3185)], tensor(0.2598), 0)]\n","Class person : ID 1: Confidence 0.702575147151947 : Bounding Box [     1699.2      938.52      3550.9      2234.1]\n","Class person : ID 2: Confidence 0.2598242461681366 : Bounding Box [     1762.7      977.27        3818      2149.7]\n","Class train : ID 3: Confidence 0.44628849625587463 : Bounding Box [    -6.4424      556.84      746.16      1498.6]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.1      734.86      3268.5      1545.4]\n","Logic frame count : 14\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 218.7ms\n","Speed: 5.0ms preprocess, 218.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1705.2197), tensor(939.5129), tensor(1852.9944), tensor(1299.7236)], tensor(0.7157), 0), ([tensor(0.1274), tensor(562.6268), tensor(736.9705), tensor(940.2249)], tensor(0.4419), 6), ([tensor(1735.8782), tensor(939.0844), tensor(1861.0110), tensor(1272.1147)], tensor(0.3158), 0), ([tensor(1819.0007), tensor(973.1812), tensor(1928.8171), tensor(1177.7405)], tensor(0.2779), 0)]\n","Class person : ID 1: Confidence 0.715709388256073 : Bounding Box [     1701.2       939.1      3555.8        2237]\n","Class person : ID 2: Confidence 0.2779090106487274 : Bounding Box [     1757.7      974.72      3811.6      2154.4]\n","Class train : ID 3: Confidence 0.44194725155830383 : Bounding Box [    -6.4988      560.64      744.14      1501.5]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.2      734.98      3268.7      1545.5]\n","Logic frame count : 15\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 212.0ms\n","Speed: 4.3ms preprocess, 212.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1707.3613), tensor(941.3619), tensor(1857.5137), tensor(1301.4225)], tensor(0.7408), 0), ([tensor(0.), tensor(560.8209), tensor(741.2489), tensor(941.9835)], tensor(0.4067), 6), ([tensor(1816.0503), tensor(973.0474), tensor(1929.0085), tensor(1181.4260)], tensor(0.2836), 0)]\n","Class person : ID 1: Confidence 0.7407607436180115 : Bounding Box [     1704.2      940.55      3561.5      2240.5]\n","Class person : ID 2: Confidence 0.28363633155822754 : Bounding Box [     1756.3      973.67      3805.2      2158.1]\n","Class train : ID 3: Confidence 0.4067404568195343 : Bounding Box [    -5.4017      560.87      744.98      1502.5]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.4      735.11      3268.9      1545.7]\n","Logic frame count : 16\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 213.7ms\n","Speed: 4.3ms preprocess, 213.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1706.1702), tensor(941.0933), tensor(1843.3582), tensor(1301.9526)], tensor(0.7744), 0), ([tensor(0.0028), tensor(561.6135), tensor(740.2905), tensor(941.4807)], tensor(0.3976), 6), ([tensor(1838.1693), tensor(973.2180), tensor(1923.8903), tensor(1094.8079)], tensor(0.3158), 0), ([tensor(1824.5586), tensor(973.1904), tensor(1929.3484), tensor(1170.1716)], tensor(0.2709), 0)]\n","Class person : ID 1: Confidence 0.7744251489639282 : Bounding Box [     1700.3      940.92      3557.8      2242.1]\n","Class person : ID 2: Confidence 0.27085062861442566 : Bounding Box [     1771.1      973.36      3799.5      2151.5]\n","Class train : ID 3: Confidence 0.39761194586753845 : Bounding Box [    -4.8705      561.47       744.6      1503.1]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.5      735.24        3269      1545.8]\n","Logic frame count : 17\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 220.2ms\n","Speed: 4.3ms preprocess, 220.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1703.0356), tensor(943.6245), tensor(1848.7214), tensor(1301.0222)], tensor(0.7101), 0), ([tensor(0.0171), tensor(562.2750), tensor(741.6907), tensor(942.2616)], tensor(0.3809), 6), ([tensor(1840.9456), tensor(972.6494), tensor(1923.9827), tensor(1094.3022)], tensor(0.3165), 0), ([tensor(1825.2426), tensor(972.7228), tensor(1929.7611), tensor(1166.8634)], tensor(0.2985), 0)]\n","Class person : ID 1: Confidence 0.7101247310638428 : Bounding Box [     1699.1      942.74      3555.6      2243.7]\n","Class person : ID 2: Confidence 0.29845961928367615 : Bounding Box [     1781.9      972.93      3793.6        2146]\n","Class train : ID 3: Confidence 0.38086867332458496 : Bounding Box [     -4.174      562.12      744.94      1504.2]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.7      735.36      3269.2      1545.9]\n","Logic frame count : 18\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 143.2ms\n","Speed: 4.0ms preprocess, 143.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1705.3655), tensor(942.7741), tensor(1849.9436), tensor(1301.1970)], tensor(0.7453), 0), ([tensor(0.0696), tensor(563.1779), tensor(742.5283), tensor(941.6998)], tensor(0.3547), 6), ([tensor(1825.4631), tensor(972.8689), tensor(1929.5906), tensor(1167.6804)], tensor(0.3342), 0), ([tensor(1842.5056), tensor(972.1848), tensor(1924.2092), tensor(1096.7139)], tensor(0.2959), 0), ([tensor(1730.0776), tensor(1162.8070), tensor(1891.8030), tensor(1440.)], tensor(0.2554), 1)]\n","Class person : ID 1: Confidence 0.7453113198280334 : Bounding Box [     1700.8      942.85      3556.8      2243.9]\n","Class person : ID 2: Confidence 0.3342159688472748 : Bounding Box [       1788      972.87      3789.7      2144.3]\n","Class train : ID 3: Confidence 0.35470372438430786 : Bounding Box [    -3.2887      562.95       745.1      1504.9]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.8      735.49      3269.4      1546.1]\n","Logic frame count : 19\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 139.6ms\n","Speed: 4.4ms preprocess, 139.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1705.5184), tensor(944.2406), tensor(1852.4791), tensor(1302.5365)], tensor(0.7081), 0), ([tensor(0.2527), tensor(565.5317), tensor(740.5280), tensor(942.1401)], tensor(0.3619), 6), ([tensor(1729.2932), tensor(1163.1279), tensor(1898.8760), tensor(1440.)], tensor(0.3253), 1), ([tensor(1822.4314), tensor(972.2682), tensor(1929.1545), tensor(1171.9303)], tensor(0.3161), 0)]\n","Class person : ID 1: Confidence 0.7080864310264587 : Bounding Box [     1701.9      943.86      3558.7      2245.8]\n","Class person : ID 2: Confidence 0.31611499190330505 : Bounding Box [     1788.3      972.44      3786.1      2145.9]\n","Class train : ID 3: Confidence 0.36187463998794556 : Bounding Box [    -3.3346      564.81      744.48      1506.9]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.9      735.61      3269.6      1546.2]\n","Logic frame count : 20\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 137.2ms\n","Speed: 4.2ms preprocess, 137.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1700.3339), tensor(944.0195), tensor(1859.1610), tensor(1301.6279)], tensor(0.7311), 0), ([tensor(0.), tensor(559.0542), tensor(740.0328), tensor(942.2891)], tensor(0.3701), 6), ([tensor(1815.8495), tensor(972.3039), tensor(1928.7157), tensor(1173.6477)], tensor(0.2604), 0)]\n","Class person : ID 1: Confidence 0.7310882806777954 : Bounding Box [     1701.4      944.08        3558      2245.8]\n","Class person : ID 2: Confidence 0.26040175557136536 : Bounding Box [     1785.1      972.31        3779      2147.4]\n","Class train : ID 3: Confidence 0.3701050877571106 : Bounding Box [    -3.4839       561.2      743.74      1503.5]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1572.1      735.74      3269.8      1546.4]\n","Logic frame count : 21\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 141.5ms\n","Speed: 4.4ms preprocess, 141.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1698.9104), tensor(945.3032), tensor(1867.6819), tensor(1298.)], tensor(0.7772), 0), ([tensor(0.), tensor(559.4989), tensor(739.8304), tensor(942.1522)], tensor(0.3752), 6), ([tensor(1844.7935), tensor(971.5183), tensor(1923.2751), tensor(1089.9626)], tensor(0.2561), 0)]\n","Class person : ID 1: Confidence 0.7772294282913208 : Bounding Box [     1704.1         945      3558.7      2244.2]\n","Class person : ID 2: Confidence 0.2560904622077942 : Bounding Box [     1843.7      971.75      3751.5      2091.8]\n","Class train : ID 3: Confidence 0.37515994906425476 : Bounding Box [    -3.3591      560.12      743.16      1502.4]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1572.2      735.86      3269.9      1546.5]\n","Logic frame count : 22\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 147.7ms\n","Speed: 4.3ms preprocess, 147.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1697.2244), tensor(944.9685), tensor(1868.1174), tensor(1296.4734)], tensor(0.7101), 0), ([tensor(0.), tensor(556.0786), tensor(748.3591), tensor(943.8398)], tensor(0.4066), 6), ([tensor(1737.0383), tensor(943.8186), tensor(1874.4319), tensor(1182.9692)], tensor(0.2557), 0)]\n","Class person : ID 1: Confidence 0.710101842880249 : Bounding Box [     1704.4      945.11      3557.8      2242.4]\n","Class person : ID 2: Confidence 0.2557491660118103 : Bounding Box [     1731.6       952.8      3696.9      2114.5]\n","Class train : ID 3: Confidence 0.40658262372016907 : Bounding Box [    -1.0768      557.44      746.37      1500.8]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1572.4      735.99      3270.1      1546.7]\n","Logic frame count : 23\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 147.8ms\n","Speed: 4.4ms preprocess, 147.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1691.9349), tensor(949.4877), tensor(1869.6466), tensor(1298.7515)], tensor(0.6309), 0), ([tensor(0.8881), tensor(560.4292), tensor(737.1058), tensor(942.1672)], tensor(0.4812), 6), ([tensor(1693.7095), tensor(1017.1699), tensor(1803.0049), tensor(1295.8669)], tensor(0.2688), 0), ([tensor(1307.8569), tensor(812.1338), tensor(1519.1616), tensor(1025.7301)], tensor(0.2543), 8)]\n","Class person : ID 1: Confidence 0.2688494622707367 : Bounding Box [     1683.5      992.92      3530.8      2289.1]\n","Class person : ID 2: Confidence 0.6308862566947937 : Bounding Box [     1608.8      949.78      3697.7      2202.5]\n","Class train : ID 3: Confidence 0.48115023970603943 : Bounding Box [    -2.4494      559.31      743.56        1502]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1572.5      736.11      3270.3      1546.8]\n","Logic frame count : 24\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 1 umbrella, 146.5ms\n","Speed: 4.4ms preprocess, 146.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1684.8558), tensor(947.9426), tensor(1866.1268), tensor(1307.2332)], tensor(0.7589), 0), ([tensor(0.7575), tensor(560.2725), tensor(742.3145), tensor(944.7268)], tensor(0.4808), 6), ([tensor(1569.9512), tensor(731.8055), tensor(1700.6187), tensor(810.5499)], tensor(0.3344), 25), ([tensor(1308.3635), tensor(811.0449), tensor(1519.1653), tensor(1024.5250)], tensor(0.2522), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1680.7      998.57      3527.2      2294.1]\n","Class person : ID 2: Confidence 0.7588576674461365 : Bounding Box [     1562.3      947.69      3687.4      2240.1]\n","Class train : ID 3: Confidence 0.4808133840560913 : Bounding Box [    -1.9157      559.93      744.72        1504]\n","Class umbrella : ID 7: Confidence 0.3343971073627472 : Bounding Box [     1571.3      731.88      3269.3      1542.4]\n","Logic frame count : 25\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 2 boats, 1 umbrella, 137.6ms\n","Speed: 7.0ms preprocess, 137.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1687.3992), tensor(951.8366), tensor(1860.3840), tensor(1310.4148)], tensor(0.7616), 0), ([tensor(1.5349), tensor(559.8956), tensor(730.8049), tensor(943.4404)], tensor(0.5363), 6), ([tensor(1307.8660), tensor(813.9296), tensor(1518.9285), tensor(1023.7666)], tensor(0.3622), 8), ([tensor(1569.7976), tensor(730.7150), tensor(1700.4473), tensor(810.2489)], tensor(0.3321), 25), ([tensor(1823.6392), tensor(973.4808), tensor(1929.8203), tensor(1174.8019)], tensor(0.2639), 0), ([tensor(1729.5896), tensor(1014.3054), tensor(2034.2268), tensor(1440.)], tensor(0.2516), 8)]\n","Class person : ID 1: Confidence 0.26386159658432007 : Bounding Box [       1878      980.23      3616.6      2181.4]\n","Class person : ID 2: Confidence 0.7616433501243591 : Bounding Box [     1551.4      949.49      3676.6      2259.1]\n","Class train : ID 3: Confidence 0.5362550616264343 : Bounding Box [    -4.0313      559.91       740.9      1503.7]\n","Class umbrella : ID 7: Confidence 0.3321009576320648 : Bounding Box [     1571.1      731.03      3269.2      1541.4]\n","Logic frame count : 26\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 1 boat, 1 umbrella, 140.7ms\n","Speed: 3.9ms preprocess, 140.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1683.4247), tensor(953.6941), tensor(1851.7279), tensor(1315.3542)], tensor(0.7555), 0), ([tensor(0.9859), tensor(562.0692), tensor(731.9774), tensor(941.6954)], tensor(0.5499), 6), ([tensor(1844.1443), tensor(973.0483), tensor(1924.9065), tensor(1092.9243)], tensor(0.3515), 0), ([tensor(1569.7249), tensor(730.8803), tensor(1700.8660), tensor(810.1114)], tensor(0.3197), 25), ([tensor(1307.2761), tensor(798.3805), tensor(1526.7805), tensor(1032.3436)], tensor(0.2855), 8), ([tensor(1831.5219), tensor(974.2917), tensor(1929.7631), tensor(1144.3230)], tensor(0.2550), 0)]\n","Class person : ID 1: Confidence 0.25503188371658325 : Bounding Box [       1933      977.02      3637.5      2135.3]\n","Class person : ID 2: Confidence 0.7554883360862732 : Bounding Box [     1548.5       951.5      3661.7      2270.5]\n","Class train : ID 3: Confidence 0.5499435067176819 : Bounding Box [    -4.0801      561.34      738.77      1503.8]\n","Class umbrella : ID 7: Confidence 0.3197128474712372 : Bounding Box [     1571.1      730.85      3269.3        1541]\n","Logic frame count : 27\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 1 boat, 162.8ms\n","Speed: 4.3ms preprocess, 162.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1684.8232), tensor(949.3995), tensor(1846.1753), tensor(1313.2318)], tensor(0.7932), 0), ([tensor(1.1639), tensor(569.1349), tensor(734.2057), tensor(943.6588)], tensor(0.5109), 6), ([tensor(1841.8008), tensor(973.9913), tensor(1924.7583), tensor(1098.1334)], tensor(0.4016), 0), ([tensor(1305.3096), tensor(788.8126), tensor(1529.4121), tensor(1032.1200)], tensor(0.3555), 8), ([tensor(1827.9995), tensor(974.2332), tensor(1928.3704), tensor(1129.2930)], tensor(0.3003), 0)]\n","Class person : ID 1: Confidence 0.30027326941490173 : Bounding Box [     1947.2      975.88      3642.6      2109.9]\n","Class person : ID 2: Confidence 0.7932303547859192 : Bounding Box [     1555.9      949.55      3646.8      2270.1]\n","Class train : ID 3: Confidence 0.5108982920646667 : Bounding Box [    -3.4877      566.55      739.09      1509.8]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.2      730.64      3269.3      1540.8]\n","Class boat : ID 16: Confidence 0.3554593026638031 : Bounding Box [     1305.8      791.41      2834.5      1823.5]\n","Logic frame count : 28\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 2 boats, 165.0ms\n","Speed: 5.6ms preprocess, 165.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1694.8857), tensor(946.9874), tensor(1837.6011), tensor(1317.1368)], tensor(0.7522), 0), ([tensor(1.5145), tensor(569.1472), tensor(734.1981), tensor(943.3085)], tensor(0.5175), 6), ([tensor(1831.3574), tensor(974.0949), tensor(1925.7148), tensor(1108.2639)], tensor(0.3353), 0), ([tensor(1727.5679), tensor(1020.5659), tensor(2034.4600), tensor(1440.)], tensor(0.3107), 8), ([tensor(1302.5613), tensor(789.8528), tensor(1529.2668), tensor(1031.9836)], tensor(0.3029), 8), ([tensor(1823.1997), tensor(974.8152), tensor(1928.1018), tensor(1155.1013)], tensor(0.2564), 0)]\n","Class person : ID 1: Confidence 0.3353298306465149 : Bounding Box [     1954.9      975.29      3643.4      2086.8]\n","Class person : ID 2: Confidence 0.7522002458572388 : Bounding Box [     1568.7      947.29      3639.9      2270.5]\n","Class train : ID 3: Confidence 0.517519474029541 : Bounding Box [    -2.6995      568.51      739.14      1511.9]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1571.2      730.42      3269.3      1540.5]\n","Class boat : ID 16: Confidence 0.3028715252876282 : Bounding Box [     1303.5      788.75      2832.9      1821.3]\n","Logic frame count : 29\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 1 umbrella, 148.1ms\n","Speed: 4.6ms preprocess, 148.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1696.4141), tensor(953.0747), tensor(1828.9746), tensor(1315.5981)], tensor(0.7785), 0), ([tensor(1.1398), tensor(567.6968), tensor(733.9920), tensor(943.3435)], tensor(0.4916), 6), ([tensor(1569.7268), tensor(731.7904), tensor(1696.8865), tensor(810.0067)], tensor(0.3409), 25), ([tensor(1832.3162), tensor(974.2678), tensor(1925.1511), tensor(1109.7615)], tensor(0.3123), 0), ([tensor(1302.6704), tensor(797.0679), tensor(1528.5732), tensor(1032.5322)], tensor(0.2803), 8)]\n","Class person : ID 1: Confidence 0.31225213408470154 : Bounding Box [     1949.6      975.12      3651.6      2080.1]\n","Class person : ID 2: Confidence 0.7785013318061829 : Bounding Box [       1580      950.47      3628.3      2273.1]\n","Class train : ID 3: Confidence 0.49160200357437134 : Bounding Box [    -2.4905      568.26      738.64      1511.7]\n","Class umbrella : ID 7: Confidence 0.3409014046192169 : Bounding Box [     1569.6      731.55      3267.4      1541.6]\n","Class boat : ID 16: Confidence 0.28029438853263855 : Bounding Box [     1302.4      793.12      2832.4      1826.1]\n","Logic frame count : 30\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 1 umbrella, 139.7ms\n","Speed: 5.1ms preprocess, 139.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1696.3563), tensor(948.2266), tensor(1822.8905), tensor(1314.5022)], tensor(0.8182), 0), ([tensor(1.4682), tensor(566.7335), tensor(734.9531), tensor(942.0854)], tensor(0.5349), 6), ([tensor(1569.7739), tensor(731.8579), tensor(1692.7744), tensor(810.0916)], tensor(0.3506), 25), ([tensor(1835.0110), tensor(975.1997), tensor(1924.8596), tensor(1109.6946)], tensor(0.3282), 0), ([tensor(1302.7948), tensor(795.1982), tensor(1530.5194), tensor(1032.2888)], tensor(0.3135), 8)]\n","Class person : ID 1: Confidence 0.328205943107605 : Bounding Box [     1942.2      975.62      3662.4      2078.8]\n","Class person : ID 2: Confidence 0.8181535601615906 : Bounding Box [     1589.7      948.57      3615.4      2269.8]\n","Class train : ID 3: Confidence 0.5348625779151917 : Bounding Box [    -1.4343       567.5      738.56      1510.1]\n","Class umbrella : ID 7: Confidence 0.3505733609199524 : Bounding Box [     1568.3      731.74      3265.4      1541.8]\n","Class boat : ID 16: Confidence 0.3134942650794983 : Bounding Box [     1302.8      793.61      2832.8      1826.5]\n","Logic frame count : 31\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 1 umbrella, 146.6ms\n","Speed: 3.7ms preprocess, 146.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1691.4486), tensor(950.7535), tensor(1818.4493), tensor(1314.6224)], tensor(0.8255), 0), ([tensor(1.8261), tensor(566.3646), tensor(733.3945), tensor(941.5992)], tensor(0.5833), 6), ([tensor(1569.8660), tensor(731.8527), tensor(1691.0132), tensor(809.9054)], tensor(0.3991), 25), ([tensor(1833.5120), tensor(973.3701), tensor(1926.5056), tensor(1120.8357)], tensor(0.3442), 0), ([tensor(1303.4834), tensor(796.0634), tensor(1530.2119), tensor(1031.3480)], tensor(0.3169), 8)]\n","Class person : ID 1: Confidence 0.34419605135917664 : Bounding Box [     1927.6      974.53      3676.5      2085.1]\n","Class person : ID 2: Confidence 0.8255194425582886 : Bounding Box [     1594.9      949.55      3600.4      2269.9]\n","Class train : ID 3: Confidence 0.5833113789558411 : Bounding Box [   -0.99241      566.94      737.95      1508.9]\n","Class umbrella : ID 7: Confidence 0.39914247393608093 : Bounding Box [     1567.7       731.8      3263.8      1541.7]\n","Class boat : ID 16: Confidence 0.3169173300266266 : Bounding Box [     1303.8      794.51      2832.9      1826.7]\n","Logic frame count : 32\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 umbrella, 144.4ms\n","Speed: 6.8ms preprocess, 144.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1686.7207), tensor(951.3522), tensor(1813.4624), tensor(1316.0817)], tensor(0.8334), 0), ([tensor(1.7731), tensor(565.9410), tensor(733.5497), tensor(942.0267)], tensor(0.5498), 6), ([tensor(1569.7859), tensor(731.6974), tensor(1690.6858), tensor(809.9561)], tensor(0.3909), 25), ([tensor(1824.6953), tensor(974.1033), tensor(1927.2781), tensor(1168.1426)], tensor(0.3234), 0)]\n","Class person : ID 1: Confidence 0.32342901825904846 : Bounding Box [       1890      974.57      3701.9      2119.9]\n","Class person : ID 2: Confidence 0.8334298133850098 : Bounding Box [     1597.4      950.37      3585.5        2271]\n","Class train : ID 3: Confidence 0.5497671365737915 : Bounding Box [   -0.75899      566.43       737.7      1508.4]\n","Class umbrella : ID 7: Confidence 0.39090853929519653 : Bounding Box [     1567.5      731.72        3263      1541.7]\n","Class boat : ID 16: Confidence None : Bounding Box [     1303.2      792.61      2833.5      1825.6]\n","Logic frame count : 33\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 1 umbrella, 143.9ms\n","Speed: 4.9ms preprocess, 143.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1680.7693), tensor(953.9683), tensor(1810.3826), tensor(1319.5508)], tensor(0.8434), 0), ([tensor(1.8464), tensor(567.0490), tensor(733.5201), tensor(942.0612)], tensor(0.5392), 6), ([tensor(1305.1097), tensor(800.6263), tensor(1530.5179), tensor(1031.6014)], tensor(0.4575), 8), ([tensor(1569.7974), tensor(731.1327), tensor(1689.9634), tensor(810.0819)], tensor(0.3430), 25), ([tensor(1813.0176), tensor(975.3950), tensor(1927.3535), tensor(1181.8826)], tensor(0.3127), 0)]\n","Class person : ID 1: Confidence 0.3127368986606598 : Bounding Box [     1859.5      975.41      3712.1        2143]\n","Class person : ID 2: Confidence 0.8434321880340576 : Bounding Box [     1597.4      952.44      3571.7      2275.1]\n","Class train : ID 3: Confidence 0.5391820073127747 : Bounding Box [   -0.49618      566.96      737.52        1509]\n","Class umbrella : ID 7: Confidence 0.34299758076667786 : Bounding Box [     1567.3      731.32      3262.3      1541.3]\n","Class boat : ID 16: Confidence 0.4575153887271881 : Bounding Box [     1305.4       798.6      2834.5      1830.7]\n","Logic frame count : 34\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 1 umbrella, 144.2ms\n","Speed: 4.8ms preprocess, 144.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1676.7111), tensor(955.9828), tensor(1803.0348), tensor(1315.7690)], tensor(0.8141), 0), ([tensor(1.2128), tensor(567.0465), tensor(733.8074), tensor(942.6154)], tensor(0.5232), 6), ([tensor(1305.9584), tensor(799.0353), tensor(1533.6705), tensor(1031.4340)], tensor(0.4538), 8), ([tensor(1814.4990), tensor(975.6920), tensor(1927.5352), tensor(1185.3501)], tensor(0.2886), 0), ([tensor(1569.9067), tensor(730.6820), tensor(1689.4526), tensor(810.2074)], tensor(0.2598), 25)]\n","Class person : ID 1: Confidence 0.28864750266075134 : Bounding Box [     1845.5      975.89      3719.9      2154.4]\n","Class person : ID 2: Confidence 0.8140861988067627 : Bounding Box [     1599.2      954.58      3555.2      2275.3]\n","Class train : ID 3: Confidence 0.5231857895851135 : Bounding Box [   -0.72704      567.14      737.14      1509.5]\n","Class umbrella : ID 7: Confidence 0.2597847282886505 : Bounding Box [     1567.3      730.87      3261.8        1541]\n","Class boat : ID 16: Confidence 0.4537612497806549 : Bounding Box [     1307.4      798.63      2836.6      1830.4]\n","Logic frame count : 35\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 1 umbrella, 143.7ms\n","Speed: 4.6ms preprocess, 143.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1670.5645), tensor(959.6841), tensor(1796.3311), tensor(1317.7434)], tensor(0.8448), 0), ([tensor(1.2198), tensor(567.3143), tensor(734.1467), tensor(942.5438)], tensor(0.5067), 6), ([tensor(1308.0354), tensor(803.0614), tensor(1534.6707), tensor(1030.8309)], tensor(0.5014), 8), ([tensor(1817.2523), tensor(975.7894), tensor(1926.8925), tensor(1182.5702)], tensor(0.3056), 0), ([tensor(1569.9607), tensor(730.6886), tensor(1690.0720), tensor(810.2589)], tensor(0.2916), 25)]\n","Class person : ID 1: Confidence 0.3056165874004364 : Bounding Box [     1841.6       976.1      3724.2        2157]\n","Class person : ID 2: Confidence 0.8447884917259216 : Bounding Box [     1597.8      957.85      3539.2      2278.8]\n","Class train : ID 3: Confidence 0.5066836476325989 : Bounding Box [   -0.57584      567.37      737.01      1509.9]\n","Class umbrella : ID 7: Confidence 0.2915691137313843 : Bounding Box [     1567.6      730.71      3261.8      1540.9]\n","Class boat : ID 16: Confidence 0.5013687610626221 : Bounding Box [     1309.9      801.35      2838.9      1832.7]\n","Logic frame count : 36\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 boat, 145.1ms\n","Speed: 5.3ms preprocess, 145.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1667.3662), tensor(960.0800), tensor(1792.0981), tensor(1317.4908)], tensor(0.8417), 0), ([tensor(1.2732), tensor(566.3971), tensor(735.8727), tensor(942.1799)], tensor(0.4872), 6), ([tensor(1306.0311), tensor(801.1824), tensor(1532.7760), tensor(1032.3745)], tensor(0.4195), 8), ([tensor(1826.8425), tensor(975.1589), tensor(1925.5242), tensor(1119.2498)], tensor(0.2781), 0)]\n","Class person : ID 1: Confidence 0.27814388275146484 : Bounding Box [     1872.9      975.74      3704.4      2115.9]\n","Class person : ID 2: Confidence 0.8416656255722046 : Bounding Box [     1598.5      959.36      3525.7        2280]\n","Class train : ID 3: Confidence 0.4872233271598816 : Bounding Box [    0.18713      566.84      737.47      1509.1]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1567.3      730.58      3261.5      1540.8]\n","Class boat : ID 16: Confidence 0.41947516798973083 : Bounding Box [     1308.1      801.13      2838.5      1833.3]\n","Logic frame count : 37\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 1 boat, 160.5ms\n","Speed: 3.4ms preprocess, 160.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1667.4124), tensor(962.1614), tensor(1788.9260), tensor(1318.5959)], tensor(0.8367), 0), ([tensor(1.2814), tensor(567.3986), tensor(735.1012), tensor(942.0118)], tensor(0.4830), 6), ([tensor(1306.1028), tensor(798.3351), tensor(1532.3020), tensor(1032.1888)], tensor(0.3985), 8), ([tensor(1819.6282), tensor(976.0482), tensor(1927.7834), tensor(1161.5570)], tensor(0.3129), 0), ([tensor(1826.2583), tensor(974.7594), tensor(1926.1516), tensor(1115.6259)], tensor(0.3047), 0)]\n","Class person : ID 1: Confidence 0.3046521544456482 : Bounding Box [     1882.7      975.29      3698.2      2097.5]\n","Class person : ID 2: Confidence 0.8366967439651489 : Bounding Box [     1601.5       961.3      3516.4      2282.3]\n","Class train : ID 3: Confidence 0.48303160071372986 : Bounding Box [    0.34067      567.29      737.29      1509.4]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1566.9      730.46      3261.2      1540.7]\n","Class boat : ID 16: Confidence 0.39851754903793335 : Bounding Box [     1307.4      799.17      2838.2      1831.5]\n","Logic frame count : 38\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 1 boat, 141.2ms\n","Speed: 4.2ms preprocess, 141.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1661.7817), tensor(961.5814), tensor(1783.1147), tensor(1322.7073)], tensor(0.8471), 0), ([tensor(1.3343), tensor(567.6066), tensor(734.7806), tensor(942.1135)], tensor(0.4902), 6), ([tensor(1305.6594), tensor(794.3181), tensor(1533.2097), tensor(1032.3398)], tensor(0.3752), 8), ([tensor(1829.4647), tensor(975.0573), tensor(1925.7296), tensor(1113.8341)], tensor(0.3057), 0), ([tensor(1818.6692), tensor(975.9785), tensor(1927.1140), tensor(1153.2134)], tensor(0.2542), 0)]\n","Class person : ID 1: Confidence 0.254201740026474 : Bounding Box [     1860.5      975.92        3712      2116.8]\n","Class person : ID 2: Confidence 0.8471227288246155 : Bounding Box [     1599.5      961.64      3505.5      2285.3]\n","Class train : ID 3: Confidence 0.4901758134365082 : Bounding Box [    0.37121      567.59      737.11      1509.7]\n","Class umbrella : ID 7: Confidence None : Bounding Box [     1566.6      730.33      3260.8      1540.6]\n","Class boat : ID 16: Confidence 0.37519413232803345 : Bounding Box [     1306.9      795.78      2838.2      1828.2]\n","Logic frame count : 39\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 2 boats, 1 umbrella, 137.6ms\n","Speed: 3.9ms preprocess, 137.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1647.9675), tensor(964.1648), tensor(1779.8640), tensor(1336.0503)], tensor(0.8391), 0), ([tensor(1.1827), tensor(567.6409), tensor(734.7651), tensor(943.3590)], tensor(0.4999), 6), ([tensor(1306.7224), tensor(796.4744), tensor(1532.5750), tensor(1030.8344)], tensor(0.4562), 8), ([tensor(1831.6912), tensor(974.6953), tensor(1926.1526), tensor(1111.1865)], tensor(0.3394), 0), ([tensor(1728.3512), tensor(1016.8435), tensor(2032.1774), tensor(1438.6885)], tensor(0.2602), 8), ([tensor(1569.9802), tensor(731.4725), tensor(1693.8860), tensor(809.9909)], tensor(0.2581), 25), ([tensor(1852.0248), tensor(974.7528), tensor(1923.3722), tensor(1097.6779)], tensor(0.2534), 0)]\n","Class person : ID 1: Confidence 0.33938711881637573 : Bounding Box [       1878      975.29      3707.7      2095.4]\n","Class person : ID 2: Confidence 0.8391422629356384 : Bounding Box [     1587.1      963.45      3493.2      2296.8]\n","Class train : ID 3: Confidence 0.49988487362861633 : Bounding Box [   0.058535      567.71      737.16      1510.6]\n","Class umbrella : ID 7: Confidence 0.2581162750720978 : Bounding Box [     1569.6      731.33      3263.4      1541.3]\n","Class boat : ID 16: Confidence 0.4562067687511444 : Bounding Box [     1307.9      795.96      2838.1      1827.5]\n","Logic frame count : 40\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 train, 2 boats, 1 umbrella, 157.3ms\n","Speed: 4.1ms preprocess, 157.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1613.5669), tensor(964.3024), tensor(1779.0229), tensor(1342.8917)], tensor(0.8245), 0), ([tensor(1.2036), tensor(567.0030), tensor(734.8705), tensor(943.2701)], tensor(0.4949), 6), ([tensor(1825.9224), tensor(976.1550), tensor(1926.9333), tensor(1116.3325)], tensor(0.3949), 0), ([tensor(1306.3599), tensor(798.4283), tensor(1532.3091), tensor(1031.2909)], tensor(0.3619), 8), ([tensor(1846.8022), tensor(974.7666), tensor(1923.8228), tensor(1102.5208)], tensor(0.3151), 0), ([tensor(1728.8740), tensor(1017.7034), tensor(2031.9038), tensor(1439.1897)], tensor(0.2620), 8), ([tensor(1569.9678), tensor(731.4930), tensor(1694.0723), tensor(810.1473)], tensor(0.2540), 25)]\n","Class person : ID 1: Confidence 0.39488646388053894 : Bounding Box [     1875.4         976        3708      2091.8]\n","Class person : ID 2: Confidence 0.8244759440422058 : Bounding Box [     1560.8      964.21        3465      2305.5]\n","Class train : ID 3: Confidence 0.4949183166027069 : Bounding Box [   0.079892      567.33      737.14      1510.5]\n","Class umbrella : ID 7: Confidence 0.25395333766937256 : Bounding Box [     1569.8      731.44      3263.8      1541.5]\n","Class boat : ID 16: Confidence 0.3618858754634857 : Bounding Box [     1307.6      797.35      2837.9      1828.8]\n","Logic frame count : 41\n","Analyzing image for objects...\n","\n","0: 384x640 4 persons, 1 train, 2 boats, 158.7ms\n","Speed: 4.4ms preprocess, 158.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1598.9316), tensor(968.3688), tensor(1777.1655), tensor(1345.3513)], tensor(0.8569), 0), ([tensor(1.1779), tensor(567.3168), tensor(734.7685), tensor(944.0034)], tensor(0.5021), 6), ([tensor(1305.9689), tensor(791.4207), tensor(1532.7543), tensor(1031.4746)], tensor(0.4272), 8), ([tensor(1830.5288), tensor(974.5305), tensor(1926.9297), tensor(1110.4958)], tensor(0.3315), 0), ([tensor(1726.3027), tensor(1018.8271), tensor(2031.8716), tensor(1439.1008)], tensor(0.3157), 8), ([tensor(1853.6685), tensor(974.6230), tensor(1923.8008), tensor(1090.6833)], tensor(0.2548), 0), ([tensor(1816.1440), tensor(976.2034), tensor(1927.2598), tensor(1159.0398)], tensor(0.2525), 0)]\n","Class person : ID 1: Confidence 0.25248536467552185 : Bounding Box [     1845.1      976.28      3724.3      2119.1]\n","Class person : ID 2: Confidence 0.8568687438964844 : Bounding Box [     1543.7      967.15      3441.5      2312.9]\n","Class train : ID 3: Confidence 0.502098560333252 : Bounding Box [  -0.053962      567.38      737.18      1511.1]\n","Class person : ID 7: Confidence 0.33149391412734985 : Bounding Box [     1632.7      892.74      3701.6      1902.2]\n","Class boat : ID 16: Confidence 0.4271995425224304 : Bounding Box [     1307.2      793.26      2837.9      1824.8]\n","Class person : ID 19: Confidence 0.254808634519577 : Bounding Box [     1904.9      974.59      3726.9      2064.9]\n","Logic frame count : 42\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 2 boats, 228.2ms\n","Speed: 5.9ms preprocess, 228.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1590.3755), tensor(969.2839), tensor(1776.3145), tensor(1349.7258)], tensor(0.8729), 0), ([tensor(1.7422), tensor(566.6020), tensor(734.0541), tensor(943.5862)], tensor(0.5122), 6), ([tensor(1307.6106), tensor(798.3828), tensor(1532.9802), tensor(1031.9954)], tensor(0.4039), 8), ([tensor(1824.8782), tensor(975.5148), tensor(1927.5076), tensor(1118.3802)], tensor(0.3334), 0), ([tensor(1726.6758), tensor(1162.3130), tensor(1892.1392), tensor(1440.)], tensor(0.3120), 1), ([tensor(1810.0447), tensor(976.0555), tensor(1927.2048), tensor(1165.9489)], tensor(0.2597), 0), ([tensor(1726.6875), tensor(1018.9719), tensor(2032.2588), tensor(1439.7224)], tensor(0.2581), 8)]\n","Class person : ID 1: Confidence 0.25973495841026306 : Bounding Box [     1825.9      976.27      3730.1      2133.8]\n","Class person : ID 2: Confidence 0.8729144334793091 : Bounding Box [     1533.4      968.85      3425.4        2319]\n","Class train : ID 3: Confidence 0.5122024416923523 : Bounding Box [    0.23565      566.92      737.13      1510.6]\n","Class bicycle : ID 7: Confidence 0.31200817227363586 : Bounding Box [       1423      1063.3      3939.3      2342.6]\n","Class boat : ID 16: Confidence 0.40387195348739624 : Bounding Box [       1308      796.35      2839.3      1828.2]\n","Class person : ID 19: Confidence 0.333429217338562 : Bounding Box [     1870.5      975.21      3725.1      2083.3]\n","Logic frame count : 43\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 2 boats, 1 umbrella, 222.8ms\n","Speed: 11.3ms preprocess, 222.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1598.7898), tensor(969.6743), tensor(1772.2083), tensor(1350.9102)], tensor(0.8728), 0), ([tensor(1.1243), tensor(567.5211), tensor(734.3599), tensor(944.2260)], tensor(0.4949), 6), ([tensor(1832.6768), tensor(974.2881), tensor(1926.8691), tensor(1112.8162)], tensor(0.3324), 0), ([tensor(1570.0063), tensor(731.7492), tensor(1693.0664), tensor(810.1243)], tensor(0.2901), 25), ([tensor(1727.6099), tensor(1164.4613), tensor(1879.8408), tensor(1440.)], tensor(0.2840), 1), ([tensor(1727.7156), tensor(1018.8684), tensor(2032.1062), tensor(1439.6802)], tensor(0.2672), 8), ([tensor(1305.6970), tensor(802.6299), tensor(1530.5203), tensor(1033.1772)], tensor(0.2606), 8), ([tensor(1814.3284), tensor(975.7375), tensor(1927.3621), tensor(1166.8779)], tensor(0.2585), 0)]\n","Class person : ID 1: Confidence 0.3323740065097809 : Bounding Box [     1857.5      975.09      3722.9      2103.3]\n","Class person : ID 2: Confidence 0.8728076815605164 : Bounding Box [     1537.2      969.72      3420.8      2322.1]\n","Class train : ID 3: Confidence 0.4949354827404022 : Bounding Box [  -0.037782      567.35      736.88      1511.4]\n","Class bicycle : ID 7: Confidence 0.2839510440826416 : Bounding Box [     1365.6      1135.4      4002.1      2526.9]\n","Class boat : ID 16: Confidence 0.2606371343135834 : Bounding Box [     1305.7      800.35      2838.2      1833.2]\n","Class person : ID 19: Confidence 0.25846195220947266 : Bounding Box [     1823.7      975.58      3744.3      2123.9]\n","Class boat : ID 20: Confidence 0.26721876859664917 : Bounding Box [     1726.6        1019      3760.1      2458.7]\n","Logic frame count : 44\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 umbrella, 264.1ms\n","Speed: 4.8ms preprocess, 264.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1613.1554), tensor(970.7517), tensor(1767.2941), tensor(1351.3289)], tensor(0.8683), 0), ([tensor(1.2617), tensor(568.0463), tensor(735.8424), tensor(943.5099)], tensor(0.4811), 6), ([tensor(1834.5654), tensor(974.2404), tensor(1926.6421), tensor(1111.9669)], tensor(0.3535), 0), ([tensor(1727.0242), tensor(1166.4264), tensor(1881.2605), tensor(1440.)], tensor(0.3396), 1), ([tensor(1569.8962), tensor(731.7625), tensor(1692.6506), tensor(810.1105)], tensor(0.3207), 25)]\n","Class person : ID 1: Confidence 0.35346755385398865 : Bounding Box [     1868.9      974.59      3723.1        2091]\n","Class person : ID 2: Confidence 0.8683372735977173 : Bounding Box [     1550.2      970.72      3424.1        2324]\n","Class train : ID 3: Confidence 0.48105525970458984 : Bounding Box [    0.60441      567.86      737.21      1511.6]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1345.4      1168.6      4071.5      2607.4]\n","Class boat : ID 16: Confidence None : Bounding Box [     1305.6      800.41      2838.5      1833.5]\n","Class person : ID 19: Confidence None : Bounding Box [     1816.3      975.66      3746.5      2129.7]\n","Class bicycle : ID 20: Confidence 0.3395865261554718 : Bounding Box [     1676.2      1124.6        3702      2564.6]\n","Logic frame count : 45\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 212.8ms\n","Speed: 4.6ms preprocess, 212.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1624.1234), tensor(972.2275), tensor(1760.1010), tensor(1353.2637)], tensor(0.8331), 0), ([tensor(1.0660), tensor(568.1982), tensor(735.6720), tensor(944.4349)], tensor(0.4762), 6), ([tensor(1727.4717), tensor(1167.6683), tensor(1895.5171), tensor(1440.)], tensor(0.3828), 1), ([tensor(1839.1266), tensor(974.0345), tensor(1927.0138), tensor(1110.1937)], tensor(0.3461), 0), ([tensor(1855.4116), tensor(974.7128), tensor(1923.8120), tensor(1090.9030)], tensor(0.3130), 0), ([tensor(1569.9324), tensor(731.4956), tensor(1692.5447), tensor(810.1724)], tensor(0.2844), 25)]\n","Class person : ID 1: Confidence 0.3460899889469147 : Bounding Box [     1874.7      974.26      3727.8      2085.2]\n","Class person : ID 2: Confidence 0.8331116437911987 : Bounding Box [     1562.9      972.04        3428      2326.8]\n","Class train : ID 3: Confidence 0.47622233629226685 : Bounding Box [    0.48482      568.14      737.32      1512.4]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1325.1      1201.9      4140.8      2687.9]\n","Class boat : ID 16: Confidence None : Bounding Box [     1305.5      800.47      2838.8      1833.8]\n","Class person : ID 19: Confidence 0.3130451738834381 : Bounding Box [     1882.3      974.92        3737      2079.5]\n","Class bicycle : ID 20: Confidence 0.38283541798591614 : Bounding Box [     1666.5      1162.1      3684.8      2602.2]\n","Logic frame count : 46\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 246.5ms\n","Speed: 12.1ms preprocess, 246.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1618.5251), tensor(973.8478), tensor(1750.3032), tensor(1352.5448)], tensor(0.8549), 0), ([tensor(0.9370), tensor(568.3907), tensor(735.0019), tensor(943.8181)], tensor(0.4868), 6), ([tensor(1833.8942), tensor(973.7546), tensor(1926.3519), tensor(1115.4153)], tensor(0.3695), 0), ([tensor(1727.5742), tensor(1167.8040), tensor(1899.4539), tensor(1440.)], tensor(0.3571), 1), ([tensor(1815.1794), tensor(974.9785), tensor(1926.8816), tensor(1183.5295)], tensor(0.2724), 0), ([tensor(1570.0906), tensor(731.2416), tensor(1691.8809), tensor(810.1827)], tensor(0.2721), 25)]\n","Class person : ID 1: Confidence 0.36950400471687317 : Bounding Box [     1868.4      973.95      3730.4      2086.5]\n","Class person : ID 2: Confidence 0.8549137711524963 : Bounding Box [     1564.9      973.57      3419.2      2328.3]\n","Class train : ID 3: Confidence 0.4868454337120056 : Bounding Box [    0.33926      568.37      736.86      1512.4]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1304.8      1235.1      4210.2      2768.5]\n","Class boat : ID 16: Confidence None : Bounding Box [     1305.4      800.52      2839.1      1834.1]\n","Class person : ID 19: Confidence 0.27244117856025696 : Bounding Box [     1817.4      974.95      3759.5      2134.3]\n","Class bicycle : ID 20: Confidence 0.35711461305618286 : Bounding Box [     1667.5      1174.4      3678.2      2614.5]\n","Logic frame count : 47\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 umbrella, 239.0ms\n","Speed: 10.3ms preprocess, 239.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1603.6754), tensor(976.2762), tensor(1735.6283), tensor(1350.3590)], tensor(0.8456), 0), ([tensor(0.9409), tensor(568.4980), tensor(735.2348), tensor(943.7686)], tensor(0.4841), 6), ([tensor(1835.5966), tensor(974.1341), tensor(1925.7172), tensor(1110.5330)], tensor(0.3854), 0), ([tensor(1570.1012), tensor(731.2139), tensor(1689.9139), tensor(809.9097)], tensor(0.3061), 25), ([tensor(1729.5331), tensor(1248.7471), tensor(1854.5731), tensor(1440.)], tensor(0.3022), 1)]\n","Class person : ID 1: Confidence 0.3853906989097595 : Bounding Box [     1867.4      974.08      3731.7      2084.1]\n","Class person : ID 2: Confidence 0.8456022143363953 : Bounding Box [     1555.7      975.71        3397      2328.8]\n","Class train : ID 3: Confidence 0.48408424854278564 : Bounding Box [    0.40376      568.52      736.72      1512.4]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1284.6      1268.4      4279.5        2849]\n","Class boat : ID 16: Confidence None : Bounding Box [     1305.3      800.58      2839.3      1834.5]\n","Class person : ID 19: Confidence None : Bounding Box [     1811.1      974.91      3762.9        2140]\n","Class bicycle : ID 20: Confidence 0.30216318368911743 : Bounding Box [     1658.4      1232.9      3658.4        2673]\n","Class umbrella : ID 21: Confidence 0.3061201274394989 : Bounding Box [       1569       731.2      3261.6      1541.2]\n","Logic frame count : 48\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 160.2ms\n","Speed: 5.4ms preprocess, 160.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1563.6050), tensor(976.7080), tensor(1726.0371), tensor(1350.3794)], tensor(0.8435), 0), ([tensor(0.4642), tensor(567.1418), tensor(735.2798), tensor(944.0361)], tensor(0.4673), 6), ([tensor(1569.8856), tensor(732.2098), tensor(1690.7130), tensor(809.9851)], tensor(0.4387), 25), ([tensor(1834.6676), tensor(973.8158), tensor(1925.0614), tensor(1118.0538)], tensor(0.3738), 0), ([tensor(1728.4429), tensor(1171.4764), tensor(1888.8154), tensor(1440.)], tensor(0.3646), 1), ([tensor(1805.9641), tensor(974.9537), tensor(1927.7080), tensor(1180.1256)], tensor(0.2910), 0)]\n","Class person : ID 1: Confidence 0.3737972378730774 : Bounding Box [     1860.8      973.92      3736.6      2088.2]\n","Class person : ID 2: Confidence 0.8434794545173645 : Bounding Box [     1526.2      976.76      3355.9      2329.1]\n","Class train : ID 3: Confidence 0.467349112033844 : Bounding Box [   0.096993      567.67       736.4      1511.7]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1264.3      1301.6      4348.9      2929.6]\n","Class boat : ID 16: Confidence None : Bounding Box [     1305.2      800.63      2839.6      1834.8]\n","Class person : ID 19: Confidence 0.29097574949264526 : Bounding Box [     1788.6      974.94      3757.9      2153.1]\n","Class bicycle : ID 20: Confidence 0.3645859956741333 : Bounding Box [     1668.5        1201      3660.3        2641]\n","Class umbrella : ID 21: Confidence 0.438703715801239 : Bounding Box [     1568.9      731.88      3261.4      1541.9]\n","Logic frame count : 49\n","Analyzing image for objects...\n","\n","0: 384x640 4 persons, 1 bicycle, 1 train, 1 umbrella, 152.6ms\n","Speed: 4.4ms preprocess, 152.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1546.1013), tensor(979.3544), tensor(1718.5745), tensor(1348.6110)], tensor(0.8439), 0), ([tensor(0.3936), tensor(567.2463), tensor(735.5512), tensor(943.8118)], tensor(0.4638), 6), ([tensor(1832.8639), tensor(973.9809), tensor(1924.4427), tensor(1137.0156)], tensor(0.4105), 0), ([tensor(1807.2532), tensor(975.2612), tensor(1926.8689), tensor(1179.8542)], tensor(0.3770), 0), ([tensor(1570.1707), tensor(731.6559), tensor(1690.5686), tensor(809.9006)], tensor(0.3518), 25), ([tensor(1728.7468), tensor(1171.2020), tensor(1892.4785), tensor(1440.)], tensor(0.3475), 1), ([tensor(1843.7408), tensor(973.9877), tensor(1922.4064), tensor(1089.3651)], tensor(0.2758), 0)]\n","Class person : ID 1: Confidence 0.4105038046836853 : Bounding Box [     1846.5      973.97      3747.2      2102.4]\n","Class person : ID 2: Confidence 0.843880295753479 : Bounding Box [     1505.3      978.87      3322.7      2329.6]\n","Class train : ID 3: Confidence 0.4637952446937561 : Bounding Box [   0.092375      567.42      736.26      1511.3]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1244.1      1334.9      4418.2      3010.1]\n","Class boat : ID 16: Confidence None : Bounding Box [       1305      800.69      2839.9      1835.1]\n","Class person : ID 19: Confidence 0.27583789825439453 : Bounding Box [     1852.8      974.29      3734.7      2094.5]\n","Class bicycle : ID 20: Confidence 0.3474932014942169 : Bounding Box [     1676.5      1187.8      3660.6      2627.8]\n","Class umbrella : ID 21: Confidence 0.35178956389427185 : Bounding Box [     1569.2      731.74      3261.4      1541.7]\n","Logic frame count : 50\n","Analyzing image for objects...\n","\n","0: 384x640 5 persons, 1 bicycle, 1 train, 1 umbrella, 158.7ms\n","Speed: 4.7ms preprocess, 158.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1534.1799), tensor(981.4441), tensor(1719.4351), tensor(1349.4421)], tensor(0.8095), 0), ([tensor(0.3518), tensor(567.3398), tensor(735.4561), tensor(943.9968)], tensor(0.4599), 6), ([tensor(1831.2869), tensor(973.6319), tensor(1924.5227), tensor(1131.0664)], tensor(0.4515), 0), ([tensor(1805.0542), tensor(974.9545), tensor(1927.1099), tensor(1179.8634)], tensor(0.4247), 0), ([tensor(1728.9572), tensor(1173.3490), tensor(1893.1368), tensor(1440.)], tensor(0.3487), 1), ([tensor(1840.9236), tensor(973.2397), tensor(1922.3484), tensor(1085.5234)], tensor(0.3363), 0), ([tensor(1570.1599), tensor(731.3468), tensor(1690.1062), tensor(810.0343)], tensor(0.3094), 25), ([tensor(1264.9641), tensor(720.1741), tensor(1333.6245), tensor(884.4526)], tensor(0.2655), 0)]\n","Class person : ID 1: Confidence 0.4514791965484619 : Bounding Box [     1842.6      973.75      3747.7      2103.7]\n","Class person : ID 2: Confidence 0.8094537258148193 : Bounding Box [     1492.5         981      3300.6      2331.6]\n","Class train : ID 3: Confidence 0.4599408805370331 : Bounding Box [   0.013943      567.38      736.17      1511.4]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1223.8      1368.1      4487.6      3090.6]\n","Class person : ID 16: Confidence 0.26552993059158325 : Bounding Box [     1275.7      724.62      2603.4      1617.4]\n","Class person : ID 19: Confidence 0.42470088601112366 : Bounding Box [     1804.5      974.71      3748.8      2135.1]\n","Class bicycle : ID 20: Confidence 0.348660945892334 : Bounding Box [     1682.4      1183.5      3659.3      2623.5]\n","Class umbrella : ID 21: Confidence 0.3093559443950653 : Bounding Box [     1569.1      731.47      3261.3      1541.5]\n","Logic frame count : 51\n","Analyzing image for objects...\n","\n","0: 384x640 5 persons, 1 bicycle, 1 train, 1 umbrella, 190.1ms\n","Speed: 4.8ms preprocess, 190.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1524.0366), tensor(984.3174), tensor(1720.0967), tensor(1351.2693)], tensor(0.8360), 0), ([tensor(1727.1760), tensor(1169.3311), tensor(1906.6262), tensor(1440.)], tensor(0.4756), 1), ([tensor(1832.3059), tensor(974.2083), tensor(1926.2024), tensor(1135.4724)], tensor(0.4651), 0), ([tensor(0.4012), tensor(567.0088), tensor(734.9075), tensor(942.2445)], tensor(0.4626), 6), ([tensor(1812.6504), tensor(975.5374), tensor(1929.0225), tensor(1181.1311)], tensor(0.4380), 0), ([tensor(1839.1633), tensor(973.9752), tensor(1923.7405), tensor(1086.3268)], tensor(0.3657), 0), ([tensor(1569.9235), tensor(731.3391), tensor(1691.4467), tensor(809.9761)], tensor(0.3503), 25), ([tensor(1267.0586), tensor(717.1538), tensor(1340.7588), tensor(869.0439)], tensor(0.2549), 0)]\n","Class person : ID 1: Confidence 0.4651404917240143 : Bounding Box [     1839.6      974.05      3751.7      2107.5]\n","Class person : ID 2: Confidence 0.8359587788581848 : Bounding Box [     1483.5      983.65      3284.5      2335.4]\n","Class train : ID 3: Confidence 0.46255412697792053 : Bounding Box [    0.27131      567.14      735.55        1510]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1203.6      1401.3      4556.9      3171.2]\n","Class person : ID 16: Confidence 0.2548975348472595 : Bounding Box [     1283.3       717.5      2586.9      1589.8]\n","Class person : ID 19: Confidence 0.4379892349243164 : Bounding Box [     1793.7      975.24      3758.8      2150.7]\n","Class bicycle : ID 20: Confidence 0.47556155920028687 : Bounding Box [     1689.8      1178.5      3660.8      2618.6]\n","Class umbrella : ID 21: Confidence 0.3503054678440094 : Bounding Box [     1569.4      731.37      3261.5      1541.3]\n","Logic frame count : 52\n","Analyzing image for objects...\n","\n","0: 384x640 4 persons, 2 bicycles, 1 train, 1 umbrella, 166.1ms\n","Speed: 4.6ms preprocess, 166.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1520.9949), tensor(981.7068), tensor(1717.0989), tensor(1350.6030)], tensor(0.8462), 0), ([tensor(0.3422), tensor(567.0665), tensor(734.9291), tensor(942.3529)], tensor(0.4630), 6), ([tensor(1831.5743), tensor(975.7151), tensor(1926.8256), tensor(1153.7869)], tensor(0.4610), 0), ([tensor(1727.1079), tensor(1170.1575), tensor(1907.6138), tensor(1440.)], tensor(0.4319), 1), ([tensor(1833.6189), tensor(975.7440), tensor(1925.8518), tensor(1103.4534)], tensor(0.3690), 0), ([tensor(1569.9561), tensor(731.6002), tensor(1692.0332), tensor(810.1936)], tensor(0.3546), 25), ([tensor(1264.3521), tensor(714.9002), tensor(1343.0107), tensor(913.2888)], tensor(0.3382), 0), ([tensor(1727.2932), tensor(1024.9031), tensor(2032.0071), tensor(1440.)], tensor(0.2867), 1)]\n","Class person : ID 1: Confidence 0.3689929246902466 : Bounding Box [     1854.1      975.18        3739      2088.9]\n","Class person : ID 2: Confidence 0.8462425470352173 : Bounding Box [     1480.2      982.87      3273.2      2334.5]\n","Class train : ID 3: Confidence 0.4630093574523926 : Bounding Box [    0.31212      567.09      735.32      1509.6]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1183.3      1434.6      4626.3      3251.7]\n","Class person : ID 16: Confidence 0.33823496103286743 : Bounding Box [     1263.9      713.73        2601      1610.2]\n","Class person : ID 19: Confidence 0.46097898483276367 : Bounding Box [     1815.8      975.56        3760      2138.6]\n","Class bicycle : ID 20: Confidence 0.43191996216773987 : Bounding Box [     1694.6      1176.7      3660.3      2616.7]\n","Class umbrella : ID 21: Confidence 0.3546218276023865 : Bounding Box [     1569.6      731.51        3262      1541.6]\n","Logic frame count : 53\n","Analyzing image for objects...\n","\n","0: 384x640 4 persons, 1 bicycle, 1 train, 1 boat, 1 umbrella, 159.0ms\n","Speed: 5.2ms preprocess, 159.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1526.2739), tensor(976.6421), tensor(1714.2397), tensor(1352.8662)], tensor(0.7929), 0), ([tensor(0.2745), tensor(566.5519), tensor(733.5146), tensor(942.5160)], tensor(0.4654), 6), ([tensor(1828.5381), tensor(978.8782), tensor(1926.7251), tensor(1173.1067)], tensor(0.4132), 0), ([tensor(1570.0803), tensor(731.9384), tensor(1692.3777), tensor(810.3707)], tensor(0.3486), 25), ([tensor(1727.7341), tensor(1170.1863), tensor(1908.3044), tensor(1440.)], tensor(0.3479), 1), ([tensor(1262.5966), tensor(718.4265), tensor(1337.5983), tensor(889.1711)], tensor(0.2977), 0), ([tensor(1288.3423), tensor(801.1304), tensor(1536.1543), tensor(1033.3901)], tensor(0.2926), 8), ([tensor(1834.5682), tensor(978.5352), tensor(1922.7487), tensor(1108.8267)], tensor(0.2890), 0)]\n","Class person : ID 1: Confidence 0.28899893164634705 : Bounding Box [     1855.1      977.46      3737.8      2087.2]\n","Class person : ID 2: Confidence 0.7928577661514282 : Bounding Box [     1483.4      979.18      3270.6      2332.2]\n","Class train : ID 3: Confidence 0.4653533399105072 : Bounding Box [   -0.15243      566.73      734.69      1509.2]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1163.1      1467.8      4695.6      3332.3]\n","Class person : ID 16: Confidence 0.2976849377155304 : Bounding Box [       1265      714.88      2592.4      1603.9]\n","Class person : ID 19: Confidence 0.41315385699272156 : Bounding Box [     1813.4      977.78      3767.4      2148.6]\n","Class bicycle : ID 20: Confidence 0.3478790819644928 : Bounding Box [     1698.8      1175.6      3659.5      2615.6]\n","Class umbrella : ID 21: Confidence 0.34863603115081787 : Bounding Box [     1569.7      731.79      3262.5      1542.1]\n","Logic frame count : 54\n","Analyzing image for objects...\n","\n","0: 384x640 5 persons, 1 bicycle, 1 train, 1 umbrella, 168.1ms\n","Speed: 4.2ms preprocess, 168.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1529.5488), tensor(972.2183), tensor(1697.2817), tensor(1348.1885)], tensor(0.7847), 0), ([tensor(0.1752), tensor(565.9871), tensor(734.5383), tensor(942.7568)], tensor(0.4608), 6), ([tensor(1727.2434), tensor(1170.0640), tensor(1907.1990), tensor(1440.)], tensor(0.4142), 1), ([tensor(1570.0066), tensor(731.7881), tensor(1692.2473), tensor(810.4544)], tensor(0.3774), 25), ([tensor(1825.1602), tensor(980.7744), tensor(1920.5850), tensor(1131.3550)], tensor(0.3275), 0), ([tensor(1811.5044), tensor(977.6953), tensor(1924.1157), tensor(1181.0605)], tensor(0.3266), 0), ([tensor(1260.7148), tensor(714.7365), tensor(1330.7056), tensor(941.1842)], tensor(0.3206), 0), ([tensor(1835.1113), tensor(980.0342), tensor(1920.1768), tensor(1095.2144)], tensor(0.2756), 0)]\n","Class person : ID 1: Confidence 0.27561622858047485 : Bounding Box [     1860.4       979.3      3731.4      2078.6]\n","Class person : ID 2: Confidence 0.7847111821174622 : Bounding Box [     1486.2      974.82        3262      2325.2]\n","Class train : ID 3: Confidence 0.46080851554870605 : Bounding Box [     -0.117      566.22       734.8      1508.9]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1142.8      1501.1        4765      3412.8]\n","Class person : ID 16: Confidence 0.3206033706665039 : Bounding Box [     1240.3      713.13        2608      1634.7]\n","Class person : ID 19: Confidence 0.327463299036026 : Bounding Box [     1829.2      979.86        3745        2126]\n","Class bicycle : ID 20: Confidence 0.4142040014266968 : Bounding Box [     1701.2      1174.7      3657.2      2614.8]\n","Class umbrella : ID 21: Confidence 0.37736114859580994 : Bounding Box [     1569.7       731.8      3262.6      1542.2]\n","Logic frame count : 55\n","Analyzing image for objects...\n","\n","0: 384x640 5 persons, 1 bicycle, 1 train, 1 umbrella, 228.2ms\n","Speed: 4.5ms preprocess, 228.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1522.8477), tensor(971.0367), tensor(1667.9307), tensor(1349.6049)], tensor(0.7622), 0), ([tensor(1727.9760), tensor(1168.1984), tensor(1908.9669), tensor(1440.)], tensor(0.5196), 1), ([tensor(0.1865), tensor(565.9301), tensor(734.6196), tensor(942.3948)], tensor(0.4643), 6), ([tensor(1529.9767), tensor(974.5101), tensor(1626.6049), tensor(1343.9078)], tensor(0.4531), 0), ([tensor(1814.1561), tensor(980.9543), tensor(1921.7904), tensor(1181.6233)], tensor(0.4002), 0), ([tensor(1828.1166), tensor(982.3054), tensor(1918.5131), tensor(1121.4299)], tensor(0.3328), 0), ([tensor(1570.0830), tensor(731.6552), tensor(1691.8916), tensor(810.5477)], tensor(0.3278), 25), ([tensor(1260.9641), tensor(712.6897), tensor(1339.6843), tensor(940.2800)], tensor(0.2538), 0)]\n","Class person : ID 1: Confidence 0.3328222632408142 : Bounding Box [     1842.9      981.48        3738      2094.4]\n","Class person : ID 2: Confidence 0.4531099796295166 : Bounding Box [     1471.8      974.67        3229      2321.1]\n","Class train : ID 3: Confidence 0.46425989270210266 : Bounding Box [   0.023563         566      734.79      1508.5]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1122.6      1534.3      4834.3      3493.3]\n","Class person : ID 16: Confidence 0.25383806228637695 : Bounding Box [     1236.5      711.27      2615.2      1644.3]\n","Class person : ID 19: Confidence 0.40020838379859924 : Bounding Box [     1803.5      980.76      3754.3      2150.7]\n","Class bicycle : ID 20: Confidence 0.5195767879486084 : Bounding Box [     1704.5      1172.9      3656.3      2612.9]\n","Class umbrella : ID 21: Confidence 0.32778680324554443 : Bounding Box [     1569.6      731.71      3262.6      1542.2]\n","Logic frame count : 56\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 165.6ms\n","Speed: 7.8ms preprocess, 165.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1517.9760), tensor(972.1435), tensor(1618.8436), tensor(1346.8472)], tensor(0.7829), 0), ([tensor(1726.4409), tensor(1166.1379), tensor(1909.9092), tensor(1440.)], tensor(0.5445), 1), ([tensor(0.1553), tensor(565.5789), tensor(734.0080), tensor(943.0162)], tensor(0.4651), 6), ([tensor(1811.1871), tensor(986.1836), tensor(1920.0106), tensor(1182.0442)], tensor(0.3974), 0), ([tensor(1570.0725), tensor(731.9258), tensor(1692.4199), tensor(810.5678)], tensor(0.3633), 25), ([tensor(1821.4316), tensor(985.3259), tensor(1914.7976), tensor(1142.7019)], tensor(0.2731), 0)]\n","Class person : ID 1: Confidence 0.2730559706687927 : Bounding Box [     1820.4      984.27      3745.2      2116.4]\n","Class person : ID 2: Confidence 0.7829081416130066 : Bounding Box [       1459      973.04      3203.6        2320]\n","Class train : ID 3: Confidence 0.4650840163230896 : Bounding Box [   -0.24819      565.68      734.67      1508.5]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1102.3      1567.6      4903.7      3573.9]\n","Class person : ID 16: Confidence None : Bounding Box [     1230.3      706.98      2607.8      1639.3]\n","Class person : ID 19: Confidence 0.3973832130432129 : Bounding Box [     1792.8      984.51        3754      2163.5]\n","Class bicycle : ID 20: Confidence 0.5445166230201721 : Bounding Box [     1706.3      1170.5      3654.3      2610.5]\n","Class umbrella : ID 21: Confidence 0.3632515072822571 : Bounding Box [     1569.7      731.86      3262.7      1542.4]\n","Logic frame count : 57\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 159.7ms\n","Speed: 4.5ms preprocess, 159.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1503.5375), tensor(971.6077), tensor(1602.3544), tensor(1356.3936)], tensor(0.8039), 0), ([tensor(0.1072), tensor(565.9955), tensor(734.3089), tensor(943.2504)], tensor(0.4639), 6), ([tensor(1251.2070), tensor(716.4371), tensor(1335.2568), tensor(952.1766)], tensor(0.4486), 0), ([tensor(1726.9021), tensor(1164.3792), tensor(1910.6228), tensor(1440.)], tensor(0.4000), 1), ([tensor(1809.8010), tensor(991.6885), tensor(1918.2854), tensor(1179.7456)], tensor(0.3297), 0), ([tensor(1570.1586), tensor(731.9695), tensor(1693.2391), tensor(810.5278)], tensor(0.2925), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1818.6      985.23      3745.1      2118.3]\n","Class person : ID 2: Confidence 0.8039460182189941 : Bounding Box [     1440.4      972.07      3178.7      2325.4]\n","Class train : ID 3: Confidence 0.4638656675815582 : Bounding Box [   -0.31632      565.84      734.74      1508.9]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1082.1      1600.8        4973      3654.4]\n","Class person : ID 16: Confidence 0.44857677817344666 : Bounding Box [     1221.4      713.42      2613.4      1661.1]\n","Class person : ID 19: Confidence 0.3296588659286499 : Bounding Box [     1789.8      989.54      3749.8      2170.4]\n","Class bicycle : ID 20: Confidence 0.400033175945282 : Bounding Box [     1708.5      1168.2      3653.2      2608.2]\n","Class umbrella : ID 21: Confidence 0.2925380766391754 : Bounding Box [     1570.1      731.94      3263.1      1542.5]\n","Logic frame count : 58\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 umbrella, 157.7ms\n","Speed: 4.6ms preprocess, 157.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1475.4369), tensor(973.7183), tensor(1592.2577), tensor(1348.5686)], tensor(0.7435), 0), ([tensor(0.1365), tensor(566.2195), tensor(734.8264), tensor(943.4456)], tensor(0.4648), 6), ([tensor(1250.3910), tensor(716.1160), tensor(1341.7377), tensor(949.9067)], tensor(0.3783), 0), ([tensor(1729.6414), tensor(1000.5547), tensor(2034.1946), tensor(1437.8481)], tensor(0.3118), 1), ([tensor(1570.0155), tensor(732.2726), tensor(1692.1935), tensor(810.1613)], tensor(0.3111), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1816.9      986.19        3745      2120.2]\n","Class person : ID 2: Confidence 0.7434568405151367 : Bounding Box [     1419.3       973.1      3140.6      2323.7]\n","Class train : ID 3: Confidence 0.4647732079029083 : Bounding Box [    -0.1916      566.06         735      1509.4]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1061.8      1634.1      5042.4        3735]\n","Class person : ID 16: Confidence 0.3783107101917267 : Bounding Box [     1223.6      714.21      2612.3      1663.7]\n","Class person : ID 19: Confidence None : Bounding Box [     1784.8       990.9      3750.9      2175.5]\n","Class bicycle : ID 20: Confidence 0.3117942810058594 : Bounding Box [     1748.3      1058.7      3699.5      2497.3]\n","Class umbrella : ID 21: Confidence 0.31114205718040466 : Bounding Box [       1570      732.17      3262.6      1542.5]\n","Logic frame count : 59\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 2 bicycles, 1 motorcycle, 1 train, 1 umbrella, 159.0ms\n","Speed: 4.5ms preprocess, 159.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1427.9968), tensor(972.7356), tensor(1586.5901), tensor(1343.8740)], tensor(0.7907), 0), ([tensor(0.0121), tensor(566.0919), tensor(735.0128), tensor(943.6592)], tensor(0.4668), 6), ([tensor(1569.8053), tensor(732.7844), tensor(1693.8651), tensor(810.1354)], tensor(0.4042), 25), ([tensor(1725.5530), tensor(1003.9698), tensor(2029.9480), tensor(1439.7953)], tensor(0.3543), 3), ([tensor(1725.2584), tensor(1165.6562), tensor(1911.1254), tensor(1440.)], tensor(0.2811), 1), ([tensor(1724.2195), tensor(1000.6357), tensor(2033.6494), tensor(1440.)], tensor(0.2758), 1), ([tensor(1247.7244), tensor(806.6772), tensor(1326.4885), tensor(949.4853)], tensor(0.2559), 0)]\n","Class person : ID 1: Confidence None : Bounding Box [     1815.2      987.15        3745      2122.2]\n","Class person : ID 2: Confidence 0.7907202243804932 : Bounding Box [     1384.1      972.84        3088      2319.3]\n","Class train : ID 3: Confidence 0.46684321761131287 : Bounding Box [   -0.20495      566.06      735.12      1509.6]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1041.5      1667.3      5111.7      3815.5]\n","Class person : ID 16: Confidence 0.2558874189853668 : Bounding Box [     1220.2      774.43      2602.8      1724.3]\n","Class person : ID 19: Confidence None : Bounding Box [     1779.7      992.26        3752      2180.5]\n","Class bicycle : ID 20: Confidence 0.2810838520526886 : Bounding Box [     1722.2      1126.7      3670.7      2566.1]\n","Class umbrella : ID 21: Confidence 0.40415099263191223 : Bounding Box [     1570.4       732.6      3262.8      1542.8]\n","Logic frame count : 60\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 umbrella, 161.4ms\n","Speed: 4.5ms preprocess, 161.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1399.1831), tensor(977.3237), tensor(1588.5027), tensor(1344.0693)], tensor(0.8374), 0), ([tensor(0.8008), tensor(566.2053), tensor(734.9077), tensor(944.0850)], tensor(0.4906), 6), ([tensor(1725.3931), tensor(1164.1890), tensor(1908.7305), tensor(1440.)], tensor(0.4834), 1), ([tensor(1569.8997), tensor(732.4304), tensor(1693.7561), tensor(810.3552)], tensor(0.3495), 25), ([tensor(1248.4558), tensor(722.9180), tensor(1323.0386), tensor(953.6318)], tensor(0.2509), 0)]\n","Class person : ID 1: Confidence None : Bounding Box [     1813.5      988.11      3744.9      2124.1]\n","Class person : ID 2: Confidence 0.837352991104126 : Bounding Box [       1356      975.78      3047.4      2320.7]\n","Class train : ID 3: Confidence 0.4905957281589508 : Bounding Box [    0.20134      566.14      735.74      1510.1]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     1021.3      1700.5        5181        3896]\n","Class person : ID 16: Confidence 0.25091835856437683 : Bounding Box [     1218.7      741.83      2598.5      1694.5]\n","Class person : ID 19: Confidence None : Bounding Box [     1774.6      993.62      3753.1      2185.6]\n","Class bicycle : ID 20: Confidence 0.4834195375442505 : Bounding Box [     1712.8      1151.5        3658      2591.3]\n","Class umbrella : ID 21: Confidence 0.3495294749736786 : Bounding Box [     1570.4      732.52      3263.1      1542.8]\n","Logic frame count : 61\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 bicycles, 1 train, 1 umbrella, 153.4ms\n","Speed: 4.7ms preprocess, 153.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1394.4780), tensor(980.4023), tensor(1593.1575), tensor(1343.5757)], tensor(0.8588), 0), ([tensor(0.7757), tensor(566.4590), tensor(734.6126), tensor(944.0256)], tensor(0.4932), 6), ([tensor(1726.1331), tensor(1166.2544), tensor(1908.8977), tensor(1440.)], tensor(0.3432), 1), ([tensor(1569.8740), tensor(731.7637), tensor(1692.2122), tensor(810.4473)], tensor(0.3271), 25), ([tensor(1724.3350), tensor(1007.4216), tensor(2034.5015), tensor(1440.)], tensor(0.2592), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1811.8      989.07      3744.8        2126]\n","Class person : ID 2: Confidence 0.8588495850563049 : Bounding Box [     1347.3      978.93      3028.4        2323]\n","Class train : ID 3: Confidence 0.49323052167892456 : Bounding Box [    0.29096      566.33      735.81      1510.4]\n","Class bicycle : ID 7: Confidence None : Bounding Box [       1001      1733.8      5250.4      3976.6]\n","Class person : ID 16: Confidence None : Bounding Box [     1212.5      741.96      2594.1      1695.8]\n","Class bicycle : ID 19: Confidence 0.2592153549194336 : Bounding Box [     1593.2        1006      3894.7      2417.4]\n","Class bicycle : ID 20: Confidence 0.34321337938308716 : Bounding Box [     1710.7      1162.2      3652.7      2602.2]\n","Class umbrella : ID 21: Confidence 0.3270798921585083 : Bounding Box [     1569.8      732.05      3262.7      1542.4]\n","Logic frame count : 62\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 174.7ms\n","Speed: 4.6ms preprocess, 174.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1394.5027), tensor(982.5801), tensor(1591.2810), tensor(1344.6030)], tensor(0.8416), 0), ([tensor(0.7376), tensor(567.1161), tensor(734.9357), tensor(945.2128)], tensor(0.4883), 6), ([tensor(1726.7156), tensor(1165.6847), tensor(1908.6667), tensor(1440.)], tensor(0.4317), 1), ([tensor(1251.3657), tensor(723.4310), tensor(1319.9062), tensor(948.9989)], tensor(0.3615), 0), ([tensor(1569.7754), tensor(732.1917), tensor(1694.2839), tensor(810.6277)], tensor(0.3157), 25), ([tensor(1800.8406), tensor(1000.6083), tensor(1924.1426), tensor(1178.9132)], tensor(0.2702), 0)]\n","Class person : ID 1: Confidence None : Bounding Box [     1810.1      990.03      3744.7      2127.9]\n","Class person : ID 2: Confidence 0.8415799736976624 : Bounding Box [     1346.4      981.54      3019.2      2325.9]\n","Class train : ID 3: Confidence 0.4882650375366211 : Bounding Box [    0.15537      566.84      736.16      1511.7]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     980.79        1767      5319.7      4057.1]\n","Class person : ID 16: Confidence 0.3614797592163086 : Bounding Box [     1223.2      727.57      2593.5      1677.9]\n","Class person : ID 19: Confidence 0.27021536231040955 : Bounding Box [     1718.6      1003.4        3791      2274.2]\n","Class bicycle : ID 20: Confidence 0.43168315291404724 : Bounding Box [     1711.3      1165.8      3650.2      2605.8]\n","Class umbrella : ID 21: Confidence 0.31571105122566223 : Bounding Box [       1570      732.15      3263.3      1542.7]\n","Logic frame count : 63\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 boat, 1 umbrella, 158.7ms\n","Speed: 5.8ms preprocess, 158.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1392.0162), tensor(978.7988), tensor(1585.7047), tensor(1342.9456)], tensor(0.8380), 0), ([tensor(1248.0127), tensor(722.3925), tensor(1320.0493), tensor(950.5590)], tensor(0.5356), 0), ([tensor(0.6971), tensor(567.2139), tensor(734.6720), tensor(945.7046)], tensor(0.4777), 6), ([tensor(1726.9380), tensor(1164.9751), tensor(1907.8569), tensor(1440.)], tensor(0.4586), 1), ([tensor(1569.4592), tensor(731.8616), tensor(1691.9441), tensor(809.8928)], tensor(0.3648), 25), ([tensor(1287.4387), tensor(792.3210), tensor(1547.6667), tensor(1017.4266)], tensor(0.2622), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1808.3      990.99      3744.7      2129.9]\n","Class person : ID 2: Confidence 0.8380433917045593 : Bounding Box [     1346.4         980      3009.9      2323.4]\n","Class train : ID 3: Confidence 0.4777207374572754 : Bounding Box [  -0.066768       567.1      736.24      1512.6]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     960.54      1800.3      5389.1      4137.7]\n","Class person : ID 16: Confidence 0.5356395244598389 : Bounding Box [     1224.1      723.62      2589.5      1674.3]\n","Class person : ID 19: Confidence None : Bounding Box [     1708.2      1005.3      3796.6      2285.9]\n","Class bicycle : ID 20: Confidence 0.45860037207603455 : Bounding Box [     1712.3      1166.5      3648.3      2606.5]\n","Class umbrella : ID 21: Confidence 0.36481356620788574 : Bounding Box [     1569.7      731.97      3262.1      1542.1]\n","Logic frame count : 64\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 boat, 1 umbrella, 178.0ms\n","Speed: 4.3ms preprocess, 178.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1388.6753), tensor(974.2579), tensor(1560.3298), tensor(1344.3353)], tensor(0.8153), 0), ([tensor(1237.0219), tensor(720.5043), tensor(1319.1007), tensor(950.5074)], tensor(0.6511), 0), ([tensor(0.7329), tensor(567.4794), tensor(733.3997), tensor(945.5669)], tensor(0.4845), 6), ([tensor(1726.1217), tensor(1167.0859), tensor(1908.8307), tensor(1440.)], tensor(0.3802), 1), ([tensor(1569.6489), tensor(731.9975), tensor(1692.3447), tensor(810.0092)], tensor(0.3085), 25), ([tensor(1296.8153), tensor(790.0444), tensor(1545.6779), tensor(1020.7581)], tensor(0.2866), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1806.6      991.95      3744.6      2131.8]\n","Class person : ID 2: Confidence 0.8153027296066284 : Bounding Box [     1339.3      976.39      2993.6      2320.4]\n","Class train : ID 3: Confidence 0.48452821373939514 : Bounding Box [   -0.40421      567.37      735.73        1513]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     940.29      1833.5      5458.4      4218.2]\n","Class person : ID 16: Confidence 0.6511183381080627 : Bounding Box [     1218.4         721        2579      1671.8]\n","Class person : ID 19: Confidence None : Bounding Box [     1697.8      1007.3      3802.1      2297.7]\n","Class bicycle : ID 20: Confidence 0.3801576495170593 : Bounding Box [     1713.2        1168      3646.7      2608.1]\n","Class umbrella : ID 21: Confidence 0.3085256516933441 : Bounding Box [     1569.7      731.99        3262        1542]\n","Logic frame count : 65\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 boat, 1 umbrella, 164.9ms\n","Speed: 4.3ms preprocess, 164.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1375.6089), tensor(972.6279), tensor(1528.2891), tensor(1346.1064)], tensor(0.8322), 0), ([tensor(1240.4348), tensor(723.0009), tensor(1319.3035), tensor(950.4542)], tensor(0.6344), 0), ([tensor(1.3550), tensor(566.2341), tensor(733.0974), tensor(943.7991)], tensor(0.4949), 6), ([tensor(1569.5920), tensor(732.1489), tensor(1692.0344), tensor(809.9355)], tensor(0.4041), 25), ([tensor(1725.5784), tensor(1168.5435), tensor(1907.5950), tensor(1440.)], tensor(0.3299), 1), ([tensor(1296.5854), tensor(787.9728), tensor(1545.0225), tensor(1020.4833)], tensor(0.2644), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1804.9      992.91      3744.5      2133.7]\n","Class person : ID 2: Confidence 0.8322356939315796 : Bounding Box [     1321.6      973.93      2965.3      2319.3]\n","Class train : ID 3: Confidence 0.49494096636772156 : Bounding Box [    0.26075      566.65      735.36      1511.1]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     920.04      1866.8      5527.8      4298.7]\n","Class person : ID 16: Confidence 0.6344496011734009 : Bounding Box [     1219.9      721.72      2576.3      1672.5]\n","Class person : ID 19: Confidence None : Bounding Box [     1687.4      1009.3      3807.7      2309.4]\n","Class bicycle : ID 20: Confidence 0.32993441820144653 : Bounding Box [     1713.6      1169.5      3644.6      2609.5]\n","Class umbrella : ID 21: Confidence 0.40405750274658203 : Bounding Box [     1569.6       732.1      3261.7      1542.1]\n","Logic frame count : 66\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 bicycle, 1 train, 1 umbrella, 259.2ms\n","Speed: 10.4ms preprocess, 259.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1364.8230), tensor(971.8542), tensor(1509.9666), tensor(1344.2559)], tensor(0.8343), 0), ([tensor(1.2657), tensor(566.2599), tensor(733.0796), tensor(944.5314)], tensor(0.4996), 6), ([tensor(1725.8290), tensor(1166.6777), tensor(1910.3480), tensor(1440.)], tensor(0.4409), 1), ([tensor(1243.1029), tensor(722.1703), tensor(1322.3488), tensor(947.2886)], tensor(0.3736), 0), ([tensor(1569.6890), tensor(731.8826), tensor(1692.8550), tensor(810.3394)], tensor(0.3379), 25), ([tensor(1788.4441), tensor(1006.5360), tensor(1904.7156), tensor(1179.8204)], tensor(0.2669), 0)]\n","Class person : ID 1: Confidence None : Bounding Box [     1803.2      993.87      3744.5      2135.6]\n","Class person : ID 2: Confidence 0.834327220916748 : Bounding Box [     1307.1      972.49      2937.3      2317.2]\n","Class train : ID 3: Confidence 0.4996373951435089 : Bounding Box [    0.33115      566.39      735.28      1510.9]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     899.79        1900      5597.1      4379.3]\n","Class person : ID 16: Confidence 0.37357503175735474 : Bounding Box [     1225.6      721.51        2576      1670.2]\n","Class person : ID 19: Confidence 0.2668918967247009 : Bounding Box [     1767.9      1007.1      3714.8      2202.5]\n","Class bicycle : ID 20: Confidence 0.44087356328964233 : Bounding Box [     1715.4      1168.6      3644.5      2608.7]\n","Class umbrella : ID 21: Confidence 0.33785948157310486 : Bounding Box [     1569.7      731.96      3262.2      1542.2]\n","Logic frame count : 67\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 boat, 1 umbrella, 215.9ms\n","Speed: 4.3ms preprocess, 215.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1348.8623), tensor(971.7068), tensor(1509.9294), tensor(1341.9514)], tensor(0.8715), 0), ([tensor(1.2925), tensor(566.6123), tensor(732.9221), tensor(944.4097)], tensor(0.4919), 6), ([tensor(1726.3062), tensor(1167.4142), tensor(1910.2961), tensor(1440.)], tensor(0.4040), 1), ([tensor(1569.8713), tensor(732.6970), tensor(1695.5740), tensor(810.3961)], tensor(0.4007), 25), ([tensor(1304.6885), tensor(806.7407), tensor(1526.4541), tensor(1015.5217)], tensor(0.2595), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1801.5      994.83      3744.4      2137.5]\n","Class person : ID 2: Confidence 0.8714941740036011 : Bounding Box [     1295.8      971.86      2912.7      2314.7]\n","Class train : ID 3: Confidence 0.4918988049030304 : Bounding Box [    0.41078      566.53      735.12        1511]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     879.54      1933.3      5666.5      4459.8]\n","Class boat : ID 16: Confidence 0.25947731733322144 : Bounding Box [     1299.5      777.41      2720.9      1770.4]\n","Class person : ID 19: Confidence None : Bounding Box [     1765.4      1008.7      3711.7      2203.7]\n","Class bicycle : ID 20: Confidence 0.403953492641449 : Bounding Box [     1716.9      1168.7      3644.2      2608.7]\n","Class umbrella : ID 21: Confidence 0.40073835849761963 : Bounding Box [     1570.5      732.45      3263.6      1542.8]\n","Logic frame count : 68\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 228.1ms\n","Speed: 8.0ms preprocess, 228.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1339.6318), tensor(968.0002), tensor(1481.6641), tensor(1342.1790)], tensor(0.8717), 0), ([tensor(0.6016), tensor(568.0073), tensor(731.9996), tensor(942.8037)], tensor(0.4961), 6), ([tensor(1726.8210), tensor(1167.7126), tensor(1910.9978), tensor(1440.)], tensor(0.4496), 1), ([tensor(1570.0997), tensor(731.8397), tensor(1693.0079), tensor(810.2593)], tensor(0.2511), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1799.7      995.79      3744.3      2139.5]\n","Class person : ID 2: Confidence 0.8717107772827148 : Bounding Box [       1281      969.19      2884.4      2311.5]\n","Class train : ID 3: Confidence 0.4960978627204895 : Bounding Box [     0.1264       567.5      733.86      1510.9]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     859.29      1966.5      5735.8      4540.4]\n","Class boat : ID 16: Confidence None : Bounding Box [       1305      782.37      2733.7      1780.4]\n","Class person : ID 19: Confidence None : Bounding Box [     1762.9      1010.3      3708.6        2205]\n","Class bicycle : ID 20: Confidence 0.4496040344238281 : Bounding Box [     1718.6      1168.8      3644.3      2608.8]\n","Class umbrella : ID 21: Confidence 0.2511257827281952 : Bounding Box [     1570.3      732.06      3263.3      1542.4]\n","Logic frame count : 69\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 248.8ms\n","Speed: 4.5ms preprocess, 248.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1325.5774), tensor(967.4165), tensor(1469.1487), tensor(1345.3792)], tensor(0.8320), 0), ([tensor(0.6326), tensor(568.0464), tensor(732.2997), tensor(942.8613)], tensor(0.5082), 6), ([tensor(1725.8062), tensor(1167.8022), tensor(1909.1885), tensor(1440.)], tensor(0.4101), 1), ([tensor(1796.9441), tensor(1020.7096), tensor(1923.9851), tensor(1166.9105)], tensor(0.2983), 0)]\n","Class person : ID 1: Confidence None : Bounding Box [       1798      996.75      3744.2      2141.4]\n","Class person : ID 2: Confidence 0.8320175409317017 : Bounding Box [     1265.5      967.81      2858.1      2312.1]\n","Class train : ID 3: Confidence 0.5081787109375 : Bounding Box [    0.14882      567.89      733.49      1510.9]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     839.04      1999.7      5805.1      4620.9]\n","Class boat : ID 16: Confidence None : Bounding Box [     1310.6      787.33      2746.6      1790.5]\n","Class person : ID 19: Confidence 0.29826048016548157 : Bounding Box [     1800.3      1019.5      3710.3      2190.2]\n","Class bicycle : ID 20: Confidence 0.41009560227394104 : Bounding Box [     1718.4      1168.8      3642.6      2608.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.4      732.07      3263.4      1542.4]\n","Logic frame count : 70\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 223.0ms\n","Speed: 5.9ms preprocess, 223.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1308.4733), tensor(967.4075), tensor(1462.9757), tensor(1339.3447)], tensor(0.8530), 0), ([tensor(0.6212), tensor(568.3282), tensor(732.2737), tensor(942.8624)], tensor(0.5044), 6), ([tensor(1725.1831), tensor(1166.5804), tensor(1909.6191), tensor(1440.)], tensor(0.4205), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1796.3      997.71      3744.2      2143.3]\n","Class person : ID 2: Confidence 0.8530373573303223 : Bounding Box [     1252.8       967.3      2829.7      2308.3]\n","Class train : ID 3: Confidence 0.5044475793838501 : Bounding Box [    0.17052      568.23      733.32      1511.1]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     818.79        2033      5874.5      4701.4]\n","Class boat : ID 16: Confidence None : Bounding Box [     1316.1      792.29      2759.4      1800.5]\n","Class person : ID 19: Confidence None : Bounding Box [       1802      1021.8      3707.6      2189.8]\n","Class bicycle : ID 20: Confidence 0.42047640681266785 : Bounding Box [     1718.5        1168      3641.3        2608]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.4      732.07      3263.4      1542.4]\n","Logic frame count : 71\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 motorcycle, 1 train, 1 boat, 254.7ms\n","Speed: 4.2ms preprocess, 254.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1292.9805), tensor(964.1962), tensor(1470.2329), tensor(1335.6611)], tensor(0.8573), 0), ([tensor(0.7359), tensor(568.0513), tensor(733.0815), tensor(944.1777)], tensor(0.4941), 6), ([tensor(1725.2518), tensor(1165.1118), tensor(1910.4298), tensor(1440.)], tensor(0.3722), 1), ([tensor(1728.4900), tensor(1019.8994), tensor(2031.1663), tensor(1439.4341)], tensor(0.2862), 3), ([tensor(1782.6819), tensor(1031.8459), tensor(1908.1072), tensor(1177.7771)], tensor(0.2725), 0), ([tensor(1299.0352), tensor(810.2186), tensor(1528.8828), tensor(1023.5361)], tensor(0.2529), 8)]\n","Class motorcycle : ID 1: Confidence 0.2861579358577728 : Bounding Box [     1565.5      1019.6      3923.4      2454.4]\n","Class person : ID 2: Confidence 0.8572903275489807 : Bounding Box [     1244.8      965.02      2807.9      2302.4]\n","Class train : ID 3: Confidence 0.4940668046474457 : Bounding Box [    0.22005      568.16       733.9      1511.9]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     798.54      2066.2      5943.8        4782]\n","Class boat : ID 16: Confidence 0.2528970539569855 : Bounding Box [     1325.8      808.64      2797.1      1830.9]\n","Class person : ID 19: Confidence 0.272535502910614 : Bounding Box [     1782.5      1030.3        3698      2205.5]\n","Class bicycle : ID 20: Confidence 0.3722386658191681 : Bounding Box [     1719.2      1166.6      3640.9      2606.6]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.5      732.08      3263.5      1542.4]\n","Logic frame count : 72\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 230.5ms\n","Speed: 4.5ms preprocess, 230.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1286.6868), tensor(953.2983), tensor(1433.6448), tensor(1329.3281)], tensor(0.8355), 0), ([tensor(0.6573), tensor(568.2555), tensor(731.8118), tensor(944.6842)], tensor(0.4924), 6), ([tensor(1724.5366), tensor(1163.4281), tensor(1920.4673), tensor(1439.8597)], tensor(0.3570), 1), ([tensor(1782.9009), tensor(1041.8325), tensor(1891.0459), tensor(1180.6177)], tensor(0.3019), 0)]\n","Class motorcycle : ID 1: Confidence None : Bounding Box [     1548.5      1021.8      3935.7      2474.4]\n","Class person : ID 2: Confidence 0.8355449438095093 : Bounding Box [     1232.6      956.97      2777.7      2288.8]\n","Class train : ID 3: Confidence 0.49243173003196716 : Bounding Box [   -0.25428      568.27      733.68      1512.6]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     778.29      2099.5      6013.2      4862.5]\n","Class boat : ID 16: Confidence None : Bounding Box [     1332.1      814.63      2811.8      1842.8]\n","Class person : ID 19: Confidence 0.3018592298030853 : Bounding Box [     1773.4      1039.1      3689.7      2217.5]\n","Class bicycle : ID 20: Confidence 0.3569600284099579 : Bounding Box [     1722.3      1164.9      3643.7      2604.8]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.5      732.08      3263.6      1542.4]\n","Logic frame count : 73\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 168.7ms\n","Speed: 4.3ms preprocess, 168.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1239.1907), tensor(945.7935), tensor(1387.0027), tensor(1321.5378)], tensor(0.8633), 0), ([tensor(0.6838), tensor(567.8199), tensor(732.0352), tensor(944.8563)], tensor(0.4827), 6), ([tensor(1726.0238), tensor(1160.6931), tensor(1920.3676), tensor(1440.)], tensor(0.4620), 1)]\n","Class motorcycle : ID 1: Confidence None : Bounding Box [     1531.5      1023.9        3948      2494.4]\n","Class person : ID 2: Confidence 0.8633086681365967 : Bounding Box [     1189.6      949.01      2712.4      2273.6]\n","Class train : ID 3: Confidence 0.48268768191337585 : Bounding Box [    -0.3217      568.01      733.67      1512.7]\n","Class bicycle : ID 7: Confidence None : Bounding Box [     758.04      2132.7      6082.5      4943.1]\n","Class boat : ID 16: Confidence None : Bounding Box [     1338.3      820.62      2826.6      1854.7]\n","Class person : ID 19: Confidence None : Bounding Box [     1771.6      1042.8        3686        2220]\n","Class bicycle : ID 20: Confidence 0.46204131841659546 : Bounding Box [     1724.4      1162.4      3645.7      2602.4]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.6      732.09      3263.7      1542.4]\n","Logic frame count : 74\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 164.3ms\n","Speed: 4.2ms preprocess, 164.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1224.2166), tensor(948.0991), tensor(1376.0974), tensor(1322.7151)], tensor(0.8422), 0), ([tensor(0.6923), tensor(567.4641), tensor(732.7803), tensor(944.1531)], tensor(0.4895), 6), ([tensor(1726.1597), tensor(1162.9326), tensor(1920.6235), tensor(1440.)], tensor(0.4409), 1), ([tensor(1262.7212), tensor(729.1019), tensor(1353.4910), tensor(961.7233)], tensor(0.4406), 0)]\n","Class motorcycle : ID 1: Confidence None : Bounding Box [     1514.6      1026.1      3960.3      2514.4]\n","Class person : ID 2: Confidence 0.8421517610549927 : Bounding Box [     1164.4      947.61      2671.2      2270.3]\n","Class train : ID 3: Confidence 0.48951637744903564 : Bounding Box [   0.085689      567.68      733.75      1512.1]\n","Class person : ID 16: Confidence 0.44061970710754395 : Bounding Box [     1264.6      744.25        2662      1718.1]\n","Class person : ID 19: Confidence None : Bounding Box [     1769.7      1046.5      3682.3      2222.6]\n","Class bicycle : ID 20: Confidence 0.44094786047935486 : Bounding Box [     1725.3      1162.9      3646.7      2602.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.6      732.09      3263.7      1542.4]\n","Logic frame count : 75\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 159.2ms\n","Speed: 4.5ms preprocess, 159.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1199.9526), tensor(946.0808), tensor(1371.9556), tensor(1316.6646)], tensor(0.8728), 0), ([tensor(1726.2676), tensor(1162.5514), tensor(1922.4478), tensor(1440.)], tensor(0.4735), 1), ([tensor(0.6378), tensor(567.7598), tensor(733.0356), tensor(944.5652)], tensor(0.4718), 6)]\n","Class motorcycle : ID 1: Confidence None : Bounding Box [     1497.6      1028.3      3972.6      2534.4]\n","Class person : ID 2: Confidence 0.8728433847427368 : Bounding Box [     1144.1      945.84      2633.4      2263.9]\n","Class train : ID 3: Confidence 0.4718244671821594 : Bounding Box [    0.20923      567.75      733.91      1512.3]\n","Class person : ID 16: Confidence None : Bounding Box [     1263.2      742.65      2660.4      1716.4]\n","Class person : ID 19: Confidence None : Bounding Box [     1767.8      1050.1      3678.6      2225.1]\n","Class bicycle : ID 20: Confidence 0.4734817445278168 : Bounding Box [     1726.3      1162.9      3647.8      2602.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.7       732.1      3263.8      1542.4]\n","Logic frame count : 76\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 165.2ms\n","Speed: 4.7ms preprocess, 165.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1190.1626), tensor(945.4752), tensor(1360.4358), tensor(1314.7701)], tensor(0.8711), 0), ([tensor(1726.3250), tensor(1164.5886), tensor(1920.8315), tensor(1440.)], tensor(0.4769), 1), ([tensor(0.6707), tensor(567.8796), tensor(733.2894), tensor(944.0688)], tensor(0.4619), 6), ([tensor(1794.9390), tensor(1056.7717), tensor(1866.7480), tensor(1173.3904)], tensor(0.3089), 0)]\n","Class motorcycle : ID 1: Confidence None : Bounding Box [     1480.6      1030.5      3984.8      2554.4]\n","Class person : ID 2: Confidence 0.871126651763916 : Bounding Box [     1131.4      944.86      2605.5        2260]\n","Class train : ID 3: Confidence 0.4618804454803467 : Bounding Box [    0.48029      567.85      733.96      1512.1]\n","Class person : ID 16: Confidence None : Bounding Box [     1261.9      741.05      2658.9      1714.6]\n","Class person : ID 19: Confidence 0.3088656961917877 : Bounding Box [     1776.3      1056.4      3678.6      2229.9]\n","Class bicycle : ID 20: Confidence 0.47691982984542847 : Bounding Box [     1726.2      1164.2      3647.6      2604.2]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.8       732.1      3263.9      1542.4]\n","Logic frame count : 77\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 1 umbrella, 183.1ms\n","Speed: 4.5ms preprocess, 183.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1191.5121), tensor(948.6453), tensor(1358.5858), tensor(1312.5098)], tensor(0.8893), 0), ([tensor(1726.3308), tensor(1166.4038), tensor(1920.3274), tensor(1440.)], tensor(0.5345), 1), ([tensor(0.6997), tensor(568.2899), tensor(733.7821), tensor(944.4996)], tensor(0.4603), 6), ([tensor(1796.4244), tensor(1058.2920), tensor(1863.3524), tensor(1176.1211)], tensor(0.2793), 0), ([tensor(1569.7212), tensor(732.1417), tensor(1694.9126), tensor(810.1598)], tensor(0.2508), 25)]\n","Class motorcycle : ID 1: Confidence None : Bounding Box [     1463.6      1032.7      3997.1      2574.4]\n","Class person : ID 2: Confidence 0.8893305063247681 : Bounding Box [     1131.8      946.67      2592.3      2259.3]\n","Class train : ID 3: Confidence 0.46029412746429443 : Bounding Box [     0.6498      568.16      734.28      1512.6]\n","Class person : ID 16: Confidence None : Bounding Box [     1260.6      739.45      2657.3      1712.8]\n","Class person : ID 19: Confidence 0.2792949974536896 : Bounding Box [     1777.8      1058.9      3676.7      2233.9]\n","Class bicycle : ID 20: Confidence 0.5345380306243896 : Bounding Box [     1726.1      1165.8      3647.4      2605.8]\n","Class umbrella : ID 21: Confidence 0.2507622539997101 : Bounding Box [     1570.6      732.14      3263.7      1542.3]\n","Logic frame count : 78\n","Analyzing image for objects...\n","\n","0: 384x640 4 persons, 1 bicycle, 1 train, 176.2ms\n","Speed: 4.5ms preprocess, 176.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1188.2334), tensor(944.9140), tensor(1358.9990), tensor(1314.1528)], tensor(0.8850), 0), ([tensor(0.7198), tensor(568.5378), tensor(733.3170), tensor(944.2089)], tensor(0.4562), 6), ([tensor(1726.0077), tensor(1167.0771), tensor(1921.2572), tensor(1440.)], tensor(0.4555), 1), ([tensor(1779.9463), tensor(1058.3894), tensor(1891.0791), tensor(1257.7898)], tensor(0.2979), 0), ([tensor(1779.5757), tensor(1057.7878), tensor(1878.4028), tensor(1192.4851)], tensor(0.2904), 0), ([tensor(1790.6260), tensor(1058.0874), tensor(1867.2300), tensor(1173.5806)], tensor(0.2531), 0)]\n","Class person : ID 1: Confidence 0.29044440388679504 : Bounding Box [     1726.3      1056.1      3712.6      2275.5]\n","Class person : ID 2: Confidence 0.884996771812439 : Bounding Box [       1133      944.96      2583.9      2257.8]\n","Class train : ID 3: Confidence 0.45619773864746094 : Bounding Box [    0.65734      568.43      734.17      1512.7]\n","Class person : ID 16: Confidence None : Bounding Box [     1259.3      737.84      2655.7        1711]\n","Class person : ID 19: Confidence 0.2531464993953705 : Bounding Box [       1778      1059.6      3671.3      2233.4]\n","Class bicycle : ID 20: Confidence 0.4555476903915405 : Bounding Box [     1726.1      1166.9      3647.4      2606.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.7      732.15      3263.8      1542.3]\n","Logic frame count : 79\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 train, 160.0ms\n","Speed: 6.3ms preprocess, 160.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1175.9885), tensor(945.9353), tensor(1360.3413), tensor(1309.6616)], tensor(0.8865), 0), ([tensor(0.7213), tensor(567.8317), tensor(733.0195), tensor(944.1090)], tensor(0.4672), 6), ([tensor(1725.8044), tensor(1165.5044), tensor(1921.9329), tensor(1440.)], tensor(0.3456), 1), ([tensor(1783.0437), tensor(1058.1945), tensor(1875.2766), tensor(1173.5707)], tensor(0.3179), 0)]\n","Class person : ID 1: Confidence 0.3178545832633972 : Bounding Box [       1759      1058.6      3680.2      2242.1]\n","Class person : ID 2: Confidence 0.8864638209342957 : Bounding Box [     1130.5      945.05        2570      2255.1]\n","Class train : ID 3: Confidence 0.46722519397735596 : Bounding Box [    0.60249      568.07      733.99      1512.2]\n","Class person : ID 16: Confidence None : Bounding Box [     1257.9      736.24      2654.1      1709.2]\n","Class person : ID 19: Confidence None : Bounding Box [     1776.7      1063.1      3668.5      2235.9]\n","Class bicycle : ID 20: Confidence 0.3455739915370941 : Bounding Box [     1726.1      1166.2      3647.5      2606.2]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.8      732.15      3263.8      1542.3]\n","Logic frame count : 80\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 159.1ms\n","Speed: 4.4ms preprocess, 159.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1175.6245), tensor(940.0432), tensor(1357.5796), tensor(1309.3420)], tensor(0.8724), 0), ([tensor(0.7087), tensor(567.5896), tensor(733.0217), tensor(943.8770)], tensor(0.4702), 6), ([tensor(1725.6697), tensor(1169.2916), tensor(1908.8596), tensor(1440.)], tensor(0.3073), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1764.1      1062.2      3669.9      2236.2]\n","Class person : ID 2: Confidence 0.8723835349082947 : Bounding Box [     1131.6      941.25        2562      2250.1]\n","Class train : ID 3: Confidence 0.470181405544281 : Bounding Box [    0.63386      567.77      733.86      1511.8]\n","Class person : ID 16: Confidence None : Bounding Box [     1256.6      734.64      2652.5      1707.4]\n","Class person : ID 19: Confidence None : Bounding Box [     1775.5      1066.5      3665.8      2238.4]\n","Class bicycle : ID 20: Confidence 0.3072735667228699 : Bounding Box [     1722.3      1168.4      3642.5      2608.4]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.8      732.16      3263.9      1542.3]\n","Logic frame count : 81\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 166.4ms\n","Speed: 4.7ms preprocess, 166.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1166.0698), tensor(940.7465), tensor(1346.0962), tensor(1310.6559)], tensor(0.8736), 0), ([tensor(0.7257), tensor(567.9269), tensor(732.9761), tensor(944.0272)], tensor(0.4658), 6), ([tensor(1725.4612), tensor(1163.6083), tensor(1919.6204), tensor(1440.)], tensor(0.3271), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1769.2      1065.7      3659.6      2230.2]\n","Class person : ID 2: Confidence 0.8735778331756592 : Bounding Box [     1124.7      940.34      2547.4      2249.7]\n","Class train : ID 3: Confidence 0.4657690227031708 : Bounding Box [    0.61458      567.88      733.83      1511.9]\n","Class person : ID 16: Confidence None : Bounding Box [     1255.3      733.03      2650.9      1705.6]\n","Class person : ID 19: Confidence None : Bounding Box [     1774.2        1070      3663.1        2241]\n","Class bicycle : ID 20: Confidence 0.3271424472332001 : Bounding Box [     1724.1      1165.5      3644.2      2605.5]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.9      732.17      3263.9      1542.3]\n","Logic frame count : 82\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 156.4ms\n","Speed: 4.5ms preprocess, 156.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1161.4214), tensor(939.5574), tensor(1331.9961), tensor(1308.8110)], tensor(0.8518), 0), ([tensor(0.7330), tensor(567.5759), tensor(732.9180), tensor(943.9865)], tensor(0.4573), 6), ([tensor(1725.5310), tensor(1162.2300), tensor(1920.7419), tensor(1440.)], tensor(0.2566), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1774.3      1069.3      3649.2      2224.3]\n","Class person : ID 2: Confidence 0.8518145084381104 : Bounding Box [     1118.3      939.28      2531.5      2247.7]\n","Class train : ID 3: Confidence 0.45730146765708923 : Bounding Box [     0.6119      567.68      733.79      1511.7]\n","Class person : ID 16: Confidence None : Bounding Box [       1254      731.43      2649.3      1703.8]\n","Class person : ID 19: Confidence None : Bounding Box [       1773      1073.4      3660.3      2243.5]\n","Class bicycle : ID 20: Confidence 0.25655269622802734 : Bounding Box [     1725.2      1163.5      3645.4      2603.5]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.9      732.18      3263.9      1542.3]\n","Logic frame count : 83\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 173.6ms\n","Speed: 9.6ms preprocess, 173.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1162.7142), tensor(939.4933), tensor(1332.7206), tensor(1306.5114)], tensor(0.8358), 0), ([tensor(0.7219), tensor(567.5254), tensor(733.0133), tensor(943.8462)], tensor(0.4493), 6)]\n","Class person : ID 1: Confidence None : Bounding Box [     1779.4      1072.9      3638.9      2218.3]\n","Class person : ID 2: Confidence 0.8357917666435242 : Bounding Box [     1120.6      938.89      2524.2      2245.5]\n","Class train : ID 3: Confidence 0.4493110179901123 : Bounding Box [    0.66873      567.58      733.76      1511.5]\n","Class person : ID 16: Confidence None : Bounding Box [     1252.6      729.83      2647.7        1702]\n","Class person : ID 19: Confidence None : Bounding Box [     1771.8      1076.9      3657.6        2246]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1725.2      1163.5      3645.4      2603.5]\n","Class umbrella : ID 21: Confidence None : Bounding Box [       1571      732.18        3264      1542.3]\n","Logic frame count : 84\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 150.1ms\n","Speed: 4.6ms preprocess, 150.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1162.7043), tensor(940.0837), tensor(1324.5032), tensor(1307.3162)], tensor(0.8213), 0), ([tensor(0.), tensor(569.5049), tensor(734.7180), tensor(942.3116)], tensor(0.4400), 6), ([tensor(1727.8693), tensor(1016.6531), tensor(2033.0660), tensor(1439.0913)], tensor(0.2607), 3)]\n","Class person : ID 1: Confidence None : Bounding Box [     1784.5      1076.4      3628.5      2212.4]\n","Class person : ID 2: Confidence 0.8212548494338989 : Bounding Box [     1121.4       939.2      2517.3      2245.7]\n","Class train : ID 3: Confidence 0.43998658657073975 : Bounding Box [     1.0338      568.85      733.58      1511.7]\n","Class person : ID 16: Confidence None : Bounding Box [     1251.3      728.23      2646.1      1700.2]\n","Class person : ID 19: Confidence None : Bounding Box [     1770.5      1080.3      3654.9      2248.5]\n","Class motorcycle : ID 20: Confidence 0.260670930147171 : Bounding Box [     1765.9      1049.1      3696.9      2488.4]\n","Class umbrella : ID 21: Confidence None : Bounding Box [       1571      732.19        3264      1542.3]\n","Logic frame count : 85\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 boat, 1 umbrella, 150.0ms\n","Speed: 8.9ms preprocess, 150.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1161.4561), tensor(937.8698), tensor(1326.4875), tensor(1304.3806)], tensor(0.8349), 0), ([tensor(0.), tensor(569.6412), tensor(734.2791), tensor(941.9301)], tensor(0.4288), 6), ([tensor(1569.5903), tensor(732.6116), tensor(1697.9619), tensor(809.9751)], tensor(0.2809), 25), ([tensor(1725.3818), tensor(1158.4247), tensor(1921.9341), tensor(1440.)], tensor(0.2784), 1), ([tensor(1273.6145), tensor(764.2045), tensor(1534.0222), tensor(1034.4379)], tensor(0.2768), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1789.6        1080      3618.2      2206.4]\n","Class person : ID 2: Confidence 0.834891140460968 : Bounding Box [     1124.8       937.9      2512.3      2242.5]\n","Class train : ID 3: Confidence 0.42882096767425537 : Bounding Box [     1.0594      569.41      733.34      1511.6]\n","Class boat : ID 16: Confidence 0.27676403522491455 : Bounding Box [     1292.4      763.25      2784.1      1796.1]\n","Class person : ID 19: Confidence None : Bounding Box [     1769.3      1083.8      3652.2      2251.1]\n","Class bicycle : ID 20: Confidence 0.2784058153629303 : Bounding Box [     1737.2      1119.1      3667.8      2558.9]\n","Class umbrella : ID 21: Confidence 0.2808515131473541 : Bounding Box [     1571.7      732.59      3265.4      1542.6]\n","Logic frame count : 86\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 boat, 1 umbrella, 172.3ms\n","Speed: 4.5ms preprocess, 172.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1158.1082), tensor(937.4948), tensor(1325.3250), tensor(1304.6760)], tensor(0.8401), 0), ([tensor(1302.4331), tensor(795.1541), tensor(1536.2070), tensor(1032.5975)], tensor(0.4520), 8), ([tensor(0.), tensor(569.9563), tensor(733.8423), tensor(941.7576)], tensor(0.4165), 6), ([tensor(1569.6052), tensor(732.7657), tensor(1698.3845), tensor(810.0764)], tensor(0.2950), 25), ([tensor(1725.2891), tensor(1160.3618), tensor(1922.8521), tensor(1440.)], tensor(0.2917), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1794.6      1083.6      3607.8      2200.5]\n","Class person : ID 2: Confidence 0.840106189250946 : Bounding Box [     1125.6      937.21      2506.6      2241.3]\n","Class train : ID 3: Confidence 0.41650065779685974 : Bounding Box [    0.92011      569.83       733.1      1511.7]\n","Class boat : ID 16: Confidence 0.45203155279159546 : Bounding Box [     1311.1      785.15      2812.2      1819.1]\n","Class person : ID 19: Confidence None : Bounding Box [       1768      1087.2      3649.4      2253.6]\n","Class bicycle : ID 20: Confidence 0.29167354106903076 : Bounding Box [     1727.5      1145.4      3657.5      2585.3]\n","Class umbrella : ID 21: Confidence 0.29504460096359253 : Bounding Box [     1571.5      732.72        3266      1542.8]\n","Logic frame count : 87\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 motorcycle, 1 train, 1 umbrella, 157.4ms\n","Speed: 4.4ms preprocess, 157.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1153.0332), tensor(939.4545), tensor(1324.4590), tensor(1304.6875)], tensor(0.8583), 0), ([tensor(0.), tensor(569.8928), tensor(733.9135), tensor(941.5168)], tensor(0.4109), 6), ([tensor(1725.4263), tensor(1158.9460), tensor(1921.6143), tensor(1440.)], tensor(0.2945), 1), ([tensor(1569.6187), tensor(732.8029), tensor(1697.8564), tensor(810.0399)], tensor(0.2853), 25), ([tensor(1303.7451), tensor(821.1825), tensor(1523.5630), tensor(1027.9540)], tensor(0.2674), 3)]\n","Class person : ID 1: Confidence None : Bounding Box [     1799.7      1087.1      3597.5      2194.6]\n","Class person : ID 2: Confidence 0.8582502007484436 : Bounding Box [     1124.3       938.3      2499.7      2242.3]\n","Class train : ID 3: Confidence 0.4109361171722412 : Bounding Box [    0.89056      569.94      733.04      1511.5]\n","Class motorcycle : ID 16: Confidence 0.26741042733192444 : Bounding Box [     1315.4      809.83      2817.4      1841.1]\n","Class person : ID 19: Confidence None : Bounding Box [     1766.8      1090.6      3646.7      2256.1]\n","Class bicycle : ID 20: Confidence 0.29445284605026245 : Bounding Box [     1723.7      1154.4        3653      2594.3]\n","Class umbrella : ID 21: Confidence 0.28525373339653015 : Bounding Box [     1571.1      732.79      3266.2      1542.8]\n","Logic frame count : 88\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 boat, 1 umbrella, 160.2ms\n","Speed: 8.1ms preprocess, 160.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1150.0291), tensor(938.2428), tensor(1321.9568), tensor(1303.6813)], tensor(0.8596), 0), ([tensor(0.), tensor(570.0312), tensor(733.7979), tensor(941.3884)], tensor(0.4054), 6), ([tensor(1300.6449), tensor(795.3647), tensor(1534.8575), tensor(1031.8662)], tensor(0.2784), 8), ([tensor(1569.7006), tensor(732.6680), tensor(1698.2645), tensor(810.0224)], tensor(0.2750), 25), ([tensor(1725.7786), tensor(1159.4978), tensor(1921.4839), tensor(1440.)], tensor(0.2633), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1804.8      1090.7      3587.1      2188.6]\n","Class person : ID 2: Confidence 0.8595718145370483 : Bounding Box [     1123.2      937.95      2492.9      2241.3]\n","Class train : ID 3: Confidence 0.40542852878570557 : Bounding Box [    0.82331      570.07         733      1511.4]\n","Class boat : ID 16: Confidence 0.2784174978733063 : Bounding Box [     1315.4      802.02      2823.9      1834.7]\n","Class person : ID 19: Confidence None : Bounding Box [     1765.5      1094.1        3644      2258.6]\n","Class bicycle : ID 20: Confidence 0.2633377015590668 : Bounding Box [     1722.7      1158.1      3651.2      2598.1]\n","Class umbrella : ID 21: Confidence 0.27503445744514465 : Bounding Box [     1571.1      732.73      3266.6      1542.8]\n","Logic frame count : 89\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 1 umbrella, 166.9ms\n","Speed: 4.1ms preprocess, 166.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1152.2334), tensor(938.5753), tensor(1320.9189), tensor(1303.4214)], tensor(0.8641), 0), ([tensor(0.), tensor(570.1195), tensor(733.6273), tensor(941.2723)], tensor(0.4001), 6), ([tensor(1569.6172), tensor(732.7119), tensor(1697.4697), tensor(809.9252)], tensor(0.2953), 25), ([tensor(1300.8394), tensor(799.6407), tensor(1532.7305), tensor(1031.7423)], tensor(0.2936), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1809.9      1094.3      3576.8      2182.7]\n","Class person : ID 2: Confidence 0.864113450050354 : Bounding Box [     1125.7      938.07      2490.3      2241.1]\n","Class train : ID 3: Confidence 0.40006113052368164 : Bounding Box [    0.72865      570.16      732.94      1511.4]\n","Class boat : ID 16: Confidence 0.29356545209884644 : Bounding Box [     1313.9       801.7      2826.3      1834.8]\n","Class person : ID 19: Confidence None : Bounding Box [     1764.3      1097.5      3641.3      2261.2]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1722.2      1159.5      3650.7      2599.5]\n","Class umbrella : ID 21: Confidence 0.2953231930732727 : Bounding Box [     1570.7      732.73      3266.4      1542.7]\n","Logic frame count : 90\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 1 umbrella, 159.2ms\n","Speed: 4.5ms preprocess, 159.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1145.5803), tensor(938.3308), tensor(1317.5676), tensor(1303.3660)], tensor(0.8760), 0), ([tensor(0.), tensor(570.3761), tensor(733.2073), tensor(941.4481)], tensor(0.4046), 6), ([tensor(1300.1316), tensor(803.7399), tensor(1532.1401), tensor(1031.4554)], tensor(0.3785), 8), ([tensor(1569.7982), tensor(732.5507), tensor(1697.5253), tensor(810.0319)], tensor(0.2507), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [       1815      1097.9      3566.4      2176.7]\n","Class person : ID 2: Confidence 0.8759588599205017 : Bounding Box [     1122.9      937.99      2482.8      2240.9]\n","Class train : ID 3: Confidence 0.4046110212802887 : Bounding Box [     0.4953      570.36      732.84      1511.7]\n","Class boat : ID 16: Confidence 0.3785436749458313 : Bounding Box [     1311.9      804.15      2826.8        1837]\n","Class person : ID 19: Confidence None : Bounding Box [     1763.1        1101      3638.5      2263.7]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1721.6      1160.8      3650.1      2600.8]\n","Class umbrella : ID 21: Confidence 0.2506610155105591 : Bounding Box [     1570.6      732.62      3266.5      1542.6]\n","Logic frame count : 91\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 173.7ms\n","Speed: 10.0ms preprocess, 173.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1143.7024), tensor(937.0168), tensor(1318.0989), tensor(1304.1389)], tensor(0.8800), 0), ([tensor(0.), tensor(570.3143), tensor(733.5798), tensor(941.3885)], tensor(0.4023), 6), ([tensor(1569.5957), tensor(732.6302), tensor(1697.2266), tensor(809.7275)], tensor(0.2795), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1820.1      1101.4      3556.1      2170.8]\n","Class person : ID 2: Confidence 0.8799974918365479 : Bounding Box [       1122      937.12      2478.3      2240.5]\n","Class train : ID 3: Confidence 0.4023171067237854 : Bounding Box [     0.5037      570.39      732.96      1511.7]\n","Class boat : ID 16: Confidence None : Bounding Box [     1314.9      807.29      2833.2      1842.5]\n","Class person : ID 19: Confidence None : Bounding Box [     1761.8      1104.4      3635.8      2266.2]\n","Class bicycle : ID 20: Confidence None : Bounding Box [       1721      1162.2      3649.6      2602.2]\n","Class umbrella : ID 21: Confidence 0.279476135969162 : Bounding Box [     1570.5      732.63      3266.2      1542.4]\n","Logic frame count : 92\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 162.6ms\n","Speed: 5.2ms preprocess, 162.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1141.6035), tensor(936.5427), tensor(1312.8779), tensor(1304.2722)], tensor(0.8872), 0), ([tensor(0.), tensor(569.8147), tensor(731.0981), tensor(941.3518)], tensor(0.4033), 6), ([tensor(1569.6423), tensor(733.0222), tensor(1696.7400), tensor(809.6953)], tensor(0.3264), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1825.2        1105      3545.7      2164.8]\n","Class person : ID 2: Confidence 0.8872366547584534 : Bounding Box [     1120.1       936.5      2472.5      2240.2]\n","Class train : ID 3: Confidence 0.40325403213500977 : Bounding Box [   -0.21691      570.06      732.09      1511.3]\n","Class boat : ID 16: Confidence None : Bounding Box [     1317.9      810.44      2839.6        1848]\n","Class person : ID 19: Confidence None : Bounding Box [     1760.6      1107.9      3633.1      2268.7]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1720.5      1163.5        3649      2603.6]\n","Class umbrella : ID 21: Confidence 0.32637476921081543 : Bounding Box [     1570.3       732.9        3266      1542.6]\n","Logic frame count : 93\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 161.7ms\n","Speed: 5.0ms preprocess, 161.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1137.3929), tensor(936.8088), tensor(1311.8734), tensor(1304.6748)], tensor(0.8864), 0), ([tensor(0.), tensor(569.4336), tensor(731.3076), tensor(941.4999)], tensor(0.4100), 6), ([tensor(1569.6925), tensor(733.0267), tensor(1696.1993), tensor(809.7674)], tensor(0.3183), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1830.3      1108.6      3535.4      2158.9]\n","Class person : ID 2: Confidence 0.886360228061676 : Bounding Box [     1117.6      936.47      2466.4      2240.6]\n","Class train : ID 3: Confidence 0.41000860929489136 : Bounding Box [   -0.42709      569.68      731.84      1511.1]\n","Class boat : ID 16: Confidence None : Bounding Box [     1320.9      813.59        2846      1853.4]\n","Class person : ID 19: Confidence None : Bounding Box [     1759.3      1111.3      3630.4      2271.3]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1719.9      1164.9      3648.5      2604.9]\n","Class umbrella : ID 21: Confidence 0.31828129291534424 : Bounding Box [       1570         733      3265.8      1542.7]\n","Logic frame count : 94\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 1 umbrella, 162.1ms\n","Speed: 4.5ms preprocess, 162.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1131.8599), tensor(937.5001), tensor(1307.8440), tensor(1305.3629)], tensor(0.8882), 0), ([tensor(0.), tensor(569.1305), tensor(731.5917), tensor(941.1732)], tensor(0.4000), 6), ([tensor(1569.8848), tensor(732.8901), tensor(1696.8513), tensor(809.7849)], tensor(0.2915), 25), ([tensor(1305.9738), tensor(822.5536), tensor(1520.8387), tensor(1026.8143)], tensor(0.2692), 3)]\n","Class person : ID 1: Confidence None : Bounding Box [     1835.4      1112.1        3525      2152.9]\n","Class person : ID 2: Confidence 0.888171911239624 : Bounding Box [     1112.9      936.94      2458.4      2241.7]\n","Class train : ID 3: Confidence 0.40000343322753906 : Bounding Box [   -0.32196      569.34      731.76      1510.5]\n","Class motorcycle : ID 16: Confidence 0.26920270919799805 : Bounding Box [     1313.6      821.88      2824.2      1850.5]\n","Class person : ID 19: Confidence None : Bounding Box [     1758.1      1114.8      3627.6      2273.8]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1719.3      1166.2      3647.9      2606.3]\n","Class umbrella : ID 21: Confidence 0.29145348072052 : Bounding Box [     1570.2      732.94      3266.1      1542.7]\n","Logic frame count : 95\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 159.9ms\n","Speed: 4.6ms preprocess, 159.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1126.3169), tensor(937.5774), tensor(1304.5112), tensor(1305.9990)], tensor(0.8814), 0), ([tensor(0.), tensor(569.1349), tensor(731.9551), tensor(941.5011)], tensor(0.3912), 6), ([tensor(1304.2146), tensor(797.0842), tensor(1531.8948), tensor(1032.1648)], tensor(0.3106), 8)]\n","Class person : ID 1: Confidence None : Bounding Box [     1840.5      1115.7      3514.7        2147]\n","Class person : ID 2: Confidence 0.8814164400100708 : Bounding Box [     1107.7      937.19      2449.8      2242.6]\n","Class train : ID 3: Confidence 0.3912138044834137 : Bounding Box [   -0.23519      569.21      731.94      1510.6]\n","Class boat : ID 16: Confidence 0.3106034994125366 : Bounding Box [     1312.3      805.92      2829.1      1837.3]\n","Class person : ID 19: Confidence None : Bounding Box [     1756.8      1118.2      3624.9      2276.3]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1718.8      1167.6      3647.4      2607.6]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.2      732.98      3266.1      1542.7]\n","Logic frame count : 96\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 186.4ms\n","Speed: 4.4ms preprocess, 186.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1122.0962), tensor(937.1270), tensor(1294.2114), tensor(1308.6987)], tensor(0.8526), 0), ([tensor(0.), tensor(568.7075), tensor(732.0529), tensor(941.1428)], tensor(0.3923), 6), ([tensor(1569.9729), tensor(733.6345), tensor(1695.2971), tensor(809.9980)], tensor(0.3307), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1845.6      1119.3      3504.3        2141]\n","Class person : ID 2: Confidence 0.8525545597076416 : Bounding Box [     1100.6      937.01      2439.8      2244.5]\n","Class train : ID 3: Confidence 0.39228498935699463 : Bounding Box [  -0.084217      568.87      731.96      1510.1]\n","Class boat : ID 16: Confidence None : Bounding Box [     1314.1      807.53      2832.8      1840.1]\n","Class person : ID 19: Confidence None : Bounding Box [     1755.6      1121.6      3622.2      2278.8]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1718.2      1168.9      3646.8        2609]\n","Class umbrella : ID 21: Confidence 0.33074915409088135 : Bounding Box [     1569.7       733.5      3265.8      1543.4]\n","Logic frame count : 97\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 1 umbrella, 226.1ms\n","Speed: 4.1ms preprocess, 226.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1120.6812), tensor(938.8317), tensor(1286.0469), tensor(1305.7885)], tensor(0.8462), 0), ([tensor(0.), tensor(568.7146), tensor(731.8558), tensor(941.2507)], tensor(0.3818), 6), ([tensor(1569.9358), tensor(733.6472), tensor(1696.0225), tensor(809.9744)], tensor(0.3133), 25), ([tensor(1307.2598), tensor(845.7056), tensor(1520.4150), tensor(1027.3865)], tensor(0.2886), 3)]\n","Class person : ID 1: Confidence None : Bounding Box [     1850.7      1122.8        3494      2135.1]\n","Class person : ID 2: Confidence 0.8462130427360535 : Bounding Box [       1097      938.08      2430.2      2244.4]\n","Class train : ID 3: Confidence 0.38178551197052 : Bounding Box [   -0.10592      568.75      731.92      1509.9]\n","Class motorcycle : ID 16: Confidence 0.2886374294757843 : Bounding Box [     1312.2      837.65      2826.5      1866.5]\n","Class person : ID 19: Confidence None : Bounding Box [     1754.4      1125.1      3619.5      2281.4]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1717.6      1170.3      3646.3      2610.4]\n","Class umbrella : ID 21: Confidence 0.3132687509059906 : Bounding Box [     1569.8      733.63        3266      1543.6]\n","Logic frame count : 98\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 255.1ms\n","Speed: 4.4ms preprocess, 255.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1120.5386), tensor(936.1099), tensor(1276.9827), tensor(1306.0923)], tensor(0.8580), 0), ([tensor(0.), tensor(568.2130), tensor(732.2904), tensor(941.1874)], tensor(0.3649), 6), ([tensor(1569.8889), tensor(733.3877), tensor(1695.2278), tensor(810.2310)], tensor(0.3264), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1855.8      1126.4      3483.6      2129.2]\n","Class person : ID 2: Confidence 0.8580297827720642 : Bounding Box [     1094.5       936.7      2422.1      2242.8]\n","Class train : ID 3: Confidence 0.36490485072135925 : Bounding Box [    0.02999      568.38      732.05      1509.5]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1313.8      842.24      2829.1      1871.7]\n","Class person : ID 19: Confidence None : Bounding Box [     1753.1      1128.5      3616.7      2283.9]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1717.1      1171.6      3645.7      2611.7]\n","Class umbrella : ID 21: Confidence 0.32635927200317383 : Bounding Box [     1569.4       733.5      3265.8      1543.6]\n","Logic frame count : 99\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 216.2ms\n","Speed: 4.0ms preprocess, 216.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1118.6313), tensor(935.7014), tensor(1272.5244), tensor(1305.4470)], tensor(0.8591), 0), ([tensor(0.), tensor(568.3263), tensor(731.9623), tensor(941.1620)], tensor(0.3614), 6), ([tensor(1569.8119), tensor(733.3824), tensor(1695.6871), tensor(810.2201)], tensor(0.3254), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1860.8        1130      3473.3      2123.2]\n","Class person : ID 2: Confidence 0.8591225147247314 : Bounding Box [     1092.9      935.92      2414.7      2241.5]\n","Class train : ID 3: Confidence 0.36141955852508545 : Bounding Box [  -0.011539      568.32      731.98      1509.4]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1315.3      846.83      2831.7        1877]\n","Class person : ID 19: Confidence None : Bounding Box [     1751.9        1132        3614      2286.4]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1716.5        1173      3645.2      2613.1]\n","Class umbrella : ID 21: Confidence 0.3254322111606598 : Bounding Box [     1569.4      733.45      3265.9      1543.6]\n","Logic frame count : 100\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 1 umbrella, 250.7ms\n","Speed: 6.7ms preprocess, 250.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1118.7074), tensor(931.5258), tensor(1273.1349), tensor(1305.0690)], tensor(0.8726), 0), ([tensor(0.), tensor(568.1416), tensor(732.0399), tensor(941.1633)], tensor(0.3485), 6), ([tensor(1569.8843), tensor(733.5078), tensor(1696.6982), tensor(810.1009)], tensor(0.3168), 25), ([tensor(1307.5486), tensor(855.3774), tensor(1520.8816), tensor(1028.2128)], tensor(0.2673), 3)]\n","Class person : ID 1: Confidence None : Bounding Box [     1865.9      1133.5      3462.9      2117.3]\n","Class person : ID 2: Confidence 0.8726344704627991 : Bounding Box [     1094.2      932.88        2411      2238.1]\n","Class train : ID 3: Confidence 0.34849312901496887 : Bounding Box [ -0.0033831      568.17      731.99      1509.3]\n","Class motorcycle : ID 16: Confidence 0.26726415753364563 : Bounding Box [     1311.6      854.78      2826.7      1883.4]\n","Class person : ID 19: Confidence None : Bounding Box [     1750.6      1135.4      3611.3      2288.9]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1715.9      1174.3      3644.6      2614.4]\n","Class umbrella : ID 21: Confidence 0.3167817294597626 : Bounding Box [     1569.8      733.51      3266.2      1543.6]\n","Logic frame count : 101\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 251.4ms\n","Speed: 6.2ms preprocess, 251.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1116.5565), tensor(928.7172), tensor(1273.5316), tensor(1306.0314)], tensor(0.8768), 0), ([tensor(0.), tensor(568.3217), tensor(731.8574), tensor(941.0140)], tensor(0.3405), 6), ([tensor(1569.9930), tensor(733.3497), tensor(1697.2189), tensor(810.2870)], tensor(0.2798), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [       1871      1137.1      3452.6      2111.3]\n","Class person : ID 2: Confidence 0.8767626285552979 : Bounding Box [     1094.6      929.89      2407.6      2235.6]\n","Class train : ID 3: Confidence 0.340548574924469 : Bounding Box [  -0.018892      568.24      731.89      1509.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1312.7       859.7      2828.4      1888.8]\n","Class person : ID 19: Confidence None : Bounding Box [     1749.4      1138.9      3608.6      2291.5]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1715.4      1175.6      3644.1      2615.8]\n","Class umbrella : ID 21: Confidence 0.2798087000846863 : Bounding Box [     1570.1      733.42      3266.7      1543.7]\n","Logic frame count : 102\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 train, 1 umbrella, 239.6ms\n","Speed: 7.2ms preprocess, 239.6ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1113.6821), tensor(925.7324), tensor(1272.3918), tensor(1301.6660)], tensor(0.8854), 0), ([tensor(0.), tensor(568.0037), tensor(731.7205), tensor(941.3010)], tensor(0.3400), 6), ([tensor(1315.4443), tensor(743.1085), tensor(1443.6753), tensor(896.4590)], tensor(0.3012), 0), ([tensor(1569.9810), tensor(733.2985), tensor(1697.0767), tensor(810.2618)], tensor(0.2769), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1876.1      1140.7      3442.2      2105.4]\n","Class person : ID 2: Confidence 0.8854039907455444 : Bounding Box [     1095.1      926.82      2401.9      2229.8]\n","Class train : ID 3: Confidence 0.33999088406562805 : Bounding Box [   -0.12479      568.06      731.87      1509.2]\n","Class person : ID 16: Confidence 0.30118557810783386 : Bounding Box [     1355.1      768.54        2734      1692.9]\n","Class person : ID 19: Confidence None : Bounding Box [     1748.2      1142.3      3605.8        2294]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1714.8        1177      3643.5      2617.2]\n","Class umbrella : ID 21: Confidence 0.27691200375556946 : Bounding Box [     1570.1      733.35      3266.9      1543.6]\n","Logic frame count : 103\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 236.0ms\n","Speed: 6.5ms preprocess, 236.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1115.7937), tensor(922.5976), tensor(1265.1560), tensor(1297.8442)], tensor(0.8769), 0), ([tensor(0.), tensor(568.2015), tensor(731.2850), tensor(941.6710)], tensor(0.3464), 6), ([tensor(1569.9771), tensor(733.6193), tensor(1698.1353), tensor(810.2769)], tensor(0.2874), 25), ([tensor(1725.4114), tensor(1162.4424), tensor(1918.8210), tensor(1440.)], tensor(0.2507), 1)]\n","Class person : ID 1: Confidence None : Bounding Box [     1881.2      1144.2      3431.9      2099.4]\n","Class person : ID 2: Confidence 0.8768921494483948 : Bounding Box [     1096.8      923.63      2396.6      2223.1]\n","Class train : ID 3: Confidence 0.3463614284992218 : Bounding Box [   -0.35845      568.12      731.77      1509.6]\n","Class person : ID 16: Confidence None : Bounding Box [     1361.4      763.61      2724.9      1677.6]\n","Class person : ID 19: Confidence None : Bounding Box [     1746.9      1145.7      3603.1      2296.5]\n","Class bicycle : ID 20: Confidence 0.2507299780845642 : Bounding Box [     1721.4      1162.7        3648      2602.7]\n","Class umbrella : ID 21: Confidence 0.2873729467391968 : Bounding Box [     1570.4      733.54      3267.3      1543.8]\n","Logic frame count : 104\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 254.9ms\n","Speed: 4.1ms preprocess, 254.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1106.5242), tensor(912.6157), tensor(1257.9739), tensor(1288.6890)], tensor(0.8773), 0), ([tensor(1569.8337), tensor(733.8539), tensor(1698.3875), tensor(810.2725)], tensor(0.3333), 25), ([tensor(0.), tensor(567.7473), tensor(731.3398), tensor(941.7192)], tensor(0.3310), 6)]\n","Class person : ID 1: Confidence None : Bounding Box [     1886.3      1147.8      3421.5      2093.5]\n","Class person : ID 2: Confidence 0.8773452043533325 : Bounding Box [       1093      915.87      2382.4      2207.9]\n","Class train : ID 3: Confidence 0.33099398016929626 : Bounding Box [   -0.41312      567.85      731.74      1509.5]\n","Class person : ID 16: Confidence None : Bounding Box [     1367.7      758.68      2715.8      1662.3]\n","Class person : ID 19: Confidence None : Bounding Box [     1745.7      1149.2      3600.4        2299]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1721.2      1163.1      3647.8      2603.1]\n","Class umbrella : ID 21: Confidence 0.3333410620689392 : Bounding Box [     1570.5      733.76      3267.5        1544]\n","Logic frame count : 105\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 170.3ms\n","Speed: 5.7ms preprocess, 170.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1105.2659), tensor(903.2615), tensor(1245.9592), tensor(1282.1863)], tensor(0.8587), 0), ([tensor(0.), tensor(567.9745), tensor(731.4993), tensor(941.8556)], tensor(0.3137), 6), ([tensor(1569.8735), tensor(733.7626), tensor(1698.0410), tensor(810.1993)], tensor(0.3027), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1891.4      1151.4      3411.2      2087.5]\n","Class person : ID 2: Confidence 0.8587027788162231 : Bounding Box [     1090.1       906.8      2369.3      2191.8]\n","Class train : ID 3: Confidence 0.31373095512390137 : Bounding Box [   -0.39213       567.9      731.79      1509.7]\n","Class person : ID 16: Confidence None : Bounding Box [       1374      753.74      2706.7      1647.1]\n","Class person : ID 19: Confidence None : Bounding Box [     1744.4      1152.6      3597.7      2301.6]\n","Class bicycle : ID 20: Confidence None : Bounding Box [       1721      1163.5      3647.6      2603.6]\n","Class umbrella : ID 21: Confidence 0.3026750087738037 : Bounding Box [     1570.4      733.78      3267.5        1544]\n","Logic frame count : 106\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 160.3ms\n","Speed: 5.5ms preprocess, 160.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1096.9050), tensor(902.1060), tensor(1237.5696), tensor(1268.4038)], tensor(0.8725), 0), ([tensor(0.), tensor(568.2488), tensor(731.3268), tensor(941.7468)], tensor(0.3493), 6), ([tensor(1569.9102), tensor(733.2377), tensor(1696.9897), tensor(810.0482)], tensor(0.3160), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1896.5      1154.9      3400.8      2081.6]\n","Class person : ID 2: Confidence 0.872454047203064 : Bounding Box [     1086.1      902.72      2351.1        2176]\n","Class train : ID 3: Confidence 0.34929555654525757 : Bounding Box [   -0.38613      568.11      731.71      1509.9]\n","Class person : ID 16: Confidence None : Bounding Box [     1380.3      748.81      2697.6      1631.8]\n","Class person : ID 19: Confidence None : Bounding Box [     1743.2      1156.1      3594.9      2304.1]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1720.8        1164      3647.4        2604]\n","Class umbrella : ID 21: Confidence 0.31604963541030884 : Bounding Box [     1570.2      733.44        3267      1543.6]\n","Logic frame count : 107\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 157.9ms\n","Speed: 20.2ms preprocess, 157.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1085.5419), tensor(902.9331), tensor(1230.0267), tensor(1259.8911)], tensor(0.8756), 0), ([tensor(0.), tensor(568.1963), tensor(731.8372), tensor(941.6584)], tensor(0.3586), 6), ([tensor(1569.9255), tensor(733.1012), tensor(1696.9346), tensor(810.1852)], tensor(0.3343), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1901.6      1158.5      3390.5      2075.7]\n","Class person : ID 2: Confidence 0.8755906820297241 : Bounding Box [       1078      901.85        2331      2165.2]\n","Class train : ID 3: Confidence 0.3585565984249115 : Bounding Box [   -0.19705      568.15      731.83      1509.9]\n","Class person : ID 16: Confidence None : Bounding Box [     1386.6      743.87      2688.5      1616.6]\n","Class person : ID 19: Confidence None : Bounding Box [     1741.9      1159.5      3592.2      2306.6]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1720.6      1164.4      3647.2      2604.4]\n","Class umbrella : ID 21: Confidence 0.33428651094436646 : Bounding Box [       1570      733.22      3266.9      1543.4]\n","Logic frame count : 108\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 157.9ms\n","Speed: 5.8ms preprocess, 157.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1081.5708), tensor(904.7087), tensor(1222.8779), tensor(1256.6091)], tensor(0.8640), 0), ([tensor(0.), tensor(569.3473), tensor(731.1175), tensor(941.7733)], tensor(0.3722), 6), ([tensor(1569.7107), tensor(732.8176), tensor(1695.5491), tensor(810.3730)], tensor(0.3155), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1906.7      1162.1      3380.1      2069.7]\n","Class person : ID 2: Confidence 0.8639832139015198 : Bounding Box [       1072      902.82        2317      2160.3]\n","Class train : ID 3: Confidence 0.372185617685318 : Bounding Box [   -0.34825      568.93      731.62      1510.7]\n","Class person : ID 16: Confidence None : Bounding Box [     1392.9      738.94      2679.4      1601.3]\n","Class person : ID 19: Confidence None : Bounding Box [     1740.7        1163      3589.5      2309.1]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1720.4      1164.8        3647      2604.9]\n","Class umbrella : ID 21: Confidence 0.31546467542648315 : Bounding Box [     1569.3      732.95      3266.4      1543.3]\n","Logic frame count : 109\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 169.2ms\n","Speed: 4.0ms preprocess, 169.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1069.1428), tensor(904.2803), tensor(1220.2434), tensor(1251.2568)], tensor(0.8688), 0), ([tensor(0.), tensor(569.6357), tensor(730.6434), tensor(941.4507)], tensor(0.3371), 6), ([tensor(1569.6349), tensor(732.7659), tensor(1696.3094), tensor(810.3420)], tensor(0.3173), 25)]\n","Class person : ID 1: Confidence None : Bounding Box [     1911.8      1165.7      3369.8      2063.8]\n","Class person : ID 2: Confidence 0.8687649965286255 : Bounding Box [     1063.1         903      2300.8        2155]\n","Class train : ID 3: Confidence 0.3371342420578003 : Bounding Box [   -0.44298      569.41      731.27        1511]\n","Class person : ID 16: Confidence None : Bounding Box [     1399.3         734      2670.3        1586]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1720.2      1165.3      3646.8      2605.3]\n","Class umbrella : ID 21: Confidence 0.3173438012599945 : Bounding Box [     1569.3      732.82      3266.3      1543.2]\n","Logic frame count : 110\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 160.3ms\n","Speed: 10.5ms preprocess, 160.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1059.0400), tensor(904.4373), tensor(1214.2471), tensor(1251.7773)], tensor(0.8520), 0), ([tensor(0.), tensor(569.5172), tensor(730.8474), tensor(940.9039)], tensor(0.3390), 6), ([tensor(1569.7122), tensor(733.0630), tensor(1695.9924), tensor(810.3931)], tensor(0.3305), 25), ([tensor(1725.4294), tensor(1161.6270), tensor(1918.9380), tensor(1440.)], tensor(0.2583), 1)]\n","Class person : ID 2: Confidence 0.8520039916038513 : Bounding Box [     1051.8      903.27      2285.7      2153.6]\n","Class train : ID 3: Confidence 0.3389894366264343 : Bounding Box [   -0.27143      569.52      731.07      1510.6]\n","Class person : ID 16: Confidence None : Bounding Box [     1405.6      729.07      2661.2      1570.8]\n","Class bicycle : ID 20: Confidence 0.2582909166812897 : Bounding Box [     1722.3      1161.8      3647.3      2601.8]\n","Class umbrella : ID 21: Confidence 0.3305268883705139 : Bounding Box [     1569.2      732.96      3266.2      1543.3]\n","Logic frame count : 111\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 160.4ms\n","Speed: 4.4ms preprocess, 160.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1056.1287), tensor(906.4778), tensor(1215.2671), tensor(1249.7578)], tensor(0.8556), 0), ([tensor(1725.3162), tensor(1160.4255), tensor(1919.2371), tensor(1440.)], tensor(0.3771), 1), ([tensor(0.), tensor(569.4830), tensor(730.7637), tensor(940.9174)], tensor(0.3371), 6), ([tensor(1569.6589), tensor(733.0280), tensor(1696.2668), tensor(810.5128)], tensor(0.3084), 25)]\n","Class person : ID 2: Confidence 0.8555947542190552 : Bounding Box [     1047.4      904.79      2277.8      2153.4]\n","Class train : ID 3: Confidence 0.33708909153938293 : Bounding Box [   -0.22212      569.53      730.96      1510.5]\n","Class person : ID 16: Confidence None : Bounding Box [     1411.9      724.14      2652.1      1555.5]\n","Class bicycle : ID 20: Confidence 0.3771086633205414 : Bounding Box [     1722.9      1160.9      3646.8      2600.9]\n","Class umbrella : ID 21: Confidence 0.308442622423172 : Bounding Box [     1569.2         733      3266.3      1543.5]\n","Logic frame count : 112\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 152.8ms\n","Speed: 10.7ms preprocess, 152.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1049.9814), tensor(900.1001), tensor(1209.0039), tensor(1246.0381)], tensor(0.8630), 0), ([tensor(1725.0348), tensor(1160.6666), tensor(1919.8715), tensor(1440.)], tensor(0.4075), 1), ([tensor(1569.4008), tensor(733.2115), tensor(1696.5499), tensor(810.5237)], tensor(0.3373), 25), ([tensor(0.), tensor(569.8994), tensor(730.7985), tensor(941.0725)], tensor(0.2940), 6)]\n","Class person : ID 2: Confidence 0.8629661202430725 : Bounding Box [     1041.6       901.2      2267.2      2146.8]\n","Class train : ID 3: Confidence 0.29404282569885254 : Bounding Box [   -0.21587      569.81      730.96      1510.8]\n","Class person : ID 16: Confidence None : Bounding Box [     1418.2       719.2        2643      1540.3]\n","Class bicycle : ID 20: Confidence 0.40754130482673645 : Bounding Box [     1723.3      1160.7      3646.5      2600.7]\n","Class umbrella : ID 21: Confidence 0.33729690313339233 : Bounding Box [     1569.1      733.13      3266.3      1543.6]\n","Logic frame count : 113\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 motorcycle, 1 train, 1 umbrella, 160.5ms\n","Speed: 4.0ms preprocess, 160.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1037.4817), tensor(896.7233), tensor(1200.3416), tensor(1240.3368)], tensor(0.8586), 0), ([tensor(1724.9385), tensor(1160.8357), tensor(1919.2051), tensor(1440.)], tensor(0.3822), 1), ([tensor(1569.2974), tensor(733.2679), tensor(1697.7217), tensor(810.5729)], tensor(0.3662), 25), ([tensor(0.), tensor(569.5342), tensor(730.8359), tensor(940.8169)], tensor(0.3102), 6), ([tensor(1308.5991), tensor(859.3318), tensor(1520.5845), tensor(1028.9832)], tensor(0.2696), 3)]\n","Class person : ID 2: Confidence 0.8586195111274719 : Bounding Box [       1031      897.67        2250      2138.6]\n","Class train : ID 3: Confidence 0.31017157435417175 : Bounding Box [   -0.13391      569.66      730.91      1510.5]\n","Class motorcycle : ID 16: Confidence 0.2695835530757904 : Bounding Box [     1305.1      856.61      2831.2      1881.5]\n","Class bicycle : ID 20: Confidence 0.38218557834625244 : Bounding Box [     1723.3      1160.8        3646      2600.8]\n","Class umbrella : ID 21: Confidence 0.3661642372608185 : Bounding Box [     1569.3      733.22      3266.6      1543.8]\n","Logic frame count : 114\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 164.2ms\n","Speed: 3.9ms preprocess, 164.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1026.5297), tensor(897.1599), tensor(1195.9872), tensor(1238.7629)], tensor(0.8541), 0), ([tensor(1725.1321), tensor(1160.7214), tensor(1918.8835), tensor(1440.)], tensor(0.3767), 1), ([tensor(1569.4666), tensor(733.0986), tensor(1697.7715), tensor(810.5413)], tensor(0.3704), 25), ([tensor(0.), tensor(569.8732), tensor(730.3395), tensor(940.8035)], tensor(0.3256), 6)]\n","Class person : ID 2: Confidence 0.8540780544281006 : Bounding Box [     1019.7       896.7      2234.1      2134.9]\n","Class train : ID 3: Confidence 0.32557448744773865 : Bounding Box [   -0.23479      569.83      730.69      1510.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1302.9      861.79      2836.2      1891.6]\n","Class bicycle : ID 20: Confidence 0.37666746973991394 : Bounding Box [     1723.5      1160.7      3645.6      2600.7]\n","Class umbrella : ID 21: Confidence 0.3704257309436798 : Bounding Box [     1569.6      733.14      3266.9      1543.7]\n","Logic frame count : 115\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 172.0ms\n","Speed: 4.4ms preprocess, 172.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1024.0916), tensor(900.5983), tensor(1181.0076), tensor(1233.0621)], tensor(0.8554), 0), ([tensor(1725.3237), tensor(1160.7000), tensor(1918.9763), tensor(1440.)], tensor(0.3582), 1), ([tensor(0.), tensor(569.9335), tensor(730.4485), tensor(940.8917)], tensor(0.3299), 6), ([tensor(1569.6062), tensor(732.7674), tensor(1697.2629), tensor(810.5986)], tensor(0.3173), 25)]\n","Class person : ID 2: Confidence 0.855402946472168 : Bounding Box [       1012      898.69      2219.1      2132.2]\n","Class train : ID 3: Confidence 0.329935222864151 : Bounding Box [   -0.24423      569.93      730.66      1510.8]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1300.6      866.97      2841.2      1901.6]\n","Class bicycle : ID 20: Confidence 0.35818225145339966 : Bounding Box [     1723.8      1160.7      3645.6      2600.7]\n","Class umbrella : ID 21: Confidence 0.31729796528816223 : Bounding Box [     1569.5      732.89      3266.9      1543.5]\n","Logic frame count : 116\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 168.0ms\n","Speed: 9.5ms preprocess, 168.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1008.6367), tensor(898.9958), tensor(1170.6331), tensor(1233.7073)], tensor(0.8509), 0), ([tensor(0.), tensor(569.8708), tensor(730.3636), tensor(941.0367)], tensor(0.3391), 6), ([tensor(1569.6963), tensor(732.7091), tensor(1698.3828), tensor(810.6959)], tensor(0.3137), 25), ([tensor(1725.5447), tensor(1161.3616), tensor(1918.2871), tensor(1440.)], tensor(0.2968), 1)]\n","Class person : ID 2: Confidence 0.8509459495544434 : Bounding Box [     996.79      898.44      2199.2      2130.8]\n","Class train : ID 3: Confidence 0.33909350633621216 : Bounding Box [   -0.28865      569.92      730.64      1510.9]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1298.3      872.15      2846.2      1911.7]\n","Class bicycle : ID 20: Confidence 0.2968491017818451 : Bounding Box [       1724      1161.1      3645.3      2601.1]\n","Class umbrella : ID 21: Confidence 0.3136543929576874 : Bounding Box [     1569.9      732.76      3267.5      1543.4]\n","Logic frame count : 117\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 159.1ms\n","Speed: 11.2ms preprocess, 159.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(999.5209), tensor(895.1202), tensor(1162.3898), tensor(1235.7784)], tensor(0.8357), 0), ([tensor(1569.7090), tensor(732.9321), tensor(1698.4001), tensor(810.5867)], tensor(0.3481), 25), ([tensor(0.), tensor(570.4446), tensor(729.9084), tensor(941.6853)], tensor(0.3404), 6), ([tensor(1725.3763), tensor(1161.7296), tensor(1918.6412), tensor(1440.)], tensor(0.2966), 1)]\n","Class person : ID 2: Confidence 0.8357191681861877 : Bounding Box [     983.34      895.83      2182.6      2129.3]\n","Class train : ID 3: Confidence 0.34043657779693604 : Bounding Box [   -0.55795       570.3      730.58      1511.7]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [       1296      877.32      2851.2      1921.8]\n","Class bicycle : ID 20: Confidence 0.29662877321243286 : Bounding Box [     1724.2      1161.5      3645.2      2601.5]\n","Class umbrella : ID 21: Confidence 0.34812337160110474 : Bounding Box [       1570      732.86      3267.7      1543.5]\n","Logic frame count : 118\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 158.7ms\n","Speed: 4.4ms preprocess, 158.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(994.3502), tensor(894.8903), tensor(1161.6992), tensor(1235.0851)], tensor(0.8424), 0), ([tensor(0.), tensor(569.8699), tensor(729.8310), tensor(940.6624)], tensor(0.3429), 6), ([tensor(1725.4700), tensor(1162.1498), tensor(1917.9895), tensor(1440.)], tensor(0.3230), 1), ([tensor(1569.5776), tensor(732.5150), tensor(1696.4370), tensor(810.6942)], tensor(0.3149), 25)]\n","Class person : ID 2: Confidence 0.8424013257026672 : Bounding Box [      976.3      894.74      2171.9      2128.3]\n","Class train : ID 3: Confidence 0.3428664803504944 : Bounding Box [   -0.41528      570.05      730.27        1511]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1293.8       882.5      2856.2      1931.8]\n","Class bicycle : ID 20: Confidence 0.32302191853523254 : Bounding Box [     1724.2        1162      3644.9        2602]\n","Class umbrella : ID 21: Confidence 0.31494489312171936 : Bounding Box [     1569.4      732.62        3267      1543.3]\n","Logic frame count : 119\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 motorcycle, 1 train, 1 umbrella, 158.8ms\n","Speed: 11.6ms preprocess, 158.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(993.3698), tensor(894.8914), tensor(1152.4647), tensor(1234.8679)], tensor(0.8593), 0), ([tensor(1309.0897), tensor(869.5469), tensor(1518.6783), tensor(1028.3716)], tensor(0.4139), 3), ([tensor(1569.5367), tensor(732.9998), tensor(1697.0773), tensor(810.4324)], tensor(0.3526), 25), ([tensor(1725.3936), tensor(1162.2810), tensor(1916.5459), tensor(1440.)], tensor(0.3501), 1), ([tensor(0.), tensor(570.0476), tensor(730.2819), tensor(941.2596)], tensor(0.3185), 6)]\n","Class person : ID 2: Confidence 0.8593382239341736 : Bounding Box [     971.81      894.39      2163.1      2127.9]\n","Class train : ID 3: Confidence 0.3184772729873657 : Bounding Box [   -0.34444      570.08      730.44      1511.2]\n","Class motorcycle : ID 16: Confidence 0.4138942360877991 : Bounding Box [     1303.2      871.01        2835      1901.5]\n","Class bicycle : ID 20: Confidence 0.3500719666481018 : Bounding Box [     1723.8      1162.2      3644.1      2602.2]\n","Class umbrella : ID 21: Confidence 0.3525658845901489 : Bounding Box [     1569.5      732.86      3266.8      1543.4]\n","Logic frame count : 120\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 motorcycle, 1 train, 1 umbrella, 164.6ms\n","Speed: 4.6ms preprocess, 164.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(997.9731), tensor(895.2805), tensor(1142.6113), tensor(1233.3115)], tensor(0.8396), 0), ([tensor(1569.8978), tensor(732.7408), tensor(1697.3207), tensor(810.0565)], tensor(0.3749), 25), ([tensor(1725.3254), tensor(1162.5920), tensor(1917.9412), tensor(1440.)], tensor(0.3560), 1), ([tensor(0.), tensor(570.5609), tensor(730.2108), tensor(942.3691)], tensor(0.3304), 6), ([tensor(1309.5088), tensor(855.4669), tensor(1517.8667), tensor(1028.3147)], tensor(0.2957), 3)]\n","Class person : ID 2: Confidence 0.839645504951477 : Bounding Box [     972.29      894.56      2158.1      2127.2]\n","Class train : ID 3: Confidence 0.3304018974304199 : Bounding Box [   -0.56291      570.42      730.71      1512.4]\n","Class motorcycle : ID 16: Confidence 0.29567956924438477 : Bounding Box [     1304.4      861.48      2833.4      1891.4]\n","Class bicycle : ID 20: Confidence 0.3560486137866974 : Bounding Box [     1724.1      1162.5      3644.1      2602.5]\n","Class umbrella : ID 21: Confidence 0.37490135431289673 : Bounding Box [       1570      732.77      3266.8        1543]\n","Logic frame count : 121\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 149.5ms\n","Speed: 11.1ms preprocess, 149.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1004.8810), tensor(894.9396), tensor(1127.9049), tensor(1229.7698)], tensor(0.8337), 0), ([tensor(1725.1294), tensor(1163.0828), tensor(1918.9858), tensor(1440.)], tensor(0.3688), 1), ([tensor(0.), tensor(570.7465), tensor(730.1220), tensor(942.8086)], tensor(0.3461), 6), ([tensor(1569.9230), tensor(732.4726), tensor(1696.7289), tensor(810.1502)], tensor(0.3343), 25)]\n","Class person : ID 2: Confidence 0.8336515426635742 : Bounding Box [     975.42      894.45      2153.4      2124.4]\n","Class train : ID 3: Confidence 0.3461051881313324 : Bounding Box [   -0.72658      570.67      730.83      1513.2]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [       1303      864.12      2836.2      1896.8]\n","Class bicycle : ID 20: Confidence 0.3688087463378906 : Bounding Box [     1724.5      1162.9      3644.4      2602.9]\n","Class umbrella : ID 21: Confidence 0.33431708812713623 : Bounding Box [       1570      732.57      3266.7      1542.7]\n","Logic frame count : 122\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 train, 1 umbrella, 163.3ms\n","Speed: 10.9ms preprocess, 163.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(995.8621), tensor(893.5256), tensor(1119.3350), tensor(1224.9417)], tensor(0.8132), 0), ([tensor(1569.9512), tensor(732.4000), tensor(1697.8540), tensor(810.1588)], tensor(0.3379), 25), ([tensor(0.), tensor(570.6316), tensor(730.0234), tensor(942.1438)], tensor(0.3375), 6), ([tensor(1725.4987), tensor(1162.9612), tensor(1918.6630), tensor(1440.)], tensor(0.2988), 1)]\n","Class person : ID 2: Confidence 0.8131893277168274 : Bounding Box [     971.22      893.51      2140.1      2119.4]\n","Class train : ID 3: Confidence 0.3374704420566559 : Bounding Box [   -0.62195      570.68      730.65        1513]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1301.7      866.75      2838.9      1902.1]\n","Class bicycle : ID 20: Confidence 0.29879119992256165 : Bounding Box [     1724.8        1163      3644.6        2603]\n","Class umbrella : ID 21: Confidence 0.337890625 : Bounding Box [     1570.3      732.44      3267.1      1542.6]\n","Logic frame count : 123\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 1 umbrella, 162.0ms\n","Speed: 5.9ms preprocess, 162.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(994.3019), tensor(891.5248), tensor(1110.5940), tensor(1216.9625)], tensor(0.8083), 0), ([tensor(1569.9519), tensor(732.6407), tensor(1698.4790), tensor(810.0211)], tensor(0.3632), 25), ([tensor(0.), tensor(570.5150), tensor(730.0430), tensor(942.4309)], tensor(0.3447), 6), ([tensor(1311.0184), tensor(860.8940), tensor(1518.3346), tensor(1029.3596)], tensor(0.3205), 8)]\n","Class person : ID 2: Confidence 0.8083390593528748 : Bounding Box [     969.94      891.87      2127.6        2111]\n","Class train : ID 3: Confidence 0.34467649459838867 : Bounding Box [   -0.60581      570.61      730.62        1513]\n","Class boat : ID 16: Confidence 0.3204612731933594 : Bounding Box [       1306      862.23      2834.6        1893]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1724.8      1163.2      3644.6      2603.2]\n","Class umbrella : ID 21: Confidence 0.36317428946495056 : Bounding Box [     1570.7      732.55      3267.4      1542.6]\n","Logic frame count : 124\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 1 umbrella, 183.7ms\n","Speed: 4.7ms preprocess, 183.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(989.0238), tensor(889.1588), tensor(1105.0472), tensor(1214.3278)], tensor(0.7735), 0), ([tensor(1569.8640), tensor(732.5503), tensor(1698.0950), tensor(810.0684)], tensor(0.3878), 25), ([tensor(0.), tensor(571.1328), tensor(729.9047), tensor(941.8828)], tensor(0.3558), 6), ([tensor(1310.9001), tensor(863.7181), tensor(1517.9417), tensor(1029.1296)], tensor(0.2703), 8)]\n","Class person : ID 2: Confidence 0.773517906665802 : Bounding Box [     966.64      889.72      2115.7      2104.7]\n","Class train : ID 3: Confidence 0.3558083474636078 : Bounding Box [   -0.48277      570.98       730.4      1513.1]\n","Class boat : ID 16: Confidence 0.27031034231185913 : Bounding Box [     1306.9      863.86      2833.5      1894.1]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1724.8      1163.4      3644.5      2603.4]\n","Class umbrella : ID 21: Confidence 0.3878409266471863 : Bounding Box [     1570.6      732.54      3267.4      1542.6]\n","Logic frame count : 125\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 2 umbrellas, 165.0ms\n","Speed: 4.5ms preprocess, 165.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(981.6667), tensor(887.3751), tensor(1100.8667), tensor(1218.2422)], tensor(0.7939), 0), ([tensor(1569.7896), tensor(732.8865), tensor(1698.6650), tensor(810.0387)], tensor(0.4430), 25), ([tensor(0.), tensor(570.9647), tensor(730.1851), tensor(941.4852)], tensor(0.3740), 6), ([tensor(1266.8823), tensor(728.2413), tensor(1386.9800), tensor(767.8018)], tensor(0.3190), 25)]\n","Class person : ID 2: Confidence 0.7938801646232605 : Bounding Box [     959.76      887.77        2105      2103.9]\n","Class train : ID 3: Confidence 0.3740405738353729 : Bounding Box [   -0.24456      571.01      730.32      1512.7]\n","Class umbrella : ID 16: Confidence 0.31902214884757996 : Bounding Box [     1343.2      774.63      2652.1      1631.6]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1724.8      1163.5      3644.5      2603.5]\n","Class umbrella : ID 21: Confidence 0.44298288226127625 : Bounding Box [     1570.6      732.76      3267.6      1542.8]\n","Logic frame count : 126\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 2 umbrellas, 164.9ms\n","Speed: 4.4ms preprocess, 164.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(977.0818), tensor(885.9614), tensor(1096.1804), tensor(1220.5356)], tensor(0.8116), 0), ([tensor(1569.8436), tensor(732.7906), tensor(1698.1478), tensor(809.8768)], tensor(0.4260), 25), ([tensor(0.), tensor(570.5980), tensor(730.4057), tensor(941.4352)], tensor(0.3695), 6), ([tensor(1267.5811), tensor(728.2472), tensor(1360.0332), tensor(772.8466)], tensor(0.2652), 25)]\n","Class person : ID 2: Confidence 0.8116477727890015 : Bounding Box [     953.76      886.15      2095.8      2104.3]\n","Class train : ID 3: Confidence 0.36951303482055664 : Bounding Box [  -0.073731      570.77      730.35      1512.3]\n","Class umbrella : ID 16: Confidence 0.2651948928833008 : Bounding Box [     1342.9      739.31      2576.9      1531.4]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1724.8      1163.7      3644.5      2603.7]\n","Class umbrella : ID 21: Confidence 0.4259718060493469 : Bounding Box [     1570.6      732.78      3267.4      1542.7]\n","Logic frame count : 127\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 2 umbrellas, 187.7ms\n","Speed: 6.3ms preprocess, 187.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(971.0748), tensor(885.8237), tensor(1091.9215), tensor(1218.8835)], tensor(0.8457), 0), ([tensor(1569.9299), tensor(732.9353), tensor(1698.2732), tensor(809.8455)], tensor(0.4190), 25), ([tensor(0.), tensor(570.4213), tensor(730.5402), tensor(942.4034)], tensor(0.3441), 6), ([tensor(1267.5690), tensor(727.9012), tensor(1359.1244), tensor(770.7831)], tensor(0.2854), 25)]\n","Class person : ID 2: Confidence 0.8456700444221497 : Bounding Box [     948.38       885.5      2085.4      2103.4]\n","Class train : ID 3: Confidence 0.34412524104118347 : Bounding Box [   -0.18433      570.56      730.63      1512.7]\n","Class umbrella : ID 16: Confidence 0.2853614389896393 : Bounding Box [     1337.9      727.87      2557.3        1498]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1724.8      1163.9      3644.5      2603.9]\n","Class umbrella : ID 21: Confidence 0.41898754239082336 : Bounding Box [     1570.6      732.88      3267.5      1542.7]\n","Logic frame count : 128\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 umbrella, 380.4ms\n","Speed: 11.0ms preprocess, 380.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(967.4403), tensor(888.1699), tensor(1099.9647), tensor(1212.9419)], tensor(0.8491), 0), ([tensor(1569.9025), tensor(732.6199), tensor(1697.3209), tensor(809.9438)], tensor(0.3544), 25), ([tensor(0.), tensor(571.2814), tensor(730.8429), tensor(942.3566)], tensor(0.3131), 6)]\n","Class person : ID 2: Confidence 0.8491284847259521 : Bounding Box [     949.37      886.85      2079.6      2100.9]\n","Class train : ID 3: Confidence 0.3131488561630249 : Bounding Box [   -0.11573      571.05      730.83      1513.3]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1344.8      716.35      2527.4      1463.3]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1724.8        1164      3644.5        2604]\n","Class umbrella : ID 21: Confidence 0.35440489649772644 : Bounding Box [     1570.2      732.71      3267.3      1542.6]\n","Logic frame count : 129\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 1 motorcycle, 1 train, 1 umbrella, 242.6ms\n","Speed: 13.7ms preprocess, 242.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(967.6244), tensor(888.5252), tensor(1099.2382), tensor(1212.0852)], tensor(0.8193), 0), ([tensor(0.), tensor(571.2701), tensor(731.2407), tensor(942.8834)], tensor(0.3168), 6), ([tensor(1308.0768), tensor(850.2197), tensor(1520.7855), tensor(1028.3352)], tensor(0.3120), 3), ([tensor(1569.8568), tensor(732.3251), tensor(1696.9672), tensor(809.8339)], tensor(0.2981), 25), ([tensor(1725.2159), tensor(1170.6709), tensor(1908.1115), tensor(1440.)], tensor(0.2579), 1), ([tensor(1333.9441), tensor(737.4794), tensor(1400.9304), tensor(835.5436)], tensor(0.2558), 0)]\n","Class person : ID 2: Confidence 0.8192925453186035 : Bounding Box [     950.99      887.64      2076.5      2099.7]\n","Class train : ID 3: Confidence 0.3168272376060486 : Bounding Box [   -0.08792      571.23      731.16      1513.9]\n","Class person : ID 16: Confidence 0.25579243898391724 : Bounding Box [     1364.3      731.51      2664.4      1546.6]\n","Class bicycle : ID 20: Confidence 0.25787073373794556 : Bounding Box [     1720.5      1170.3      3638.6      2610.3]\n","Class umbrella : ID 21: Confidence 0.2981491982936859 : Bounding Box [       1570      732.45      3266.9      1542.3]\n","Logic frame count : 130\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 1 boat, 511.6ms\n","Speed: 11.4ms preprocess, 511.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(958.3735), tensor(889.4467), tensor(1098.7329), tensor(1212.2875)], tensor(0.7935), 0), ([tensor(1308.5928), tensor(813.1898), tensor(1525.9482), tensor(1029.7769)], tensor(0.2974), 8), ([tensor(0.), tensor(571.2653), tensor(731.3509), tensor(942.8932)], tensor(0.2955), 6), ([tensor(1726.1990), tensor(1016.7078), tensor(2032.8699), tensor(1439.7056)], tensor(0.2530), 3)]\n","Class person : ID 2: Confidence 0.793461799621582 : Bounding Box [     946.27      888.59      2068.7      2100.1]\n","Class train : ID 3: Confidence 0.2955431342124939 : Bounding Box [  -0.043878      571.29      731.32      1514.1]\n","Class person : ID 16: Confidence None : Bounding Box [     1373.7      723.75      2657.3      1528.5]\n","Class motorcycle : ID 20: Confidence 0.25299689173698425 : Bounding Box [     1756.4      1064.3      3689.8      2504.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [       1570      732.41      3266.9      1542.2]\n","Logic frame count : 131\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 1 boat, 234.2ms\n","Speed: 15.6ms preprocess, 234.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(948.8604), tensor(891.1246), tensor(1095.4445), tensor(1213.2672)], tensor(0.8242), 0), ([tensor(1308.7654), tensor(816.6960), tensor(1524.8665), tensor(1028.8306)], tensor(0.3469), 8), ([tensor(0.), tensor(571.6252), tensor(730.6417), tensor(942.5209)], tensor(0.2814), 6), ([tensor(1726.0842), tensor(1016.4346), tensor(2032.8542), tensor(1440.)], tensor(0.2703), 3)]\n","Class person : ID 2: Confidence 0.8242340087890625 : Bounding Box [     937.92      890.08      2058.1      2102.1]\n","Class train : ID 3: Confidence 0.2813604772090912 : Bounding Box [   -0.14468      571.54      731.03      1514.2]\n","Class person : ID 16: Confidence None : Bounding Box [     1383.1         716      2650.3      1510.5]\n","Class motorcycle : ID 20: Confidence 0.2702689468860626 : Bounding Box [     1764.2      1029.2      3710.4      2469.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.1      732.38      3266.8      1542.2]\n","Logic frame count : 132\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 247.7ms\n","Speed: 15.8ms preprocess, 247.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(937.2299), tensor(894.8721), tensor(1068.9139), tensor(1213.5715)], tensor(0.8654), 0), ([tensor(0.), tensor(572.1017), tensor(729.7215), tensor(942.4325)], tensor(0.3021), 6), ([tensor(1726.2722), tensor(1016.3022), tensor(2032.4329), tensor(1439.5391)], tensor(0.2597), 3)]\n","Class person : ID 2: Confidence 0.8654396533966064 : Bounding Box [     920.44      893.14        2036      2105.6]\n","Class train : ID 3: Confidence 0.3021390736103058 : Bounding Box [   -0.41175      571.95      730.54      1514.5]\n","Class person : ID 16: Confidence None : Bounding Box [     1392.5      708.25      2643.3      1492.4]\n","Class motorcycle : ID 20: Confidence 0.2596572935581207 : Bounding Box [     1764.3      1016.4      3720.6      2456.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.1      732.34      3266.8      1542.1]\n","Logic frame count : 133\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 1 boat, 1 umbrella, 231.2ms\n","Speed: 7.0ms preprocess, 231.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(917.5441), tensor(894.4286), tensor(1052.2156), tensor(1219.7142)], tensor(0.8605), 0), ([tensor(0.), tensor(572.0759), tensor(729.5766), tensor(942.9033)], tensor(0.2762), 6), ([tensor(1571.1160), tensor(736.6259), tensor(1682.2703), tensor(809.3571)], tensor(0.2634), 25), ([tensor(1726.3174), tensor(1015.6157), tensor(2032.5759), tensor(1439.8223)], tensor(0.2580), 3), ([tensor(1308.7435), tensor(819.7032), tensor(1521.3146), tensor(1027.7128)], tensor(0.2533), 8)]\n","Class person : ID 2: Confidence 0.8604755401611328 : Bounding Box [     896.09      894.01      2008.9      2110.8]\n","Class train : ID 3: Confidence 0.27620676159858704 : Bounding Box [   -0.62402      572.08      730.37      1514.9]\n","Class person : ID 16: Confidence None : Bounding Box [     1401.9      700.49      2636.2      1474.3]\n","Class motorcycle : ID 20: Confidence 0.257994145154953 : Bounding Box [     1761.6      1011.6      3726.9      2451.4]\n","Class umbrella : ID 21: Confidence 0.2634454071521759 : Bounding Box [     1565.8      736.13      3260.1      1545.5]\n","Logic frame count : 134\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 1 boat, 257.7ms\n","Speed: 4.7ms preprocess, 257.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(904.9360), tensor(890.9994), tensor(1031.9396), tensor(1220.5160)], tensor(0.8170), 0), ([tensor(1309.2640), tensor(813.1798), tensor(1522.9698), tensor(1027.9918)], tensor(0.4435), 8), ([tensor(0.), tensor(572.3790), tensor(729.0739), tensor(942.5797)], tensor(0.2866), 6), ([tensor(1726.2732), tensor(1015.7263), tensor(2032.3872), tensor(1440.)], tensor(0.2521), 3)]\n","Class person : ID 2: Confidence 0.8170142769813538 : Bounding Box [     874.74      892.07      1981.5      2111.1]\n","Class train : ID 3: Confidence 0.2865629196166992 : Bounding Box [   -0.73151      572.33         730        1515]\n","Class boat : ID 16: Confidence 0.443534255027771 : Bounding Box [     1271.4       804.2      2862.6      1812.5]\n","Class motorcycle : ID 20: Confidence 0.2521030902862549 : Bounding Box [     1758.2      1010.4      3731.1      2450.3]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1565.3      736.46      3259.5      1545.8]\n","Logic frame count : 135\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 166.0ms\n","Speed: 9.9ms preprocess, 166.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(883.8260), tensor(894.3123), tensor(1018.5695), tensor(1218.0007)], tensor(0.7856), 0), ([tensor(1309.0472), tensor(813.1201), tensor(1525.5040), tensor(1028.6897)], tensor(0.5053), 8)]\n","Class person : ID 2: Confidence 0.7855839133262634 : Bounding Box [     852.12      893.52      1949.8      2111.7]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.81847      572.48      729.96      1515.2]\n","Class boat : ID 16: Confidence 0.5052724480628967 : Bounding Box [     1269.1      810.98      2875.2      1837.2]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1762.4      999.82      3735.3      2439.7]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1564.8      736.78      3258.9      1546.1]\n","Logic frame count : 136\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 1 boat, 1 umbrella, 159.9ms\n","Speed: 5.9ms preprocess, 159.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(875.1398), tensor(890.9720), tensor(1010.1381), tensor(1213.2301)], tensor(0.8301), 0), ([tensor(1308.9146), tensor(822.7152), tensor(1520.8682), tensor(1027.8878)], tensor(0.2702), 8), ([tensor(0.), tensor(572.2225), tensor(729.5380), tensor(942.3661)], tensor(0.2571), 6), ([tensor(1571.0685), tensor(735.9459), tensor(1684.4557), tensor(809.4015)], tensor(0.2545), 25)]\n","Class person : ID 2: Confidence 0.8301398754119873 : Bounding Box [     839.34      891.86      1925.9      2106.6]\n","Class train : ID 3: Confidence 0.2571103274822235 : Bounding Box [   -0.52141      572.31      729.94      1514.8]\n","Class boat : ID 16: Confidence 0.2702350914478302 : Bounding Box [     1270.3      819.74      2874.6      1852.1]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1766.6      989.27      3739.5      2429.2]\n","Class umbrella : ID 21: Confidence 0.2545357644557953 : Bounding Box [     1566.5      736.12      3259.5      1545.5]\n","Logic frame count : 137\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 train, 165.0ms\n","Speed: 7.1ms preprocess, 165.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(867.0935), tensor(890.8556), tensor(1008.2522), tensor(1213.8783)], tensor(0.8481), 0), ([tensor(0.), tensor(571.5638), tensor(729.7917), tensor(941.8228)], tensor(0.2650), 6), ([tensor(1725.2432), tensor(1016.0951), tensor(2032.2935), tensor(1439.7286)], tensor(0.2601), 3)]\n","Class person : ID 2: Confidence 0.848052442073822 : Bounding Box [     831.11      891.16      1909.5      2105.1]\n","Class train : ID 3: Confidence 0.26503950357437134 : Bounding Box [   -0.23672      571.85      729.88      1513.9]\n","Class boat : ID 16: Confidence None : Bounding Box [     1265.1      823.66      2890.3      1869.4]\n","Class motorcycle : ID 20: Confidence 0.26006782054901123 : Bounding Box [     1753.8      1010.3      3733.9      2450.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1566.2      736.35      3259.1      1545.7]\n","Logic frame count : 138\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 train, 164.3ms\n","Speed: 11.3ms preprocess, 164.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(863.4773), tensor(894.0599), tensor(1008.9652), tensor(1213.8102)], tensor(0.8395), 0), ([tensor(0.), tensor(571.5624), tensor(729.5238), tensor(941.7630)], tensor(0.2644), 6)]\n","Class person : ID 2: Confidence 0.8394785523414612 : Bounding Box [     828.32      893.02      1899.8      2106.6]\n","Class train : ID 3: Confidence 0.2644484043121338 : Bounding Box [   -0.19234      571.68      729.74      1513.5]\n","Class boat : ID 16: Confidence None : Bounding Box [       1260      827.57      2905.9      1886.7]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1756.7        1003      3736.7      2442.7]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1565.9      736.58      3258.7      1545.9]\n","Logic frame count : 139\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 umbrellas, 174.0ms\n","Speed: 8.1ms preprocess, 174.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(861.9091), tensor(893.6766), tensor(1010.8480), tensor(1214.8368)], tensor(0.8285), 0), ([tensor(1570.5415), tensor(732.4156), tensor(1696.8457), tensor(810.1478)], tensor(0.2858), 25), ([tensor(1267.2561), tensor(726.6594), tensor(1349.2249), tensor(766.5059)], tensor(0.2554), 25)]\n","Class person : ID 2: Confidence 0.8285282254219055 : Bounding Box [     828.77      893.47      1894.8      2107.6]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.20426      571.71      729.69      1513.5]\n","Class boat : ID 16: Confidence None : Bounding Box [     1254.8      831.49      2921.6        1904]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1759.5      995.59      3739.6      2435.3]\n","Class umbrella : ID 21: Confidence 0.2857591211795807 : Bounding Box [     1570.6      733.07      3265.2      1543.1]\n","Logic frame count : 140\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 umbrellas, 144.7ms\n","Speed: 4.5ms preprocess, 144.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(865.9019), tensor(889.8804), tensor(1007.1875), tensor(1207.7549)], tensor(0.8339), 0), ([tensor(1268.7795), tensor(725.8159), tensor(1349.2634), tensor(762.1270)], tensor(0.4023), 25), ([tensor(1571.5260), tensor(736.9336), tensor(1706.1014), tensor(806.9846)], tensor(0.3002), 25)]\n","Class person : ID 2: Confidence 0.8339290022850037 : Bounding Box [     834.35      891.13      1891.4      2100.8]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.21618      571.74      729.64      1513.5]\n","Class boat : ID 16: Confidence None : Bounding Box [     1249.7       835.4      2937.3      1921.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1762.4      988.22      3742.4      2427.9]\n","Class umbrella : ID 21: Confidence 0.3001728951931 : Bounding Box [     1576.2      735.67      3268.9      1543.6]\n","Logic frame count : 141\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 bicycle, 2 umbrellas, 158.4ms\n","Speed: 9.1ms preprocess, 158.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(863.4716), tensor(892.0925), tensor(991.0350), tensor(1198.5437)], tensor(0.8119), 0), ([tensor(1268.1290), tensor(726.8182), tensor(1345.6503), tensor(763.0157)], tensor(0.4266), 25), ([tensor(1570.8440), tensor(735.9359), tensor(1702.8113), tensor(806.9983)], tensor(0.3123), 25), ([tensor(862.6536), tensor(888.0563), tensor(952.8196), tensor(1202.2946)], tensor(0.2729), 0), ([tensor(1726.6858), tensor(1170.4207), tensor(1904.8308), tensor(1440.)], tensor(0.2573), 1)]\n","Class person : ID 2: Confidence 0.27287134528160095 : Bounding Box [     822.05      889.04      1865.1      2093.5]\n","Class train : ID 3: Confidence None : Bounding Box [    -0.2281      571.77      729.59      1513.5]\n","Class boat : ID 16: Confidence None : Bounding Box [     1244.5      839.32        2953      1938.6]\n","Class bicycle : ID 20: Confidence 0.2572811245918274 : Bounding Box [     1702.7      1149.5      3672.4      2589.4]\n","Class umbrella : ID 21: Confidence 0.3122597932815552 : Bounding Box [     1576.2       735.9      3268.9      1543.2]\n","Logic frame count : 142\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 3 umbrellas, 160.2ms\n","Speed: 11.6ms preprocess, 160.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(853.1624), tensor(891.1724), tensor(950.7375), tensor(1197.5464)], tensor(0.7596), 0), ([tensor(1267.8535), tensor(726.7159), tensor(1345.7498), tensor(762.3646)], tensor(0.4653), 25), ([tensor(1571.1261), tensor(737.0504), tensor(1703.3727), tensor(807.1622)], tensor(0.3258), 25), ([tensor(1449.8438), tensor(712.0867), tensor(1529.8901), tensor(766.5549)], tensor(0.2508), 25)]\n","Class person : ID 2: Confidence 0.7596392631530762 : Bounding Box [     814.57      890.32      1844.9      2089.7]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.24002      571.81      729.54      1513.5]\n","Class boat : ID 16: Confidence None : Bounding Box [     1239.4      843.23      2968.7        1956]\n","Class bicycle : ID 20: Confidence None : Bounding Box [     1699.2      1157.9      3668.9      2597.9]\n","Class umbrella : ID 21: Confidence 0.3258487284183502 : Bounding Box [     1576.1      736.71      3269.8      1543.8]\n","Logic frame count : 143\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 boats, 1 umbrella, 174.9ms\n","Speed: 4.4ms preprocess, 174.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(837.4801), tensor(883.7892), tensor(929.5175), tensor(1189.7965)], tensor(0.7673), 0), ([tensor(1729.4297), tensor(1016.2559), tensor(2031.2603), tensor(1439.3262)], tensor(0.4053), 3), ([tensor(1268.2458), tensor(726.2701), tensor(1345.3840), tensor(762.1593)], tensor(0.4041), 25), ([tensor(1310.7073), tensor(830.6265), tensor(1517.8435), tensor(1024.9153)], tensor(0.3276), 8), ([tensor(1298.8271), tensor(758.8313), tensor(1532.3579), tensor(1033.1953)], tensor(0.2634), 8)]\n","Class person : ID 2: Confidence 0.7672507762908936 : Bounding Box [     799.84      885.92      1815.3      2078.3]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.25194      571.84       729.5      1513.5]\n","Class umbrella : ID 16: Confidence 0.4041057527065277 : Bounding Box [     1326.8      734.88      2579.1        1523]\n","Class motorcycle : ID 20: Confidence 0.4052540361881256 : Bounding Box [     1742.8      1047.4      3720.5      2486.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1576.9      736.95      3270.1      1543.8]\n","Class boat : ID 31: Confidence 0.2634052634239197 : Bounding Box [     1191.3      750.47      2873.4      1712.7]\n","Logic frame count : 144\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 175.2ms\n","Speed: 5.3ms preprocess, 175.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(822.0994), tensor(881.), tensor(923.9158), tensor(1184.6541)], tensor(0.7547), 0), ([tensor(1268.8220), tensor(726.3042), tensor(1348.7693), tensor(762.5349)], tensor(0.3471), 25), ([tensor(1729.8877), tensor(1017.0071), tensor(2031.2681), tensor(1439.5120)], tensor(0.2963), 3), ([tensor(1571.3198), tensor(740.6046), tensor(1692.8921), tensor(805.2723)], tensor(0.2523), 25)]\n","Class person : ID 2: Confidence 0.7547175884246826 : Bounding Box [     786.52      882.43      1788.8      2068.9]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.26386      571.87      729.45      1513.4]\n","Class umbrella : ID 16: Confidence 0.34706756472587585 : Bounding Box [     1325.3      727.27      2562.7        1493]\n","Class motorcycle : ID 20: Confidence 0.29626160860061646 : Bounding Box [     1749.3      1025.7      3733.9      2465.2]\n","Class umbrella : ID 21: Confidence 0.2523226737976074 : Bounding Box [     1573.8      739.85      3264.5      1545.4]\n","Class boat : ID 31: Confidence None : Bounding Box [       1168      757.22        2947      1774.9]\n","Logic frame count : 145\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 umbrella, 153.4ms\n","Speed: 12.2ms preprocess, 153.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(814.5453), tensor(879.0479), tensor(924.3564), tensor(1190.2522)], tensor(0.7845), 0), ([tensor(1267.6860), tensor(726.0758), tensor(1348.4668), tensor(762.5557)], tensor(0.4048), 25), ([tensor(1729.7673), tensor(1017.0254), tensor(2031.3127), tensor(1439.8015)], tensor(0.3244), 3)]\n","Class person : ID 2: Confidence 0.7845410704612732 : Bounding Box [      777.9      879.85      1773.7      2067.9]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.27577       571.9       729.4      1513.4]\n","Class umbrella : ID 16: Confidence 0.4047594964504242 : Bounding Box [     1318.8      725.04      2562.5      1484.8]\n","Class motorcycle : ID 20: Confidence 0.3244205713272095 : Bounding Box [     1749.7      1018.1      3740.3      2457.8]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1574.2      740.37      3264.2      1545.6]\n","Class boat : ID 31: Confidence None : Bounding Box [     1144.6      763.97      3020.7      1837.1]\n","Logic frame count : 146\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 167.6ms\n","Speed: 9.3ms preprocess, 167.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(807.3831), tensor(879.0645), tensor(923.8372), tensor(1190.4536)], tensor(0.8087), 0), ([tensor(1269.4700), tensor(726.0161), tensor(1347.0139), tensor(764.8708)], tensor(0.4191), 25), ([tensor(1570.7798), tensor(736.3713), tensor(1703.5720), tensor(806.8184)], tensor(0.3039), 25), ([tensor(1731.6929), tensor(1017.5586), tensor(2031.3506), tensor(1439.9463)], tensor(0.2638), 3)]\n","Class person : ID 2: Confidence 0.8087498545646667 : Bounding Box [     772.25      878.94      1761.7      2067.8]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.28769      571.93      729.35      1513.4]\n","Class umbrella : ID 16: Confidence 0.419063001871109 : Bounding Box [     1312.5      724.38      2568.3      1484.1]\n","Class motorcycle : ID 20: Confidence 0.2638412415981293 : Bounding Box [     1749.7      1015.8      3745.2      2455.6]\n","Class umbrella : ID 21: Confidence 0.30391010642051697 : Bounding Box [       1575      737.33      3268.8      1543.7]\n","Class boat : ID 31: Confidence None : Bounding Box [     1121.3      770.72      3094.3      1899.4]\n","Logic frame count : 147\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 160.2ms\n","Speed: 4.9ms preprocess, 160.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(788.5443), tensor(881.1592), tensor(924.2341), tensor(1186.9614)], tensor(0.8249), 0), ([tensor(1268.0132), tensor(725.5785), tensor(1347.2310), tensor(763.9164)], tensor(0.4711), 25), ([tensor(1570.6741), tensor(735.8750), tensor(1703.3074), tensor(806.8400)], tensor(0.3167), 25), ([tensor(1729.1660), tensor(1017.4790), tensor(2031.0605), tensor(1439.7529)], tensor(0.3154), 3)]\n","Class person : ID 2: Confidence 0.8248971104621887 : Bounding Box [     760.95      880.02      1742.6      2066.9]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.29961      571.97       729.3      1513.4]\n","Class umbrella : ID 16: Confidence 0.47105881571769714 : Bounding Box [     1306.8      724.02      2572.5      1483.5]\n","Class motorcycle : ID 20: Confidence 0.31539386510849 : Bounding Box [     1746.8      1015.1      3746.3      2454.9]\n","Class umbrella : ID 21: Confidence 0.3166867792606354 : Bounding Box [     1574.6      736.39      3269.9        1543]\n","Class boat : ID 31: Confidence None : Bounding Box [     1097.9      777.47        3168      1961.6]\n","Logic frame count : 148\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 163.3ms\n","Speed: 4.3ms preprocess, 163.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(779.2729), tensor(886.5575), tensor(922.5891), tensor(1184.6957)], tensor(0.8450), 0), ([tensor(1270.0109), tensor(725.4012), tensor(1347.2599), tensor(761.6395)], tensor(0.4239), 25), ([tensor(1728.0715), tensor(1017.0659), tensor(2030.8132), tensor(1439.3506)], tensor(0.4194), 3), ([tensor(1571.2686), tensor(740.9312), tensor(1692.9009), tensor(805.3320)], tensor(0.2740), 25)]\n","Class person : ID 2: Confidence 0.844961404800415 : Bounding Box [      752.7      884.05      1727.1      2068.8]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.31153         572      729.25      1513.4]\n","Class umbrella : ID 16: Confidence 0.42389580607414246 : Bounding Box [     1304.6      723.93      2577.3      1482.2]\n","Class motorcycle : ID 20: Confidence 0.4194157123565674 : Bounding Box [       1744      1014.7      3746.6      2454.2]\n","Class umbrella : ID 21: Confidence 0.27404752373695374 : Bounding Box [     1572.7      739.41      3266.1      1545.1]\n","Class boat : ID 31: Confidence None : Bounding Box [     1074.6      784.22      3241.6      2023.8]\n","Logic frame count : 149\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 242.0ms\n","Speed: 8.2ms preprocess, 242.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(771.6773), tensor(884.2600), tensor(913.2476), tensor(1183.6936)], tensor(0.8171), 0), ([tensor(1268.7003), tensor(725.5773), tensor(1346.6676), tensor(761.3785)], tensor(0.4403), 25), ([tensor(1729.1731), tensor(1017.9722), tensor(2031.0854), tensor(1440.)], tensor(0.3180), 3), ([tensor(1571.1494), tensor(741.0103), tensor(1693.3037), tensor(805.3140)], tensor(0.2706), 25)]\n","Class person : ID 2: Confidence 0.8171380162239075 : Bounding Box [     744.08      884.07      1711.6      2067.5]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.32345      572.03      729.21      1513.4]\n","Class umbrella : ID 16: Confidence 0.44031625986099243 : Bounding Box [     1300.5      724.15      2580.5      1482.2]\n","Class motorcycle : ID 20: Confidence 0.31800904870033264 : Bounding Box [     1742.5      1015.4      3748.5      2455.2]\n","Class umbrella : ID 21: Confidence 0.2706321179866791 : Bounding Box [       1572      740.59      3264.7        1546]\n","Class boat : ID 31: Confidence None : Bounding Box [     1051.2      790.96      3315.3      2086.1]\n","Logic frame count : 150\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 388.4ms\n","Speed: 20.3ms preprocess, 388.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(765.1888), tensor(880.7389), tensor(910.0050), tensor(1183.5084)], tensor(0.8060), 0), ([tensor(1267.6196), tensor(725.6305), tensor(1347.4165), tensor(764.3263)], tensor(0.4117), 25), ([tensor(1728.3148), tensor(1017.6781), tensor(2031.0087), tensor(1439.8854)], tensor(0.3858), 3), ([tensor(1570.7556), tensor(735.8801), tensor(1702.3599), tensor(806.8358)], tensor(0.2732), 25)]\n","Class person : ID 2: Confidence 0.8060168027877808 : Bounding Box [     737.66      881.76      1699.1      2064.6]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.33537      572.06      729.16      1513.4]\n","Class umbrella : ID 16: Confidence 0.4116895794868469 : Bounding Box [     1295.1      724.38      2584.9      1484.6]\n","Class motorcycle : ID 20: Confidence 0.38576164841651917 : Bounding Box [     1740.6      1015.6      3749.3      2455.5]\n","Class umbrella : ID 21: Confidence 0.2732471823692322 : Bounding Box [       1573      737.63      3268.3      1543.9]\n","Class boat : ID 31: Confidence None : Bounding Box [     1027.9      797.71      3388.9      2148.3]\n","Logic frame count : 151\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 401.1ms\n","Speed: 7.3ms preprocess, 401.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(763.9088), tensor(879.8592), tensor(891.8116), tensor(1180.4246)], tensor(0.8009), 0), ([tensor(1268.1377), tensor(725.9374), tensor(1347.3350), tensor(764.4346)], tensor(0.3847), 25), ([tensor(1728.4587), tensor(1017.8086), tensor(2030.9099), tensor(1440.)], tensor(0.3591), 3), ([tensor(1571.3506), tensor(741.3483), tensor(1691.8394), tensor(805.4135)], tensor(0.2656), 25)]\n","Class person : ID 2: Confidence 0.8009248375892639 : Bounding Box [     731.88      880.33      1685.1        2061]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.34729       572.1      729.11      1513.3]\n","Class umbrella : ID 16: Confidence 0.3846827745437622 : Bounding Box [     1291.7      724.76      2588.8      1486.1]\n","Class motorcycle : ID 20: Confidence 0.3591492176055908 : Bounding Box [     1739.2      1015.9      3750.3      2455.9]\n","Class umbrella : ID 21: Confidence 0.26558423042297363 : Bounding Box [     1571.5      740.11      3265.4      1545.8]\n","Class boat : ID 31: Confidence None : Bounding Box [     1004.6      804.46      3462.6      2210.5]\n","Logic frame count : 152\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 227.0ms\n","Speed: 4.6ms preprocess, 227.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(755.9216), tensor(875.0817), tensor(885.4968), tensor(1177.3739)], tensor(0.8268), 0), ([tensor(1268.0427), tensor(725.6943), tensor(1347.2639), tensor(764.3284)], tensor(0.4017), 25), ([tensor(1728.7444), tensor(1017.9308), tensor(2030.9089), tensor(1440.)], tensor(0.3471), 3), ([tensor(1571.2617), tensor(741.1809), tensor(1692.3105), tensor(805.4258)], tensor(0.2590), 25)]\n","Class person : ID 2: Confidence 0.8267976641654968 : Bounding Box [      725.5      876.64      1670.2      2054.5]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.35921      572.13      729.06      1513.3]\n","Class umbrella : ID 16: Confidence 0.40169191360473633 : Bounding Box [     1289.1      724.81      2591.8      1486.7]\n","Class motorcycle : ID 20: Confidence 0.3471285402774811 : Bounding Box [     1738.2      1016.2      3751.4      2456.2]\n","Class umbrella : ID 21: Confidence 0.2590295374393463 : Bounding Box [     1571.1      740.93      3264.4      1546.4]\n","Class boat : ID 31: Confidence None : Bounding Box [      981.2      811.21      3536.2      2272.8]\n","Logic frame count : 153\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 203.9ms\n","Speed: 4.1ms preprocess, 203.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(749.7126), tensor(873.9698), tensor(854.1708), tensor(1180.5078)], tensor(0.7614), 0), ([tensor(1267.9761), tensor(726.5470), tensor(1346.6064), tensor(762.4510)], tensor(0.4346), 25), ([tensor(1728.0654), tensor(1017.0996), tensor(2031.0156), tensor(1439.7080)], tensor(0.3632), 3), ([tensor(1571.4468), tensor(741.1703), tensor(1690.8022), tensor(805.6749)], tensor(0.2581), 25)]\n","Class person : ID 2: Confidence 0.7613665461540222 : Bounding Box [     711.47      874.55      1648.2      2053.5]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.37113      572.16      729.01      1513.3]\n","Class umbrella : ID 16: Confidence 0.43459323048591614 : Bounding Box [     1287.5      725.47      2593.1      1486.5]\n","Class motorcycle : ID 20: Confidence 0.3632343113422394 : Bounding Box [       1737      1015.9      3751.7      2455.7]\n","Class umbrella : ID 21: Confidence 0.2581256031990051 : Bounding Box [     1570.5      741.22      3263.7      1546.8]\n","Class boat : ID 31: Confidence None : Bounding Box [     957.86      817.96      3609.9        2335]\n","Logic frame count : 154\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 237.6ms\n","Speed: 11.3ms preprocess, 237.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(747.8354), tensor(873.3988), tensor(873.0959), tensor(1180.3356)], tensor(0.7777), 0), ([tensor(1268.2051), tensor(726.5721), tensor(1346.6943), tensor(762.6252)], tensor(0.3951), 25), ([tensor(1728.3412), tensor(1016.6414), tensor(2031.2560), tensor(1439.5574)], tensor(0.3387), 3), ([tensor(1570.9783), tensor(735.9955), tensor(1701.3533), tensor(807.0824)], tensor(0.2610), 25)]\n","Class person : ID 2: Confidence 0.7777445912361145 : Bounding Box [     713.14      873.42      1643.9      2052.7]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.38305      572.19      728.96      1513.3]\n","Class umbrella : ID 16: Confidence 0.3950578272342682 : Bounding Box [     1285.8      725.79      2595.2      1486.7]\n","Class motorcycle : ID 20: Confidence 0.3387277126312256 : Bounding Box [     1736.3      1015.6      3752.5      2455.2]\n","Class umbrella : ID 21: Confidence 0.26100024580955505 : Bounding Box [     1572.2       737.9      3267.9      1544.4]\n","Class boat : ID 31: Confidence None : Bounding Box [     934.51      824.71      3683.5      2397.2]\n","Logic frame count : 155\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 motorcycle, 1 umbrella, 234.1ms\n","Speed: 4.7ms preprocess, 234.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(744.5697), tensor(871.0905), tensor(856.0801), tensor(1181.0258)], tensor(0.7532), 0), ([tensor(1727.1189), tensor(1016.8096), tensor(2031.3772), tensor(1439.3335)], tensor(0.4080), 3), ([tensor(1268.0632), tensor(725.4697), tensor(1344.1462), tensor(764.5183)], tensor(0.3597), 25), ([tensor(1331.7400), tensor(731.6562), tensor(1399.1174), tensor(837.9446)], tensor(0.3040), 0)]\n","Class person : ID 2: Confidence 0.7531852126121521 : Bounding Box [     708.73      871.51      1632.7      2051.5]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.39497      572.22      728.92      1513.3]\n","Class umbrella : ID 16: Confidence 0.3596871495246887 : Bounding Box [     1282.4      725.22        2597      1487.5]\n","Class motorcycle : ID 20: Confidence 0.40800032019615173 : Bounding Box [     1734.9      1015.7      3752.3      2455.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1572.3      737.87        3268      1544.4]\n","Class person : ID 31: Confidence 0.3039812445640564 : Bounding Box [     1288.3      735.63      2798.5        1605]\n","Logic frame count : 156\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 1 umbrella, 249.9ms\n","Speed: 4.4ms preprocess, 249.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(741.7993), tensor(864.9092), tensor(840.4268), tensor(1182.5322)], tensor(0.8227), 0), ([tensor(1268.3989), tensor(725.4449), tensor(1345.7534), tensor(761.5280)], tensor(0.3367), 25), ([tensor(1727.6444), tensor(1016.9360), tensor(2031.5507), tensor(1438.9883)], tensor(0.3317), 3), ([tensor(1311.3994), tensor(817.3951), tensor(1520.8076), tensor(1031.1449)], tensor(0.2524), 8)]\n","Class person : ID 2: Confidence 0.8227233290672302 : Bounding Box [     702.81      866.74      1619.6        2048]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.40689      572.26      728.87      1513.3]\n","Class umbrella : ID 16: Confidence 0.33666789531707764 : Bounding Box [     1282.5      725.03        2598        1486]\n","Class motorcycle : ID 20: Confidence 0.33173149824142456 : Bounding Box [     1734.5      1015.9      3752.9        2455]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1572.4      737.84      3268.1      1544.3]\n","Class boat : ID 31: Confidence 0.2523561418056488 : Bounding Box [     1212.1      804.04      2924.2      1809.7]\n","Logic frame count : 157\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 225.6ms\n","Speed: 7.6ms preprocess, 225.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(741.4922), tensor(864.2277), tensor(835.9476), tensor(1179.9218)], tensor(0.8248), 0), ([tensor(1268.4626), tensor(726.0007), tensor(1346.4861), tensor(762.8428)], tensor(0.3651), 25), ([tensor(1727.0151), tensor(1016.4248), tensor(2031.4932), tensor(1438.0454)], tensor(0.3627), 3), ([tensor(1571.4492), tensor(741.5753), tensor(1689.4551), tensor(805.7490)], tensor(0.2574), 25)]\n","Class person : ID 2: Confidence 0.8248351216316223 : Bounding Box [     702.26      864.54      1610.5      2044.6]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.41881      572.29      728.82      1513.3]\n","Class umbrella : ID 16: Confidence 0.36510026454925537 : Bounding Box [     1281.3      725.37      2600.3      1486.8]\n","Class motorcycle : ID 20: Confidence 0.36266955733299255 : Bounding Box [     1733.9      1015.7      3752.7      2454.1]\n","Class umbrella : ID 21: Confidence 0.2573946714401245 : Bounding Box [     1569.9      740.99      3263.7      1546.8]\n","Class boat : ID 31: Confidence None : Bounding Box [     1202.6      812.21      2950.8      1839.1]\n","Logic frame count : 158\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 motorcycle, 2 umbrellas, 216.5ms\n","Speed: 7.8ms preprocess, 216.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(739.4839), tensor(862.0107), tensor(832.7883), tensor(1174.2930)], tensor(0.8211), 0), ([tensor(1727.1284), tensor(1016.1479), tensor(2031.6562), tensor(1438.1035)], tensor(0.4118), 3), ([tensor(1337.9421), tensor(722.7548), tensor(1403.2126), tensor(838.8480)], tensor(0.3520), 0), ([tensor(1268.0229), tensor(725.2006), tensor(1345.4873), tensor(761.3773)], tensor(0.2985), 25), ([tensor(1571.2771), tensor(741.4426), tensor(1689.7742), tensor(805.7798)], tensor(0.2607), 25)]\n","Class person : ID 2: Confidence 0.8211461901664734 : Bounding Box [     703.48      862.31      1601.7      2038.3]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.43073      572.32      728.77      1513.2]\n","Class umbrella : ID 16: Confidence 0.2985379099845886 : Bounding Box [     1280.2         725      2600.6      1485.7]\n","Class motorcycle : ID 20: Confidence 0.4117530882358551 : Bounding Box [     1733.3      1015.5      3753.1      2453.6]\n","Class umbrella : ID 21: Confidence 0.26065605878829956 : Bounding Box [     1569.7      741.39      3262.9      1547.2]\n","Class person : ID 31: Confidence 0.35197997093200684 : Bounding Box [     1298.6      743.86      2800.3      1627.9]\n","Logic frame count : 159\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 271.2ms\n","Speed: 9.2ms preprocess, 271.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(737.8088), tensor(858.8997), tensor(830.9917), tensor(1172.2576)], tensor(0.7908), 0), ([tensor(1268.0132), tensor(724.6744), tensor(1346.1064), tensor(760.9948)], tensor(0.3556), 25), ([tensor(1727.6426), tensor(1016.2595), tensor(2031.1943), tensor(1438.5964)], tensor(0.3534), 3), ([tensor(1570.6398), tensor(734.9567), tensor(1700.9176), tensor(807.0773)], tensor(0.2824), 25)]\n","Class person : ID 2: Confidence 0.7907639145851135 : Bounding Box [     704.94      859.47      1594.8      2032.5]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.44265      572.35      728.72      1513.2]\n","Class umbrella : ID 16: Confidence 0.35557931661605835 : Bounding Box [     1279.5      724.54      2601.5      1484.8]\n","Class motorcycle : ID 20: Confidence 0.35335779190063477 : Bounding Box [     1732.8      1015.5        3754      2453.9]\n","Class umbrella : ID 21: Confidence 0.2824108600616455 : Bounding Box [     1571.6      737.22      3267.2      1543.8]\n","Class person : ID 31: Confidence None : Bounding Box [       1304         741      2801.6      1622.7]\n","Logic frame count : 160\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 1 umbrella, 243.0ms\n","Speed: 4.8ms preprocess, 243.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(736.6814), tensor(841.8171), tensor(827.6929), tensor(1168.3662)], tensor(0.7873), 0), ([tensor(1729.0723), tensor(1016.2660), tensor(2031.0923), tensor(1439.3495)], tensor(0.3479), 3), ([tensor(1312.3496), tensor(806.3508), tensor(1527.9458), tensor(1029.5012)], tensor(0.2948), 8), ([tensor(1268.2280), tensor(723.5500), tensor(1348.3828), tensor(763.2264)], tensor(0.2852), 25)]\n","Class person : ID 2: Confidence 0.7872939705848694 : Bounding Box [     706.63      847.16      1588.1      2016.6]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.45457      572.39      728.67      1513.2]\n","Class umbrella : ID 16: Confidence 0.28520530462265015 : Bounding Box [     1278.2      723.65      2604.8      1485.3]\n","Class motorcycle : ID 20: Confidence 0.34786394238471985 : Bounding Box [     1732.9      1015.6      3755.8      2454.6]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1571.7      737.04      3267.3      1543.7]\n","Class boat : ID 31: Confidence 0.2947995066642761 : Bounding Box [     1234.4      794.75      2911.3      1798.7]\n","Logic frame count : 161\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 178.8ms\n","Speed: 6.6ms preprocess, 178.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(728.8359), tensor(842.3335), tensor(821.6531), tensor(1154.3064)], tensor(0.8006), 0), ([tensor(1728.5625), tensor(1016.7383), tensor(2030.9404), tensor(1439.0049)], tensor(0.3174), 3), ([tensor(1310.1587), tensor(800.3080), tensor(1527.5569), tensor(1030.9723)], tensor(0.2921), 8)]\n","Class person : ID 2: Confidence 0.8005589246749878 : Bounding Box [     705.16      842.95      1573.9      2001.8]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.46649      572.42      728.63      1513.2]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1278.7      722.94      2602.9      1483.2]\n","Class motorcycle : ID 20: Confidence 0.3173593282699585 : Bounding Box [     1732.5        1016      3756.1        2455]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1571.8      736.85      3267.4      1543.5]\n","Class boat : ID 31: Confidence 0.2920730710029602 : Bounding Box [     1229.7      800.29      2921.4      1827.5]\n","Logic frame count : 162\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 1 umbrella, 161.9ms\n","Speed: 8.5ms preprocess, 161.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(722.4574), tensor(850.9420), tensor(818.7052), tensor(1150.1680)], tensor(0.8190), 0), ([tensor(1727.7269), tensor(1016.9248), tensor(2030.7120), tensor(1438.6494)], tensor(0.3457), 3), ([tensor(1269.1655), tensor(724.4102), tensor(1343.5933), tensor(758.2670)], tensor(0.3067), 25), ([tensor(1310.9526), tensor(803.7855), tensor(1522.8269), tensor(1030.8016)], tensor(0.2893), 8)]\n","Class person : ID 2: Confidence 0.8189666867256165 : Bounding Box [     701.94       847.2      1561.5      1999.4]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.47841      572.45      728.58      1513.2]\n","Class umbrella : ID 16: Confidence 0.3067178428173065 : Bounding Box [     1279.1      723.93      2602.5      1482.3]\n","Class motorcycle : ID 20: Confidence 0.3457355499267578 : Bounding Box [     1731.6      1016.3      3755.6      2455.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1571.9      736.67      3267.5      1543.3]\n","Class boat : ID 31: Confidence 0.28931012749671936 : Bounding Box [     1233.1      804.48      2917.5      1839.9]\n","Logic frame count : 163\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 166.3ms\n","Speed: 5.0ms preprocess, 166.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(721.0255), tensor(863.2633), tensor(815.2218), tensor(1147.8309)], tensor(0.8055), 0), ([tensor(1729.4965), tensor(1017.1880), tensor(2030.8898), tensor(1439.4155)], tensor(0.3154), 3), ([tensor(1310.6266), tensor(805.3044), tensor(1524.7035), tensor(1031.0847)], tensor(0.2665), 8)]\n","Class person : ID 2: Confidence 0.8055402040481567 : Bounding Box [     700.68      857.09      1553.3      2005.3]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.49033      572.48      728.53      1513.2]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1279.7       723.4      2600.6      1480.3]\n","Class motorcycle : ID 20: Confidence 0.3153928220272064 : Bounding Box [       1732      1016.6      3757.2      2455.8]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1571.9      736.48      3267.6      1543.2]\n","Class boat : ID 31: Confidence 0.2664823532104492 : Bounding Box [     1239.8      806.86        2911      1844.9]\n","Logic frame count : 164\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 166.1ms\n","Speed: 8.1ms preprocess, 166.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(715.8803), tensor(855.9853), tensor(812.9947), tensor(1140.3184)], tensor(0.7931), 0), ([tensor(1728.9854), tensor(1017.0444), tensor(2030.8936), tensor(1439.1755)], tensor(0.3353), 3), ([tensor(1310.7133), tensor(810.4048), tensor(1525.5748), tensor(1031.0879)], tensor(0.2781), 8)]\n","Class person : ID 2: Confidence 0.793129563331604 : Bounding Box [     699.05      856.05      1543.7      1997.9]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.50225      572.51      728.48      1513.2]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1280.4      722.86      2598.7      1478.3]\n","Class motorcycle : ID 20: Confidence 0.3352658152580261 : Bounding Box [     1731.7      1016.7      3757.5      2455.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [       1572       736.3      3267.7        1543]\n","Class boat : ID 31: Confidence 0.27806416153907776 : Bounding Box [     1247.2      810.91        2904      1849.3]\n","Logic frame count : 165\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 3 umbrellas, 167.1ms\n","Speed: 4.6ms preprocess, 167.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(712.9784), tensor(856.7334), tensor(814.5997), tensor(1140.3550)], tensor(0.8060), 0), ([tensor(1269.3743), tensor(725.1051), tensor(1344.0935), tensor(759.4205)], tensor(0.3276), 25), ([tensor(1571.0186), tensor(737.7170), tensor(1700.8921), tensor(807.1917)], tensor(0.3107), 25), ([tensor(1729.7505), tensor(1016.8784), tensor(2031.0984), tensor(1439.4893)], tensor(0.3076), 3), ([tensor(1269.2188), tensor(719.7966), tensor(1386.2988), tensor(761.8150)], tensor(0.2596), 25)]\n","Class person : ID 2: Confidence 0.8059884309768677 : Bounding Box [     698.08      856.19      1538.2      1995.9]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.51417      572.55      728.43      1513.1]\n","Class umbrella : ID 16: Confidence 0.327623575925827 : Bounding Box [       1278      724.69      2604.1      1483.3]\n","Class motorcycle : ID 20: Confidence 0.3075689971446991 : Bounding Box [     1731.9      1016.6      3758.4        2456]\n","Class umbrella : ID 21: Confidence 0.31072908639907837 : Bounding Box [     1572.7      737.61        3270      1544.8]\n","Class umbrella : ID 31: Confidence 0.2595607042312622 : Bounding Box [     1305.3      752.65      2699.5      1613.5]\n","Logic frame count : 166\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 160.6ms\n","Speed: 3.9ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(705.6700), tensor(853.6873), tensor(816.2303), tensor(1140.6724)], tensor(0.8229), 0), ([tensor(1729.0193), tensor(1016.7952), tensor(2031.1145), tensor(1439.3962)], tensor(0.3325), 3), ([tensor(1269.3088), tensor(725.7238), tensor(1343.6060), tensor(761.1219)], tensor(0.3015), 25), ([tensor(1570.9486), tensor(737.3915), tensor(1701.8834), tensor(807.2213)], tensor(0.2857), 25)]\n","Class person : ID 2: Confidence 0.8229321241378784 : Bounding Box [     694.23      854.25      1531.6      1993.4]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.52609      572.58      728.38      1513.1]\n","Class umbrella : ID 16: Confidence 0.30147528648376465 : Bounding Box [       1276       725.3        2606      1485.3]\n","Class motorcycle : ID 20: Confidence 0.3325226604938507 : Bounding Box [     1731.4      1016.6      3758.4        2456]\n","Class umbrella : ID 21: Confidence 0.2856791317462921 : Bounding Box [     1572.8      737.44      3270.8      1544.7]\n","Class umbrella : ID 31: Confidence None : Bounding Box [     1307.2      750.54        2688        1603]\n","Logic frame count : 167\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 2 umbrellas, 179.9ms\n","Speed: 3.8ms preprocess, 179.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(701.9422), tensor(847.5339), tensor(816.0508), tensor(1140.8516)], tensor(0.8283), 0), ([tensor(1728.5123), tensor(1016.6370), tensor(2031.0551), tensor(1439.5432)], tensor(0.3597), 3), ([tensor(1310.8330), tensor(812.9614), tensor(1525.8892), tensor(1031.6943)], tensor(0.2867), 8), ([tensor(1570.9089), tensor(736.9736), tensor(1701.2864), tensor(807.3035)], tensor(0.2735), 25), ([tensor(1269.5635), tensor(725.9395), tensor(1344.1814), tensor(759.8745)], tensor(0.2579), 25)]\n","Class person : ID 2: Confidence 0.8283445239067078 : Bounding Box [     691.08      849.48      1526.3      1988.7]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.53801      572.61      728.34      1513.1]\n","Class umbrella : ID 16: Confidence 0.2579340636730194 : Bounding Box [     1275.8      725.66      2606.8      1485.3]\n","Class motorcycle : ID 20: Confidence 0.3597027659416199 : Bounding Box [     1730.7      1016.5      3758.2        2456]\n","Class umbrella : ID 21: Confidence 0.27350616455078125 : Bounding Box [     1572.4      737.11        3271      1544.4]\n","Class boat : ID 31: Confidence 0.28674644231796265 : Bounding Box [     1259.3      801.04      2858.4      1798.1]\n","Logic frame count : 168\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 160.8ms\n","Speed: 11.6ms preprocess, 160.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(694.6819), tensor(847.4475), tensor(817.1246), tensor(1139.4204)], tensor(0.8444), 0), ([tensor(1727.3372), tensor(1016.3350), tensor(2031.2385), tensor(1438.4941)], tensor(0.3986), 3), ([tensor(1309.9131), tensor(810.3059), tensor(1531.5991), tensor(1031.5477)], tensor(0.3483), 8)]\n","Class person : ID 2: Confidence 0.8444233536720276 : Bounding Box [     686.49      847.67      1519.3      1986.1]\n","Class train : ID 3: Confidence None : Bounding Box [   -0.54993      572.64      728.29      1513.1]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1276.1      725.52      2605.9      1484.5]\n","Class motorcycle : ID 20: Confidence 0.39859136939048767 : Bounding Box [       1730      1016.2      3757.1      2455.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1572.6      737.01      3271.3      1544.4]\n","Class boat : ID 31: Confidence 0.3482528030872345 : Bounding Box [     1257.1      808.65      2884.9      1831.9]\n","Logic frame count : 169\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 2 umbrellas, 163.9ms\n","Speed: 14.7ms preprocess, 163.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(685.8530), tensor(849.6038), tensor(817.6285), tensor(1138.9065)], tensor(0.8080), 0), ([tensor(1728.0320), tensor(1015.8738), tensor(2031.4551), tensor(1439.4368)], tensor(0.3796), 3), ([tensor(1269.6205), tensor(726.6104), tensor(1345.2372), tensor(758.6189)], tensor(0.2845), 25), ([tensor(1571.1694), tensor(739.1622), tensor(1699.2461), tensor(806.9103)], tensor(0.2831), 25), ([tensor(1310.9854), tensor(817.4062), tensor(1527.9880), tensor(1031.5352)], tensor(0.2634), 8)]\n","Class person : ID 2: Confidence 0.8079754710197449 : Bounding Box [     679.85      848.47      1510.7      1986.4]\n","Class umbrella : ID 16: Confidence 0.284505158662796 : Bounding Box [     1276.4      726.34      2607.4      1484.9]\n","Class motorcycle : ID 20: Confidence 0.37956544756889343 : Bounding Box [     1729.7      1015.9      3757.7      2455.1]\n","Class umbrella : ID 21: Confidence 0.28314217925071716 : Bounding Box [       1572      738.67      3270.2      1545.7]\n","Class boat : ID 31: Confidence 0.26337045431137085 : Bounding Box [     1259.5      816.06      2890.9      1849.2]\n","Logic frame count : 170\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 160.2ms\n","Speed: 4.7ms preprocess, 160.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(677.5665), tensor(851.9657), tensor(809.4679), tensor(1137.2565)], tensor(0.7460), 0), ([tensor(1728.7891), tensor(1016.1753), tensor(2031.5405), tensor(1439.1091)], tensor(0.3484), 3), ([tensor(1269.6147), tensor(726.2288), tensor(1345.3267), tensor(757.6453)], tensor(0.3007), 25), ([tensor(1570.9695), tensor(741.9039), tensor(1685.9480), tensor(805.5878)], tensor(0.2535), 25)]\n","Class person : ID 2: Confidence 0.7460402846336365 : Bounding Box [     670.52      850.39      1498.5      1987.1]\n","Class umbrella : ID 16: Confidence 0.30073195695877075 : Bounding Box [     1276.5      726.25      2607.8        1484]\n","Class motorcycle : ID 20: Confidence 0.34842947125434875 : Bounding Box [     1730.2      1015.9      3758.4      2455.1]\n","Class umbrella : ID 21: Confidence 0.2535127103328705 : Bounding Box [     1568.8      740.88      3263.8      1546.9]\n","Class boat : ID 31: Confidence None : Bounding Box [     1254.2      821.25      2905.3      1866.9]\n","Logic frame count : 171\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 2 umbrellas, 169.8ms\n","Speed: 4.0ms preprocess, 169.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(669.2264), tensor(858.7038), tensor(792.8915), tensor(1131.2209)], tensor(0.7838), 0), ([tensor(1728.4377), tensor(1016.4104), tensor(2031.3357), tensor(1438.8396)], tensor(0.3155), 3), ([tensor(1571.2808), tensor(739.1042), tensor(1698.7939), tensor(806.6794)], tensor(0.3068), 25), ([tensor(1313.1182), tensor(818.5234), tensor(1521.9419), tensor(1029.0371)], tensor(0.2908), 8), ([tensor(1269.7024), tensor(726.8724), tensor(1344.0291), tensor(758.4069)], tensor(0.2854), 25)]\n","Class person : ID 2: Confidence 0.7837527394294739 : Bounding Box [      658.8      855.61      1480.5      1987.9]\n","Class umbrella : ID 16: Confidence 0.2854200005531311 : Bounding Box [     1275.3      726.65      2608.3      1484.6]\n","Class motorcycle : ID 20: Confidence 0.3155137002468109 : Bounding Box [     1730.1      1016.1      3758.4      2455.1]\n","Class umbrella : ID 21: Confidence 0.30675414204597473 : Bounding Box [       1571      739.81      3267.1      1546.3]\n","Class boat : ID 31: Confidence 0.2908262610435486 : Bounding Box [     1264.5      820.35      2888.4      1856.1]\n","Logic frame count : 172\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 165.8ms\n","Speed: 5.5ms preprocess, 165.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(660.8998), tensor(858.6655), tensor(778.4415), tensor(1130.5747)], tensor(0.7581), 0), ([tensor(1728.4764), tensor(1016.3442), tensor(2031.2411), tensor(1439.0930)], tensor(0.3072), 3), ([tensor(1269.6569), tensor(726.8885), tensor(1345.4974), tensor(759.2902)], tensor(0.3026), 25), ([tensor(1571.4552), tensor(738.5204), tensor(1699.5939), tensor(806.5381)], tensor(0.2841), 25)]\n","Class person : ID 2: Confidence 0.7581015825271606 : Bounding Box [     645.94      857.58      1462.2      1987.9]\n","Class umbrella : ID 16: Confidence 0.3026335537433624 : Bounding Box [     1274.5      726.81      2609.8      1485.4]\n","Class motorcycle : ID 20: Confidence 0.307197242975235 : Bounding Box [     1729.8      1016.2      3758.5      2455.2]\n","Class umbrella : ID 21: Confidence 0.28412771224975586 : Bounding Box [     1572.2      739.01      3268.8      1545.5]\n","Class boat : ID 31: Confidence None : Bounding Box [     1260.2      824.94      2900.2        1871]\n","Logic frame count : 173\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 bicycle, 1 motorcycle, 2 umbrellas, 165.3ms\n","Speed: 4.8ms preprocess, 165.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(654.6972), tensor(863.3663), tensor(762.6209), tensor(1133.8212)], tensor(0.7576), 0), ([tensor(1571.7822), tensor(739.1620), tensor(1698.5425), tensor(806.3961)], tensor(0.3414), 25), ([tensor(1269.6425), tensor(726.8380), tensor(1345.2103), tensor(758.7706)], tensor(0.3141), 25), ([tensor(1726.5039), tensor(1169.2783), tensor(1907.6460), tensor(1440.)], tensor(0.3013), 1), ([tensor(1731.3743), tensor(1016.2803), tensor(2031.7068), tensor(1439.7544)], tensor(0.2571), 3)]\n","Class person : ID 2: Confidence 0.7576361298561096 : Bounding Box [      633.2      861.44      1445.1      1993.3]\n","Class umbrella : ID 16: Confidence 0.31407520174980164 : Bounding Box [     1274.1      726.84      2610.3      1485.4]\n","Class motorcycle : ID 20: Confidence 0.2571253478527069 : Bounding Box [     1731.5      1016.2        3761      2455.7]\n","Class umbrella : ID 21: Confidence 0.3413669764995575 : Bounding Box [     1572.6      739.13      3269.2      1545.6]\n","Class boat : ID 31: Confidence None : Bounding Box [     1255.9      829.53      2912.1      1885.9]\n","Logic frame count : 174\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 172.1ms\n","Speed: 4.7ms preprocess, 172.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(648.6014), tensor(859.1768), tensor(757.0333), tensor(1132.3467)], tensor(0.6568), 0), ([tensor(1728.7334), tensor(1016.1226), tensor(2031.3345), tensor(1438.6421)], tensor(0.3518), 3), ([tensor(1571.3560), tensor(737.1716), tensor(1698.9834), tensor(806.4760)], tensor(0.3032), 25), ([tensor(1269.3655), tensor(726.7423), tensor(1345.1477), tensor(759.6998)], tensor(0.2914), 25)]\n","Class person : ID 2: Confidence 0.6568236947059631 : Bounding Box [     624.83      860.11      1431.2      1991.6]\n","Class umbrella : ID 16: Confidence 0.29142042994499207 : Bounding Box [       1273      726.78        2611        1486]\n","Class motorcycle : ID 20: Confidence 0.35181891918182373 : Bounding Box [     1730.6      1016.1      3759.7        2455]\n","Class umbrella : ID 21: Confidence 0.3032304644584656 : Bounding Box [     1572.4      737.86      3269.4      1544.3]\n","Class boat : ID 31: Confidence None : Bounding Box [     1251.6      834.12      2923.9      1900.8]\n","Logic frame count : 175\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 2 umbrellas, 154.7ms\n","Speed: 4.6ms preprocess, 154.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(623.5038), tensor(854.1952), tensor(752.2843), tensor(1132.4049)], tensor(0.5936), 0), ([tensor(1730.1826), tensor(1016.1353), tensor(2033.9177), tensor(1437.1997)], tensor(0.3616), 3), ([tensor(1570.5007), tensor(736.6050), tensor(1699.3547), tensor(806.7355)], tensor(0.3341), 25), ([tensor(1269.2693), tensor(726.7700), tensor(1343.6433), tensor(759.6468)], tensor(0.2724), 25), ([tensor(1311.8684), tensor(820.2095), tensor(1532.4246), tensor(1031.5076)], tensor(0.2564), 8)]\n","Class person : ID 2: Confidence 0.5936446189880371 : Bounding Box [     605.45       856.3      1406.5      1987.8]\n","Class umbrella : ID 16: Confidence 0.27241888642311096 : Bounding Box [     1271.9      726.78      2610.8      1486.2]\n","Class motorcycle : ID 20: Confidence 0.3615896999835968 : Bounding Box [     1732.5        1016      3760.6      2453.8]\n","Class umbrella : ID 21: Confidence 0.33407050371170044 : Bounding Box [     1571.7         737      3269.2      1543.6]\n","Class boat : ID 31: Confidence 0.25640246272087097 : Bounding Box [     1272.4      822.43      2886.9      1859.4]\n","Logic frame count : 176\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 2 umbrellas, 177.1ms\n","Speed: 9.0ms preprocess, 177.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(578.0031), tensor(848.9728), tensor(749.4062), tensor(1129.6833)], tensor(0.6337), 0), ([tensor(1571.5332), tensor(738.4976), tensor(1699.3130), tensor(806.6589)], tensor(0.3279), 25), ([tensor(1727.7814), tensor(1016.0872), tensor(2031.4171), tensor(1437.1716)], tensor(0.3048), 3), ([tensor(1269.3972), tensor(727.0437), tensor(1344.0935), tensor(759.4404)], tensor(0.2631), 25)]\n","Class person : ID 2: Confidence 0.6337359547615051 : Bounding Box [     569.54       851.4      1364.5      1981.2]\n","Class umbrella : ID 16: Confidence 0.2631314992904663 : Bounding Box [     1271.7      726.96        2611      1486.3]\n","Class motorcycle : ID 20: Confidence 0.3047518730163574 : Bounding Box [     1730.7        1016      3758.6      2453.3]\n","Class umbrella : ID 21: Confidence 0.327850878238678 : Bounding Box [     1572.1      737.93      3269.8      1544.6]\n","Class boat : ID 31: Confidence None : Bounding Box [     1269.9      825.56      2894.9      1869.2]\n","Logic frame count : 177\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 2 umbrellas, 165.8ms\n","Speed: 4.2ms preprocess, 165.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(596.0005), tensor(846.8840), tensor(749.4986), tensor(1128.2639)], tensor(0.6371), 0), ([tensor(1728.0222), tensor(1016.1519), tensor(2031.2131), tensor(1437.5811)], tensor(0.3097), 3), ([tensor(1571.2063), tensor(739.6729), tensor(1699.8689), tensor(806.3523)], tensor(0.2970), 25), ([tensor(1269.3362), tensor(726.8750), tensor(1345.1653), tensor(759.6609)], tensor(0.2843), 25), ([tensor(1312.1162), tensor(834.1925), tensor(1530.1118), tensor(1031.9961)], tensor(0.2618), 8)]\n","Class person : ID 2: Confidence 0.6370808482170105 : Bounding Box [     569.93      848.18      1359.5      1976.4]\n","Class umbrella : ID 16: Confidence 0.2843457758426666 : Bounding Box [     1271.6      726.91      2611.7      1486.4]\n","Class motorcycle : ID 20: Confidence 0.3096632957458496 : Bounding Box [     1729.8        1016      3758.1      2453.5]\n","Class umbrella : ID 21: Confidence 0.29703086614608765 : Bounding Box [     1572.3      739.07      3269.9      1545.5]\n","Class boat : ID 31: Confidence 0.2618452310562134 : Bounding Box [     1277.5      833.04      2880.1      1868.9]\n","Logic frame count : 178\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 umbrella, 168.6ms\n","Speed: 4.7ms preprocess, 168.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(585.1654), tensor(842.4097), tensor(746.4854), tensor(1127.3413)], tensor(0.6363), 0), ([tensor(1727.2244), tensor(1015.9851), tensor(2031.4890), tensor(1437.1829)], tensor(0.4067), 3), ([tensor(1269.2490), tensor(726.6143), tensor(1344.6699), tensor(760.3666)], tensor(0.2841), 25)]\n","Class person : ID 2: Confidence 0.6363482475280762 : Bounding Box [     563.88      844.03      1348.6      1971.1]\n","Class umbrella : ID 16: Confidence 0.28412771224975586 : Bounding Box [     1270.9      726.72      2612.3      1486.7]\n","Class motorcycle : ID 20: Confidence 0.4067316949367523 : Bounding Box [     1729.1        1016      3757.5      2453.2]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1572.4      739.15        3270      1545.6]\n","Class boat : ID 31: Confidence None : Bounding Box [     1275.6       836.6      2886.3      1877.7]\n","Logic frame count : 179\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 162.2ms\n","Speed: 4.0ms preprocess, 162.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(599.5217), tensor(850.9435), tensor(745.1491), tensor(1127.8430)], tensor(0.6234), 0), ([tensor(1727.7590), tensor(1016.3005), tensor(2031.2971), tensor(1437.9631)], tensor(0.3685), 3)]\n","Class person : ID 2: Confidence 0.6234100461006165 : Bounding Box [     572.18      848.16      1352.9      1975.2]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1270.9      726.72      2612.1      1486.6]\n","Class motorcycle : ID 20: Confidence 0.36852067708969116 : Bounding Box [     1728.8      1016.1      3757.9      2453.8]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1572.6      739.23      3270.1      1545.6]\n","Class boat : ID 31: Confidence None : Bounding Box [     1273.6      840.16      2892.4      1886.6]\n","Logic frame count : 180\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 168.1ms\n","Speed: 4.3ms preprocess, 168.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(640.6410), tensor(841.7189), tensor(743.1688), tensor(1122.2465)], tensor(0.6640), 0), ([tensor(1726.5476), tensor(1015.7802), tensor(2031.4084), tensor(1436.4291)], tensor(0.5540), 3)]\n","Class person : ID 2: Confidence 0.6639697551727295 : Bounding Box [      604.5      843.66      1379.4      1967.1]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1270.9      726.72      2611.9      1486.5]\n","Class motorcycle : ID 20: Confidence 0.5540333986282349 : Bounding Box [     1728.4      1015.9      3756.8      2452.7]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1572.7      739.32      3270.2      1545.7]\n","Class boat : ID 31: Confidence None : Bounding Box [     1271.6      843.72      2898.6      1895.4]\n","Logic frame count : 181\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 1 umbrella, 168.5ms\n","Speed: 5.2ms preprocess, 168.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(640.4436), tensor(829.1663), tensor(728.9595), tensor(1109.4863)], tensor(0.6461), 0), ([tensor(1727.1359), tensor(1016.4750), tensor(2031.3649), tensor(1436.7389)], tensor(0.4018), 3), ([tensor(1317.9070), tensor(804.1306), tensor(1577.1399), tensor(1017.2346)], tensor(0.3156), 3), ([tensor(1571.9915), tensor(742.4552), tensor(1691.0327), tensor(805.2623)], tensor(0.2583), 25)]\n","Class person : ID 2: Confidence 0.6460922956466675 : Bounding Box [     616.26      833.68      1380.9      1947.3]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1270.9      726.72      2611.8      1486.4]\n","Class motorcycle : ID 20: Confidence 0.40176549553871155 : Bounding Box [     1728.4      1016.2      3756.9      2452.9]\n","Class umbrella : ID 21: Confidence 0.25831595063209534 : Bounding Box [     1570.5      742.11      3265.4      1547.5]\n","Class motorcycle : ID 31: Confidence 0.31561028957366943 : Bounding Box [     1313.6      809.08      2894.9      1830.9]\n","Logic frame count : 182\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 157.7ms\n","Speed: 6.7ms preprocess, 157.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(635.6494), tensor(824.2932), tensor(721.3115), tensor(1107.6753)], tensor(0.6688), 0), ([tensor(1320.5242), tensor(803.8494), tensor(1582.4592), tensor(1013.7615)], tensor(0.4101), 3), ([tensor(1728.8916), tensor(1016.3325), tensor(2031.2922), tensor(1437.0405)], tensor(0.3588), 3)]\n","Class person : ID 2: Confidence 0.6687704920768738 : Bounding Box [     616.82      826.74      1374.3      1935.6]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1270.9      726.72      2611.6      1486.3]\n","Class motorcycle : ID 20: Confidence 0.3587570786476135 : Bounding Box [     1729.4      1016.3      3758.4      2453.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.4      742.45        3265      1547.7]\n","Class motorcycle : ID 31: Confidence 0.4100978970527649 : Bounding Box [     1322.8      805.45      2898.3      1822.3]\n","Logic frame count : 183\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 163.8ms\n","Speed: 4.1ms preprocess, 163.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(634.4431), tensor(825.1467), tensor(717.0137), tensor(1096.1536)], tensor(0.6474), 0), ([tensor(1729.2271), tensor(1016.7556), tensor(2031.3369), tensor(1436.7722)], tensor(0.3165), 3), ([tensor(1325.5298), tensor(824.1860), tensor(1533.1372), tensor(1007.4724)], tensor(0.2926), 3)]\n","Class person : ID 2: Confidence 0.6473991870880127 : Bounding Box [     618.39      824.79      1366.4      1924.3]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1270.9      726.71      2611.5      1486.2]\n","Class motorcycle : ID 20: Confidence 0.31650376319885254 : Bounding Box [       1730      1016.6      3759.1      2453.3]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.3      742.79      3264.7      1547.9]\n","Class motorcycle : ID 31: Confidence 0.29260241985321045 : Bounding Box [     1318.1       817.8      2881.1      1828.8]\n","Logic frame count : 184\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 185.4ms\n","Speed: 4.3ms preprocess, 185.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(629.9059), tensor(826.8247), tensor(719.2147), tensor(1098.4746)], tensor(0.6951), 0), ([tensor(1331.2722), tensor(816.3741), tensor(1545.4338), tensor(1007.1599)], tensor(0.4158), 3)]\n","Class person : ID 2: Confidence 0.6950753927230835 : Bounding Box [     617.08      825.28      1361.1      1922.9]\n","Class umbrella : ID 16: Confidence None : Bounding Box [     1270.9      726.71      2611.3      1486.1]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.2      1016.6      3759.1      2453.2]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.2      743.13      3264.3      1548.2]\n","Class motorcycle : ID 31: Confidence 0.4158335328102112 : Bounding Box [     1324.4       817.2      2882.2      1825.8]\n","Logic frame count : 185\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 242.1ms\n","Speed: 4.3ms preprocess, 242.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(627.3895), tensor(825.5815), tensor(717.5406), tensor(1096.5222)], tensor(0.7469), 0), ([tensor(1324.8682), tensor(807.4216), tensor(1547.7661), tensor(1006.1201)], tensor(0.4574), 3), ([tensor(1521.0962), tensor(808.8100), tensor(1605.0142), tensor(917.4274)], tensor(0.2689), 3)]\n","Class person : ID 2: Confidence 0.7469438314437866 : Bounding Box [     615.67      824.73      1355.9      1920.5]\n","Class motorcycle : ID 16: Confidence 0.2689063549041748 : Bounding Box [     1501.5      804.26      3103.4      1712.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.4      1016.5      3759.1        2453]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1570.1      743.47        3264      1548.4]\n","Class motorcycle : ID 31: Confidence 0.45743662118911743 : Bounding Box [     1323.8         811      2878.4        1818]\n","Logic frame count : 186\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 252.5ms\n","Speed: 4.7ms preprocess, 252.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(623.0431), tensor(826.2170), tensor(719.4467), tensor(1094.0444)], tensor(0.7448), 0), ([tensor(1332.2817), tensor(797.1116), tensor(1562.0840), tensor(1001.7537)], tensor(0.2731), 3), ([tensor(1729.6028), tensor(1016.6862), tensor(2031.5098), tensor(1436.8007)], tensor(0.2566), 3)]\n","Class person : ID 2: Confidence 0.7447863817214966 : Bounding Box [     614.05      825.03      1350.8      1918.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1520.2       810.6      3143.5      1731.4]\n","Class motorcycle : ID 20: Confidence 0.2565937936306 : Bounding Box [     1730.6      1016.7      3759.9      2453.4]\n","Class umbrella : ID 21: Confidence None : Bounding Box [       1570      743.81      3263.6      1548.6]\n","Class motorcycle : ID 31: Confidence 0.2730858623981476 : Bounding Box [     1334.4      801.81      2885.3      1805.3]\n","Logic frame count : 187\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 umbrella, 238.5ms\n","Speed: 7.4ms preprocess, 238.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(620.4387), tensor(825.4878), tensor(718.3821), tensor(1098.7720)], tensor(0.7017), 0), ([tensor(1569.4480), tensor(741.5571), tensor(1688.8923), tensor(807.4358)], tensor(0.2813), 25)]\n","Class person : ID 2: Confidence 0.7017284631729126 : Bounding Box [     611.13      824.74      1347.4      1920.7]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [       1539      816.94      3183.6      1749.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.8      1016.7        3760      2453.3]\n","Class umbrella : ID 21: Confidence 0.28125810623168945 : Bounding Box [     1565.5      741.73      3262.7        1549]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1337.4       800.8      2887.7      1803.9]\n","Logic frame count : 188\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 247.4ms\n","Speed: 4.4ms preprocess, 247.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(619.2338), tensor(834.2253), tensor(708.5227), tensor(1093.9646)], tensor(0.6655), 0)]\n","Class person : ID 2: Confidence 0.6654963493347168 : Bounding Box [     608.04      830.47      1340.3      1924.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1557.8      823.27      3223.8      1768.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1731      1016.7      3760.1      2453.2]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1564.9      741.87      3262.3      1549.2]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1340.5      799.79        2890      1802.4]\n","Logic frame count : 189\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 224.4ms\n","Speed: 9.3ms preprocess, 224.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(614.6413), tensor(832.8488), tensor(715.8893), tensor(1094.7604)], tensor(0.6729), 0), ([tensor(1361.3529), tensor(799.8846), tensor(1598.4379), tensor(990.1855)], tensor(0.4007), 3), ([tensor(1730.6285), tensor(1017.1787), tensor(2033.7411), tensor(1437.0073)], tensor(0.3229), 3)]\n","Class person : ID 2: Confidence 0.6729214787483215 : Bounding Box [     606.57      831.78        1337      1925.4]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1576.5      829.61      3263.9      1786.7]\n","Class motorcycle : ID 20: Confidence 0.32293665409088135 : Bounding Box [     1732.2      1017.1      3762.3        2454]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1564.4         742        3262      1549.5]\n","Class motorcycle : ID 31: Confidence 0.4006898105144501 : Bounding Box [     1383.2      799.72      2924.9      1791.7]\n","Logic frame count : 190\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 umbrella, 223.3ms\n","Speed: 12.7ms preprocess, 223.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(615.9976), tensor(830.5013), tensor(710.1162), tensor(1090.9362)], tensor(0.6364), 0), ([tensor(1571.0713), tensor(740.8142), tensor(1686.0898), tensor(806.3755)], tensor(0.3230), 25)]\n","Class person : ID 2: Confidence 0.6363751888275146 : Bounding Box [     606.57      830.75      1333.5      1921.9]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1595.3      835.94        3304      1805.2]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1732.5      1017.1      3762.5        2454]\n","Class umbrella : ID 21: Confidence 0.32298019528388977 : Bounding Box [     1566.8      741.01      3261.1      1547.6]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1390.8       798.8      2930.1      1789.3]\n","Logic frame count : 191\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 229.7ms\n","Speed: 4.6ms preprocess, 229.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(603.5254), tensor(830.8966), tensor(710.9359), tensor(1094.6204)], tensor(0.6767), 0), ([tensor(1730.2793), tensor(1017.3145), tensor(2034.1831), tensor(1437.1299)], tensor(0.3932), 3), ([tensor(1380.0684), tensor(782.7588), tensor(1621.4907), tensor(982.7158)], tensor(0.3242), 3)]\n","Class person : ID 2: Confidence 0.6766626834869385 : Bounding Box [     598.45      830.64      1324.6      1923.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1614.1      842.28      3344.2      1823.6]\n","Class motorcycle : ID 20: Confidence 0.39315858483314514 : Bounding Box [     1732.1      1017.3      3762.9      2454.4]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1566.4      741.04      3260.7      1547.6]\n","Class motorcycle : ID 31: Confidence 0.32419735193252563 : Bounding Box [     1415.3      785.88      2956.4      1769.9]\n","Logic frame count : 192\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 242.5ms\n","Speed: 6.5ms preprocess, 242.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(599.1862), tensor(827.1294), tensor(717.7208), tensor(1100.8911)], tensor(0.7075), 0), ([tensor(1730.3616), tensor(1016.0774), tensor(2032.3767), tensor(1438.3928)], tensor(0.2897), 3)]\n","Class person : ID 2: Confidence 0.7074602246284485 : Bounding Box [     593.86      828.13      1321.8      1925.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1632.8      848.62      3384.3        1842]\n","Class motorcycle : ID 20: Confidence 0.28972914814949036 : Bounding Box [     1730.9      1016.5      3762.9      2454.4]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1566.1      741.06      3260.4      1547.6]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1425.3       783.7      2963.2      1765.7]\n","Logic frame count : 193\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 220.7ms\n","Speed: 8.5ms preprocess, 220.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(593.7313), tensor(829.7887), tensor(716.7548), tensor(1101.0721)], tensor(0.6933), 0), ([tensor(1729.7948), tensor(1015.6954), tensor(2032.4850), tensor(1438.5778)], tensor(0.2958), 3)]\n","Class person : ID 2: Confidence 0.6933348178863525 : Bounding Box [     588.72      828.97      1316.6      1928.5]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1651.6      854.95      3424.5      1860.5]\n","Class motorcycle : ID 20: Confidence 0.295774906873703 : Bounding Box [     1730.1      1015.9      3762.7      2454.3]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1565.8      741.09      3260.1      1547.7]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1435.3      781.52      2970.1      1761.5]\n","Logic frame count : 194\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 199.5ms\n","Speed: 10.0ms preprocess, 199.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(593.5701), tensor(833.9877), tensor(711.2019), tensor(1099.2433)], tensor(0.7314), 0), ([tensor(1730.8162), tensor(1016.4586), tensor(2034.8616), tensor(1437.4945)], tensor(0.4110), 3)]\n","Class person : ID 2: Confidence 0.7314469814300537 : Bounding Box [     585.95      832.08      1311.9      1931.2]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1670.4      861.29      3464.6      1878.9]\n","Class motorcycle : ID 20: Confidence 0.410993367433548 : Bounding Box [     1731.5      1016.3      3763.8      2454.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1565.4      741.12      3259.7      1547.7]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1445.3      779.35      2976.9      1757.3]\n","Logic frame count : 195\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 327.3ms\n","Speed: 9.5ms preprocess, 327.3ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(576.7044), tensor(819.0165), tensor(704.1746), tensor(1100.2772)], tensor(0.6435), 0), ([tensor(1416.9778), tensor(774.4026), tensor(1619.5430), tensor(963.7168)], tensor(0.3868), 3), ([tensor(1729.8379), tensor(1015.6332), tensor(2032.8513), tensor(1437.8246)], tensor(0.2903), 3)]\n","Class person : ID 2: Confidence 0.6435344219207764 : Bounding Box [     572.17      823.39      1296.3        1923]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1689.1      867.62      3504.7      1897.4]\n","Class motorcycle : ID 20: Confidence 0.2903355360031128 : Bounding Box [     1730.7      1015.8        3763      2453.7]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1565.1      741.15      3259.4      1547.7]\n","Class motorcycle : ID 31: Confidence 0.38677477836608887 : Bounding Box [     1462.4       774.7      2989.5      1739.8]\n","Logic frame count : 196\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 143.7ms\n","Speed: 9.6ms preprocess, 143.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(580.7921), tensor(815.2961), tensor(697.3983), tensor(1102.6487)], tensor(0.6195), 0)]\n","Class person : ID 2: Confidence 0.6194969415664673 : Bounding Box [     568.05      817.66      1290.5      1919.1]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1707.9      873.96      3544.9      1915.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.7      1015.7      3763.1      2453.6]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1564.7      741.17      3259.1      1547.8]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1473.9      772.29      2996.1      1734.3]\n","Logic frame count : 197\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 3 motorcycles, 195.6ms\n","Speed: 6.2ms preprocess, 195.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1436.7666), tensor(823.4061), tensor(1621.5396), tensor(952.8389)], tensor(0.6123), 3), ([tensor(571.6349), tensor(821.8989), tensor(685.8717), tensor(1100.0374)], tensor(0.5840), 0), ([tensor(1437.4496), tensor(768.3959), tensor(1624.6190), tensor(954.2168)], tensor(0.2876), 3), ([tensor(1730.1418), tensor(1015.4058), tensor(2031.8013), tensor(1437.4424)], tensor(0.2856), 3)]\n","Class person : ID 2: Confidence 0.5839844942092896 : Bounding Box [     558.62      819.91        1277      1920.3]\n","Class motorcycle : ID 16: Confidence 0.28757941722869873 : Bounding Box [     1425.3      772.16      3101.5      1729.7]\n","Class motorcycle : ID 20: Confidence 0.2856343984603882 : Bounding Box [     1730.3      1015.5      3762.2        2453]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1564.4       741.2      3258.8      1547.8]\n","Class motorcycle : ID 31: Confidence 0.6122562289237976 : Bounding Box [       1484      812.53      3009.6      1766.6]\n","Logic frame count : 198\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 156.2ms\n","Speed: 5.3ms preprocess, 156.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(578.9796), tensor(826.8821), tensor(688.5043), tensor(1097.9255)], tensor(0.5734), 0), ([tensor(532.1959), tensor(820.5591), tensor(684.3932), tensor(1093.4943)], tensor(0.4364), 0)]\n","Class person : ID 2: Confidence 0.5734034180641174 : Bounding Box [     562.16      824.11      1276.6      1922.8]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1425.1      771.83      3112.2      1735.6]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.3      1015.3      3762.2      2452.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1564.1      741.23      3258.4      1547.8]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1496.2      814.57      3016.1      1765.1]\n","Logic frame count : 199\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 170.0ms\n","Speed: 5.3ms preprocess, 170.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(536.9022), tensor(824.4694), tensor(686.0231), tensor(1095.5991)], tensor(0.5198), 0)]\n","Class person : ID 2: Confidence 0.5197986364364624 : Bounding Box [     536.35      824.14      1246.7      1920.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [       1425       771.5      3122.9      1741.4]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.3      1015.2      3762.3      2452.8]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1563.7      741.26      3258.1      1547.8]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1508.3      816.61      3022.6      1763.6]\n","Logic frame count : 200\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 1 umbrella, 193.8ms\n","Speed: 5.8ms preprocess, 193.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(527.0122), tensor(821.4030), tensor(681.8677), tensor(1094.6915)], tensor(0.5902), 0), ([tensor(1454.5261), tensor(816.3740), tensor(1641.9895), tensor(943.8545)], tensor(0.2915), 8), ([tensor(1269.9014), tensor(726.7507), tensor(1345.7100), tensor(759.9773)], tensor(0.2701), 25)]\n","Class person : ID 2: Confidence 0.5901668071746826 : Bounding Box [     519.96      822.15      1226.7      1917.2]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1424.8      771.17      3133.5      1747.2]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.4      1015.1      3762.3      2452.7]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1563.4      741.28      3257.8      1547.9]\n","Class boat : ID 31: Confidence 0.2915206551551819 : Bounding Box [     1511.2      816.71      3039.6      1760.5]\n","Logic frame count : 201\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 163.5ms\n","Speed: 4.7ms preprocess, 163.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(529.9843), tensor(831.0862), tensor(655.3975), tensor(1068.0189)], tensor(0.5068), 0), ([tensor(543.5972), tensor(832.5083), tensor(680.7252), tensor(1088.1936)], tensor(0.4472), 0), ([tensor(1458.1458), tensor(820.2333), tensor(1645.4260), tensor(934.6041)], tensor(0.2613), 8)]\n","Class person : ID 2: Confidence 0.5067878365516663 : Bounding Box [     514.14      827.83      1205.9      1904.7]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1424.7      770.84      3144.2      1753.1]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.4        1015      3762.3      2452.5]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1563.1      741.31      3257.5      1547.9]\n","Class boat : ID 31: Confidence 0.261312335729599 : Bounding Box [       1515      819.71      3049.2      1756.1]\n","Logic frame count : 202\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 159.1ms\n","Speed: 11.8ms preprocess, 159.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(533.1344), tensor(826.0297), tensor(654.6854), tensor(1082.2003)], tensor(0.5848), 0)]\n","Class person : ID 2: Confidence 0.5847830176353455 : Bounding Box [     512.47      826.63        1202      1906.2]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1424.5      770.51      3154.9      1758.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.4      1014.9      3762.3      2452.4]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1562.7      741.34      3257.1      1547.9]\n","Class boat : ID 31: Confidence None : Bounding Box [     1527.3      821.67        3055      1754.1]\n","Logic frame count : 203\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 motorcycle, 168.3ms\n","Speed: 4.1ms preprocess, 168.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(529.2440), tensor(821.2081), tensor(644.7279), tensor(1072.9789)], tensor(0.4356), 0), ([tensor(521.7075), tensor(815.6538), tensor(682.7612), tensor(1087.3644)], tensor(0.3419), 0), ([tensor(1729.4375), tensor(1015.9370), tensor(2033.5874), tensor(1437.7285)], tensor(0.2594), 3)]\n","Class person : ID 2: Confidence 0.43558648228645325 : Bounding Box [     509.45         823      1191.8      1897.5]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1424.4      770.18      3165.6      1764.7]\n","Class motorcycle : ID 20: Confidence 0.2594124376773834 : Bounding Box [       1730      1015.9      3762.4      2453.6]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1562.4      741.36      3256.8        1548]\n","Class boat : ID 31: Confidence None : Bounding Box [     1539.7      823.64      3060.9      1752.1]\n","Logic frame count : 204\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 160.5ms\n","Speed: 10.7ms preprocess, 160.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(529.8756), tensor(808.3853), tensor(682.0116), tensor(1085.7449)], tensor(0.4702), 0)]\n","Class person : ID 2: Confidence 0.470182865858078 : Bounding Box [     518.37      813.13      1204.1      1894.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1424.2      769.85      3176.2      1770.6]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1730      1015.8      3762.5      2453.6]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1562.1      741.39      3256.5        1548]\n","Class boat : ID 31: Confidence None : Bounding Box [       1552      825.61      3066.7      1750.1]\n","Logic frame count : 205\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 156.3ms\n","Speed: 11.6ms preprocess, 156.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(529.9755), tensor(814.7606), tensor(661.4974), tensor(1078.3511)], tensor(0.6112), 0), ([tensor(1487.6389), tensor(732.0308), tensor(1703.4272), tensor(923.2869)], tensor(0.4392), 3), ([tensor(1729.7058), tensor(1016.3772), tensor(2032.2751), tensor(1436.6199)], tensor(0.3205), 3)]\n","Class person : ID 2: Confidence 0.6112223267555237 : Bounding Box [     517.77      813.67      1199.8      1892.5]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1424.1      769.51      3186.9      1776.4]\n","Class motorcycle : ID 20: Confidence 0.32045215368270874 : Bounding Box [     1730.2      1016.3      3761.6      2453.1]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1561.7      741.42      3256.1        1548]\n","Class motorcycle : ID 31: Confidence 0.4391687512397766 : Bounding Box [     1566.8      742.48      3107.3      1665.5]\n","Logic frame count : 206\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 164.1ms\n","Speed: 5.8ms preprocess, 164.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(528.6050), tensor(810.3203), tensor(653.9903), tensor(1075.8658)], tensor(0.6046), 0), ([tensor(1729.5330), tensor(1016.5061), tensor(2032.0022), tensor(1436.4041)], tensor(0.3526), 3)]\n","Class person : ID 2: Confidence 0.6045993566513062 : Bounding Box [     515.76         811      1193.7      1887.4]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1423.9      769.18      3197.6      1782.2]\n","Class motorcycle : ID 20: Confidence 0.35263314843177795 : Bounding Box [     1730.1      1016.4      3761.2        2453]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1561.4      741.45      3255.8      1548.1]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1580.8      736.12      3115.1      1655.4]\n","Logic frame count : 207\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 160.6ms\n","Speed: 8.5ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(525.3718), tensor(817.2808), tensor(644.8164), tensor(1069.0510)], tensor(0.6111), 0)]\n","Class person : ID 2: Confidence 0.6111091375350952 : Bounding Box [     512.37      814.66      1184.1      1885.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1423.8      768.85      3208.2      1788.1]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.1      1016.5      3761.1      2452.9]\n","Class umbrella : ID 21: Confidence None : Bounding Box [       1561      741.47      3255.5      1548.1]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1594.8      729.76        3123      1645.3]\n","Logic frame count : 208\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 163.6ms\n","Speed: 6.0ms preprocess, 163.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(520.8109), tensor(811.5184), tensor(632.9586), tensor(1068.4404)], tensor(0.5088), 0), ([tensor(1517.2031), tensor(715.4260), tensor(1715.2520), tensor(910.0120)], tensor(0.2516), 8)]\n","Class person : ID 2: Confidence 0.5087568163871765 : Bounding Box [     505.75      812.27      1172.4      1880.9]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1423.6      768.52      3218.9      1793.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.1      1016.5        3761      2452.8]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1560.7       741.5      3255.2      1548.1]\n","Class boat : ID 31: Confidence 0.2516234517097473 : Bounding Box [     1599.6      716.57      3148.6      1626.9]\n","Logic frame count : 209\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 161.3ms\n","Speed: 5.5ms preprocess, 161.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(520.6523), tensor(805.8380), tensor(633.6281), tensor(1068.2021)], tensor(0.7036), 0), ([tensor(1522.0802), tensor(734.2040), tensor(1709.8529), tensor(910.2006)], tensor(0.4846), 3), ([tensor(1730.1227), tensor(1015.3236), tensor(2033.5597), tensor(1437.7535)], tensor(0.3410), 3)]\n","Class person : ID 2: Confidence 0.7036237716674805 : Bounding Box [     504.54      807.64      1167.4      1875.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1423.5      768.19      3229.6      1799.7]\n","Class motorcycle : ID 20: Confidence 0.34097447991371155 : Bounding Box [     1730.4      1015.5        3763        2453]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1560.4      741.53      3254.8      1548.2]\n","Class motorcycle : ID 31: Confidence 0.4845687448978424 : Bounding Box [       1595      726.49      3164.3      1635.5]\n","Logic frame count : 210\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 161.9ms\n","Speed: 5.7ms preprocess, 161.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(519.2001), tensor(810.5052), tensor(626.5038), tensor(1066.0714)], tensor(0.5940), 0), ([tensor(1730.7920), tensor(1015.1755), tensor(2033.3215), tensor(1437.5969)], tensor(0.3041), 3)]\n","Class person : ID 2: Confidence 0.5939533114433289 : Bounding Box [     502.53      809.04      1160.9        1875]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1423.3      767.86      3240.2      1805.6]\n","Class motorcycle : ID 20: Confidence 0.30414703488349915 : Bounding Box [     1730.8      1015.3      3763.6      2452.8]\n","Class umbrella : ID 21: Confidence None : Bounding Box [       1560      741.56      3254.5      1548.2]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1608.8      721.28      3171.9      1626.7]\n","Logic frame count : 211\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 159.7ms\n","Speed: 4.0ms preprocess, 159.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(515.7745), tensor(810.5909), tensor(626.7120), tensor(1075.5499)], tensor(0.5796), 0), ([tensor(1730.3939), tensor(1015.9658), tensor(2033.4872), tensor(1437.9766)], tensor(0.3149), 3)]\n","Class person : ID 2: Confidence 0.5796123743057251 : Bounding Box [     499.01      809.68      1157.2      1881.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1423.2      767.53      3250.9      1811.4]\n","Class motorcycle : ID 20: Confidence 0.3148893713951111 : Bounding Box [     1730.6      1015.7      3763.8      2453.6]\n","Class umbrella : ID 21: Confidence None : Bounding Box [     1559.7      741.58      3254.2      1548.2]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1622.6      716.08      3179.5      1617.9]\n","Logic frame count : 212\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 2 motorcycles, 165.6ms\n","Speed: 4.1ms preprocess, 165.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(521.5622), tensor(829.4935), tensor(633.3593), tensor(1080.8011)], tensor(0.5897), 0), ([tensor(1546.2456), tensor(723.4144), tensor(1730.8857), tensor(902.0951)], tensor(0.3288), 3), ([tensor(1730.3604), tensor(1015.7019), tensor(2033.7998), tensor(1438.1624)], tensor(0.3152), 3), ([tensor(1619.7632), tensor(651.1110), tensor(1666.9336), tensor(717.1396)], tensor(0.2714), 0), ([tensor(1617.5261), tensor(653.8400), tensor(1670.9309), tensor(746.7599)], tensor(0.2572), 0)]\n","Class person : ID 2: Confidence 0.5897049307823181 : Bounding Box [     503.62      822.44      1162.3      1899.8]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [       1423       767.2      3261.6      1817.2]\n","Class motorcycle : ID 20: Confidence 0.31521597504615784 : Bounding Box [     1730.5      1015.7      3764.1      2453.8]\n","Class person : ID 21: Confidence 0.27136123180389404 : Bounding Box [       1677      651.72      3228.8      1369.5]\n","Class motorcycle : ID 31: Confidence 0.32877394556999207 : Bounding Box [       1621      721.53      3202.4        1623]\n","Logic frame count : 213\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 2 motorcycles, 174.2ms\n","Speed: 8.3ms preprocess, 174.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(522.3662), tensor(836.6932), tensor(640.9993), tensor(1087.1818)], tensor(0.4657), 0), ([tensor(1729.5244), tensor(1015.4707), tensor(2034.2524), tensor(1437.9897)], tensor(0.3906), 3), ([tensor(1552.4619), tensor(717.8495), tensor(1740.4292), tensor(898.3517)], tensor(0.3743), 3), ([tensor(520.6672), tensor(827.9021), tensor(726.1062), tensor(1085.6500)], tensor(0.2514), 0)]\n","Class person : ID 2: Confidence 0.2513832449913025 : Bounding Box [      531.1      826.24      1198.9      1908.9]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1422.9      766.87      3272.3      1823.1]\n","Class motorcycle : ID 20: Confidence 0.39062315225601196 : Bounding Box [     1730.1      1015.5      3763.7      2453.6]\n","Class person : ID 21: Confidence None : Bounding Box [     1683.3      647.58      3226.2      1361.2]\n","Class motorcycle : ID 31: Confidence 0.3742953836917877 : Bounding Box [     1623.7      717.71      3221.5        1616]\n","Logic frame count : 214\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 motorcycle, 168.6ms\n","Speed: 6.1ms preprocess, 168.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(530.0732), tensor(841.4344), tensor(701.5339), tensor(1088.6447)], tensor(0.4765), 0), ([tensor(530.8875), tensor(843.7632), tensor(649.9406), tensor(1086.6669)], tensor(0.3417), 0), ([tensor(1730.1235), tensor(1016.0378), tensor(2033.0283), tensor(1437.8103)], tensor(0.2796), 3)]\n","Class person : ID 2: Confidence 0.47648152709007263 : Bounding Box [     538.82      836.57      1211.9      1923.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1422.7      766.54      3282.9      1828.9]\n","Class motorcycle : ID 20: Confidence 0.27958622574806213 : Bounding Box [       1730      1015.9      3763.4      2453.8]\n","Class person : ID 21: Confidence None : Bounding Box [     1689.6      643.43      3223.5      1352.9]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1637.3      713.63      3229.4      1608.7]\n","Logic frame count : 215\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 174.5ms\n","Speed: 4.2ms preprocess, 174.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(526.6670), tensor(827.9244), tensor(640.8931), tensor(1089.1620)], tensor(0.5901), 0)]\n","Class person : ID 2: Confidence 0.5900638103485107 : Bounding Box [     521.58      831.54      1192.5      1920.1]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1422.6      766.21      3293.6      1834.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1730      1015.9      3763.5      2453.8]\n","Class person : ID 21: Confidence None : Bounding Box [     1695.8      639.29      3220.9      1344.7]\n","Class motorcycle : ID 31: Confidence None : Bounding Box [     1650.9      709.55      3237.2      1601.4]\n","Logic frame count : 216\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 168.7ms\n","Speed: 8.5ms preprocess, 168.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(519.7382), tensor(851.6611), tensor(638.5016), tensor(1089.3020)], tensor(0.5541), 0), ([tensor(1572.0510), tensor(708.8849), tensor(1748.9600), tensor(886.2821)], tensor(0.5193), 3), ([tensor(1730.5454), tensor(1016.8633), tensor(2035.8440), tensor(1438.3350)], tensor(0.4316), 3)]\n","Class person : ID 2: Confidence 0.5541254878044128 : Bounding Box [     510.64      845.21      1178.8      1934.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1422.4      765.88      3304.3      1840.6]\n","Class motorcycle : ID 20: Confidence 0.4315923750400543 : Bounding Box [       1731      1016.6      3765.2      2454.9]\n","Class person : ID 21: Confidence None : Bounding Box [     1702.1      635.15      3218.3      1336.4]\n","Class motorcycle : ID 31: Confidence 0.519263505935669 : Bounding Box [     1647.1      708.37      3248.5        1595]\n","Logic frame count : 217\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 147.2ms\n","Speed: 8.0ms preprocess, 147.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(522.0399), tensor(853.1461), tensor(638.2476), tensor(1087.9230)], tensor(0.6089), 0), ([tensor(1577.9625), tensor(712.3893), tensor(1751.9039), tensor(882.6052)], tensor(0.3657), 3), ([tensor(1730.8707), tensor(1017.1902), tensor(2035.8497), tensor(1438.2800)], tensor(0.3282), 3)]\n","Class person : ID 2: Confidence 0.6088827252388 : Bounding Box [      509.1      851.31        1174        1940]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1422.3      765.55        3315      1846.4]\n","Class motorcycle : ID 20: Confidence 0.32817062735557556 : Bounding Box [     1731.4        1017      3765.8      2455.3]\n","Class person : ID 21: Confidence None : Bounding Box [     1708.3      631.01      3215.6      1328.2]\n","Class motorcycle : ID 31: Confidence 0.3656878173351288 : Bounding Box [     1647.5      709.94      3262.7      1592.7]\n","Logic frame count : 218\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 motorcycles, 161.7ms\n","Speed: 5.1ms preprocess, 161.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(517.2280), tensor(853.2297), tensor(633.4375), tensor(1084.5869)], tensor(0.6838), 0), ([tensor(1731.6667), tensor(1016.7549), tensor(2035.2385), tensor(1438.2710)], tensor(0.3352), 3), ([tensor(1586.3523), tensor(738.4193), tensor(1754.5261), tensor(876.8385)], tensor(0.2707), 3)]\n","Class person : ID 2: Confidence 0.6838274598121643 : Bounding Box [     505.41      853.57        1166      1939.8]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1422.1      765.22      3325.6      1852.3]\n","Class motorcycle : ID 20: Confidence 0.33516064286231995 : Bounding Box [     1731.9      1016.9      3766.4      2455.2]\n","Class person : ID 21: Confidence None : Bounding Box [     1714.6      626.87        3213      1319.9]\n","Class motorcycle : ID 31: Confidence 0.2706528306007385 : Bounding Box [     1651.8      727.86      3276.2      1605.5]\n","Logic frame count : 219\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 250.6ms\n","Speed: 10.0ms preprocess, 250.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(507.6878), tensor(855.0066), tensor(636.5979), tensor(1079.8420)], tensor(0.5800), 0), ([tensor(1583.0873), tensor(708.7870), tensor(1759.1422), tensor(874.4020)], tensor(0.4544), 8), ([tensor(1729.6423), tensor(1015.2974), tensor(2032.9768), tensor(1438.8008)], tensor(0.2688), 3)]\n","Class person : ID 2: Confidence 0.580048680305481 : Bounding Box [     500.28      855.49      1156.4      1937.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [       1422      764.89      3336.3      1858.1]\n","Class motorcycle : ID 20: Confidence 0.26879990100860596 : Bounding Box [     1729.8      1015.9      3764.6      2454.5]\n","Class person : ID 21: Confidence None : Bounding Box [     1720.9      622.73      3210.4      1311.6]\n","Class boat : ID 31: Confidence 0.4543960392475128 : Bounding Box [       1649       714.9      3283.7      1589.2]\n","Logic frame count : 220\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 boat, 230.3ms\n","Speed: 5.3ms preprocess, 230.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(510.1125), tensor(867.6704), tensor(636.7989), tensor(1082.2612)], tensor(0.3318), 0), ([tensor(576.2827), tensor(885.0341), tensor(635.1121), tensor(1078.2750)], tensor(0.3161), 0), ([tensor(551.5787), tensor(878.9337), tensor(634.8988), tensor(1079.2382)], tensor(0.2952), 0), ([tensor(1578.8583), tensor(709.5685), tensor(1762.0275), tensor(872.8629)], tensor(0.2804), 8)]\n","Class person : ID 2: Confidence 0.33182600140571594 : Bounding Box [     500.23      864.48      1154.4      1946.6]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1421.8      764.56        3347      1863.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1729.8      1015.8      3764.7      2454.6]\n","Class person : ID 21: Confidence None : Bounding Box [     1727.1      618.58      3207.7      1303.4]\n","Class boat : ID 31: Confidence 0.28038734197616577 : Bounding Box [     1642.3      710.61      3287.7      1582.7]\n","Logic frame count : 221\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 234.7ms\n","Speed: 4.2ms preprocess, 234.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1586.7675), tensor(711.3158), tensor(1768.5836), tensor(868.7797)], tensor(0.4989), 8), ([tensor(508.6961), tensor(867.8408), tensor(634.1514), tensor(1081.2070)], tensor(0.4114), 0), ([tensor(545.1434), tensor(878.0549), tensor(634.5698), tensor(1077.0398)], tensor(0.3244), 0)]\n","Class person : ID 2: Confidence 0.4113544821739197 : Bounding Box [     499.34      867.87      1151.2      1949.4]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1421.6      764.23      3357.6      1869.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1729.7      1015.8      3764.7      2454.6]\n","Class person : ID 21: Confidence None : Bounding Box [     1733.4      614.44      3205.1      1295.1]\n","Class boat : ID 31: Confidence 0.49891018867492676 : Bounding Box [     1644.9      710.26      3298.2        1579]\n","Logic frame count : 222\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 229.0ms\n","Speed: 9.1ms preprocess, 229.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(512.1163), tensor(863.4137), tensor(635.7548), tensor(1074.8180)], tensor(0.5182), 0), ([tensor(1599.2925), tensor(708.2260), tensor(1772.6602), tensor(865.6337)], tensor(0.3444), 8)]\n","Class person : ID 2: Confidence 0.5182303190231323 : Bounding Box [     503.43      866.09      1151.3      1943.1]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1421.5       763.9      3368.3      1875.6]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1729.6      1015.8      3764.8      2454.7]\n","Class person : ID 21: Confidence None : Bounding Box [     1739.7       610.3      3202.5      1286.9]\n","Class boat : ID 31: Confidence 0.344377338886261 : Bounding Box [     1653.1      708.17      3313.7      1573.7]\n","Logic frame count : 223\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 220.5ms\n","Speed: 9.9ms preprocess, 220.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(515.6348), tensor(860.3960), tensor(633.9600), tensor(1075.4429)], tensor(0.6054), 0), ([tensor(1615.9778), tensor(712.0676), tensor(1773.6108), tensor(861.4236)], tensor(0.2702), 8)]\n","Class person : ID 2: Confidence 0.6054401993751526 : Bounding Box [     507.13      863.29      1152.9        1939]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1421.3      763.57        3379      1881.4]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1729.6      1015.7      3764.9      2454.7]\n","Class person : ID 21: Confidence None : Bounding Box [     1745.9      606.16      3199.8      1278.6]\n","Class boat : ID 31: Confidence 0.2701519727706909 : Bounding Box [     1666.1      710.02      3331.9      1571.7]\n","Logic frame count : 224\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 228.9ms\n","Speed: 4.6ms preprocess, 228.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(514.3871), tensor(860.2386), tensor(636.9481), tensor(1078.0868)], tensor(0.6191), 0), ([tensor(1622.7314), tensor(701.1486), tensor(1778.9355), tensor(857.7572)], tensor(0.5744), 8)]\n","Class person : ID 2: Confidence 0.6190927624702454 : Bounding Box [     508.52      862.02        1154      1939.1]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1421.2      763.24      3389.7      1887.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1729.5      1015.7        3765      2454.8]\n","Class person : ID 21: Confidence None : Bounding Box [     1752.2      602.02      3197.2      1270.3]\n","Class boat : ID 31: Confidence 0.5744413733482361 : Bounding Box [     1675.2      703.54      3346.3      1561.4]\n","Logic frame count : 225\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 247.9ms\n","Speed: 4.2ms preprocess, 247.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(512.5033), tensor(858.3397), tensor(637.4883), tensor(1078.7811)], tensor(0.6400), 0), ([tensor(1628.9172), tensor(722.5242), tensor(1784.5925), tensor(854.1582)], tensor(0.4714), 8), ([tensor(1728.9500), tensor(1015.8623), tensor(2032.1353), tensor(1439.0840)], tensor(0.2594), 3)]\n","Class person : ID 2: Confidence 0.6399936676025391 : Bounding Box [     508.17      860.21      1153.3      1938.2]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [       1421      762.91      3400.3      1893.1]\n","Class motorcycle : ID 20: Confidence 0.25940200686454773 : Bounding Box [     1727.7      1015.8      3762.7      2454.9]\n","Class person : ID 21: Confidence None : Bounding Box [     1758.5      597.88      3194.6      1262.1]\n","Class boat : ID 31: Confidence 0.47141218185424805 : Bounding Box [     1682.7      715.39      3358.9      1569.6]\n","Logic frame count : 226\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 255.7ms\n","Speed: 4.4ms preprocess, 255.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(510.5424), tensor(858.1027), tensor(638.4786), tensor(1078.8165)], tensor(0.6403), 0)]\n","Class person : ID 2: Confidence 0.6403214931488037 : Bounding Box [     507.32      859.31        1152      1937.7]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1420.9      762.58        3411      1898.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1727.4      1015.8      3762.5        2455]\n","Class person : ID 21: Confidence None : Bounding Box [     1764.7      593.73      3191.9      1253.8]\n","Class boat : ID 31: Confidence None : Bounding Box [     1695.4      714.72      3364.7      1565.4]\n","Logic frame count : 227\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 246.2ms\n","Speed: 6.3ms preprocess, 246.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(509.1064), tensor(858.3861), tensor(638.4608), tensor(1076.6542)], tensor(0.5837), 0), ([tensor(1635.4741), tensor(700.5560), tensor(1789.8718), tensor(850.5944)], tensor(0.3590), 8)]\n","Class person : ID 2: Confidence 0.5836523175239563 : Bounding Box [     506.66       859.1        1150      1936.3]\n","Class motorcycle : ID 16: Confidence None : Bounding Box [     1420.7      762.25      3421.7      1904.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1727.2      1015.8      3762.4      2455.1]\n","Class person : ID 21: Confidence None : Bounding Box [       1771      589.59      3189.3      1245.5]\n","Class boat : ID 31: Confidence 0.3590114116668701 : Bounding Box [       1692      703.47      3372.7      1553.3]\n","Logic frame count : 228\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 168.8ms\n","Speed: 9.0ms preprocess, 168.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(515.2057), tensor(859.6176), tensor(638.0864), tensor(1074.7397)], tensor(0.5741), 0), ([tensor(1624.0403), tensor(690.5242), tensor(1797.1277), tensor(847.6587)], tensor(0.2920), 8)]\n","Class person : ID 2: Confidence 0.5740570425987244 : Bounding Box [     510.87      859.79      1152.7      1935.2]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1727      1015.8      3762.3      2455.1]\n","Class person : ID 21: Confidence None : Bounding Box [     1777.3      585.45      3186.7      1237.3]\n","Class boat : ID 31: Confidence 0.2919926047325134 : Bounding Box [     1684.2       694.1      3372.7      1541.4]\n","Logic frame count : 229\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 160.9ms\n","Speed: 7.8ms preprocess, 160.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(515.7878), tensor(862.0579), tensor(637.3088), tensor(1075.3101)], tensor(0.5817), 0), ([tensor(1643.6567), tensor(691.2031), tensor(1798.5962), tensor(844.0071)], tensor(0.3331), 8)]\n","Class person : ID 2: Confidence 0.5817378759384155 : Bounding Box [     512.68      861.62      1153.8      1936.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1726.7      1015.8      3762.2      2455.2]\n","Class person : ID 21: Confidence None : Bounding Box [     1783.5      581.31        3184        1229]\n","Class boat : ID 31: Confidence 0.33312639594078064 : Bounding Box [     1693.4      691.29      3387.6      1535.3]\n","Logic frame count : 230\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 162.7ms\n","Speed: 9.3ms preprocess, 162.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(516.9673), tensor(860.5431), tensor(637.8535), tensor(1076.5531)], tensor(0.5701), 0), ([tensor(1654.7275), tensor(703.6116), tensor(1802.1208), tensor(838.9663)], tensor(0.3300), 8)]\n","Class person : ID 2: Confidence 0.570093035697937 : Bounding Box [     514.25      861.28      1155.4      1937.2]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1726.5      1015.8      3762.1      2455.3]\n","Class person : ID 21: Confidence None : Bounding Box [     1789.8      577.17      3181.4      1220.8]\n","Class boat : ID 31: Confidence 0.3299991190433502 : Bounding Box [     1704.6      698.57      3401.9      1538.2]\n","Logic frame count : 231\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 167.6ms\n","Speed: 4.7ms preprocess, 167.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(519.1287), tensor(861.1830), tensor(637.2197), tensor(1075.4363)], tensor(0.6249), 0), ([tensor(1659.8896), tensor(701.1667), tensor(1805.4722), tensor(837.4836)], tensor(0.2558), 8)]\n","Class person : ID 2: Confidence 0.6249285936355591 : Bounding Box [     516.41      861.53      1156.8        1937]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1726.2      1015.7        3762      2455.3]\n","Class person : ID 21: Confidence None : Bounding Box [     1796.1      573.03      3178.8      1212.5]\n","Class boat : ID 31: Confidence 0.25579744577407837 : Bounding Box [     1710.5      699.78        3414      1536.9]\n","Logic frame count : 232\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 161.7ms\n","Speed: 4.8ms preprocess, 161.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(518.5559), tensor(863.5156), tensor(637.0889), tensor(1074.7449)], tensor(0.5756), 0), ([tensor(1662.3108), tensor(694.2546), tensor(1807.7927), tensor(830.9750)], tensor(0.4055), 8)]\n","Class person : ID 2: Confidence 0.5756226181983948 : Bounding Box [     517.05      863.13      1156.8        1938]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1726      1015.7      3761.9      2455.4]\n","Class person : ID 21: Confidence None : Bounding Box [     1802.3      568.88      3176.1      1204.2]\n","Class boat : ID 31: Confidence 0.4054524302482605 : Bounding Box [     1715.3       695.7      3420.1      1527.6]\n","Logic frame count : 233\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 166.7ms\n","Speed: 4.1ms preprocess, 166.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(520.2390), tensor(859.2303), tensor(640.0005), tensor(1073.1967)], tensor(0.3721), 0), ([tensor(1662.4775), tensor(690.5679), tensor(1812.3931), tensor(830.5903)], tensor(0.2998), 8)]\n","Class person : ID 2: Confidence 0.37209466099739075 : Bounding Box [     519.59      860.87      1158.6      1934.5]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1725.8      1015.7      3761.7      2455.5]\n","Class person : ID 21: Confidence None : Bounding Box [     1808.6      564.74      3173.5        1196]\n","Class boat : ID 31: Confidence 0.29984062910079956 : Bounding Box [     1715.3      691.76      3426.8      1521.7]\n","Logic frame count : 234\n","Analyzing image for objects...\n","\n","0: 384x640 1 boat, 160.3ms\n","Speed: 11.6ms preprocess, 160.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1673.3298), tensor(686.1832), tensor(1814.0300), tensor(817.3422)], tensor(0.5037), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     519.97      861.45      1158.6      1934.6]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1725.5      1015.7      3761.6      2455.5]\n","Class person : ID 21: Confidence None : Bounding Box [     1814.9       560.6      3170.9      1187.7]\n","Class boat : ID 31: Confidence 0.5036508440971375 : Bounding Box [     1727.4      687.44      3432.1      1507.9]\n","Logic frame count : 235\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 160.8ms\n","Speed: 5.1ms preprocess, 160.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(518.3110), tensor(861.8121), tensor(637.1980), tensor(1077.6105)], tensor(0.5182), 0), ([tensor(1672.6414), tensor(681.9684), tensor(1818.6865), tensor(816.3107)], tensor(0.4239), 8)]\n","Class person : ID 2: Confidence 0.5181974768638611 : Bounding Box [     517.41      861.86      1157.5      1938.4]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1725.3      1015.7      3761.5      2455.6]\n","Class person : ID 21: Confidence None : Bounding Box [     1821.1      556.46      3168.2      1179.5]\n","Class boat : ID 31: Confidence 0.42387649416923523 : Bounding Box [     1729.7      683.09      3437.9      1499.5]\n","Logic frame count : 236\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 178.3ms\n","Speed: 4.1ms preprocess, 178.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(515.5367), tensor(864.6870), tensor(636.4950), tensor(1076.4309)], tensor(0.5427), 0), ([tensor(1680.3132), tensor(685.2542), tensor(1820.9182), tensor(813.8333)], tensor(0.3677), 8), ([tensor(1730.1248), tensor(1016.9846), tensor(2034.1824), tensor(1438.2898)], tensor(0.2556), 3)]\n","Class person : ID 2: Confidence 0.5427436828613281 : Bounding Box [     515.16      863.95      1154.8      1940.4]\n","Class motorcycle : ID 20: Confidence 0.25559234619140625 : Bounding Box [     1730.1      1016.9      3764.1      2455.3]\n","Class person : ID 21: Confidence None : Bounding Box [     1827.4      552.32      3165.6      1171.2]\n","Class boat : ID 31: Confidence 0.36765021085739136 : Bounding Box [     1734.1      683.74      3447.6      1497.1]\n","Logic frame count : 237\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 165.0ms\n","Speed: 4.4ms preprocess, 165.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(501.6439), tensor(862.2412), tensor(635.5657), tensor(1077.1196)], tensor(0.4904), 0), ([tensor(1679.9304), tensor(678.9735), tensor(1823.2976), tensor(808.4788)], tensor(0.4002), 8)]\n","Class person : ID 2: Confidence 0.4904354512691498 : Bounding Box [     504.91      863.06      1144.2      1939.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.2        1017      3764.2      2455.3]\n","Class person : ID 21: Confidence None : Bounding Box [     1833.7      548.18        3163      1162.9]\n","Class boat : ID 31: Confidence 0.4001966416835785 : Bounding Box [       1736      679.89      3451.5      1488.7]\n","Logic frame count : 238\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 163.9ms\n","Speed: 4.2ms preprocess, 163.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(500.0642), tensor(863.9623), tensor(637.1667), tensor(1077.2816)], tensor(0.5453), 0), ([tensor(1691.7507), tensor(679.3805), tensor(1830.6375), tensor(804.3704)], tensor(0.3977), 8), ([tensor(1730.2490), tensor(1016.9868), tensor(2034.8062), tensor(1438.5913)], tensor(0.2863), 3)]\n","Class person : ID 2: Confidence 0.5453373789787292 : Bounding Box [      500.6      863.84      1139.8        1941]\n","Class motorcycle : ID 20: Confidence 0.2862592339515686 : Bounding Box [     1730.4        1017      3764.8      2455.5]\n","Class person : ID 21: Confidence None : Bounding Box [     1839.9      544.03      3160.3      1154.7]\n","Class boat : ID 31: Confidence 0.3977232277393341 : Bounding Box [     1745.6      678.81      3464.1      1483.3]\n","Logic frame count : 239\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 1 boat, 187.6ms\n","Speed: 6.8ms preprocess, 187.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(503.4245), tensor(865.3040), tensor(636.5112), tensor(1077.0374)], tensor(0.5413), 0), ([tensor(1689.5017), tensor(674.4872), tensor(1835.0662), tensor(809.9625)], tensor(0.3198), 8), ([tensor(1729.7253), tensor(1017.0721), tensor(2034.6697), tensor(1438.3949)], tensor(0.2913), 3)]\n","Class person : ID 2: Confidence 0.541325032711029 : Bounding Box [     501.19         865        1140      1942.1]\n","Class motorcycle : ID 20: Confidence 0.2913256883621216 : Bounding Box [     1730.1      1017.1      3764.4      2455.5]\n","Class person : ID 21: Confidence None : Bounding Box [     1846.2      539.89      3157.7      1146.4]\n","Class boat : ID 31: Confidence 0.3197682499885559 : Bounding Box [       1742      675.23      3475.5        1482]\n","Logic frame count : 240\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 173.7ms\n","Speed: 4.7ms preprocess, 173.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(501.5692), tensor(858.8246), tensor(635.2421), tensor(1077.2018)], tensor(0.5045), 0), ([tensor(1692.7820), tensor(671.3450), tensor(1834.9465), tensor(807.3171)], tensor(0.2885), 8)]\n","Class person : ID 2: Confidence 0.5045130252838135 : Bounding Box [     499.96      861.14      1138.4      1938.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.2      1017.1      3764.5      2455.6]\n","Class person : ID 21: Confidence None : Bounding Box [     1852.5      535.75      3155.1      1138.2]\n","Class boat : ID 31: Confidence 0.2884851098060608 : Bounding Box [     1741.3      671.89      3482.8      1477.9]\n","Logic frame count : 241\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 153.4ms\n","Speed: 4.1ms preprocess, 153.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(507.2470), tensor(859.9005), tensor(636.5610), tensor(1077.3395)], tensor(0.5605), 0), ([tensor(1692.4871), tensor(672.4185), tensor(1837.3079), tensor(797.4998)], tensor(0.2929), 8)]\n","Class person : ID 2: Confidence 0.5605162978172302 : Bounding Box [     503.77      860.37        1142      1937.7]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.2      1017.2      3764.6      2455.6]\n","Class person : ID 21: Confidence None : Bounding Box [     1858.7      531.61      3152.4      1129.9]\n","Class boat : ID 31: Confidence 0.292900949716568 : Bounding Box [     1744.6      671.44      3482.7      1470.8]\n","Logic frame count : 242\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 169.1ms\n","Speed: 8.1ms preprocess, 169.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(507.1455), tensor(858.7201), tensor(637.1421), tensor(1077.6412)], tensor(0.5309), 0), ([tensor(1697.9941), tensor(669.3650), tensor(1839.3872), tensor(795.0448)], tensor(0.2771), 8)]\n","Class person : ID 2: Confidence 0.5309495329856873 : Bounding Box [     505.37      859.29      1143.5      1936.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.3      1017.3      3764.6      2455.6]\n","Class person : ID 21: Confidence None : Bounding Box [       1865      527.47      3149.8      1121.6]\n","Class boat : ID 31: Confidence 0.27714061737060547 : Bounding Box [     1748.3      669.33      3488.3      1464.7]\n","Logic frame count : 243\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 173.3ms\n","Speed: 7.5ms preprocess, 173.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(506.0724), tensor(858.8062), tensor(635.5552), tensor(1077.4976)], tensor(0.5190), 0), ([tensor(1707.1644), tensor(673.2006), tensor(1842.6224), tensor(792.6133)], tensor(0.4633), 8)]\n","Class person : ID 2: Confidence 0.5190362334251404 : Bounding Box [     504.94      858.94      1142.7      1936.5]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.4      1017.3      3764.7      2455.7]\n","Class boat : ID 31: Confidence 0.46334585547447205 : Bounding Box [       1755      671.18      3498.9      1463.5]\n","Logic frame count : 244\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 159.7ms\n","Speed: 11.2ms preprocess, 159.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(500.2585), tensor(858.3459), tensor(636.6790), tensor(1075.9170)], tensor(0.5430), 0), ([tensor(1699.4543), tensor(669.1910), tensor(1848.3474), tensor(791.9837)], tensor(0.3819), 8)]\n","Class person : ID 2: Confidence 0.5430151224136353 : Bounding Box [     501.62      858.51      1138.7        1935]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.5      1017.4      3764.8      2455.7]\n","Class boat : ID 31: Confidence 0.38190028071403503 : Bounding Box [     1751.4      669.28      3502.2      1460.2]\n","Logic frame count : 245\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 175.8ms\n","Speed: 4.2ms preprocess, 175.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(503.9075), tensor(859.6305), tensor(636.0414), tensor(1074.5049)], tensor(0.5830), 0), ([tensor(1715.3760), tensor(670.6857), tensor(1848.3291), tensor(791.5119)], tensor(0.3372), 8)]\n","Class person : ID 2: Confidence 0.583009660243988 : Bounding Box [     502.89       859.2      1139.1      1934.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.5      1017.4      3764.8      2455.8]\n","Class boat : ID 31: Confidence 0.3372493386268616 : Bounding Box [     1757.8      669.63      3516.3      1459.9]\n","Logic frame count : 246\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 169.2ms\n","Speed: 4.0ms preprocess, 169.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1717.8228), tensor(670.8885), tensor(1849.8435), tensor(789.4989)], tensor(0.5110), 8), ([tensor(509.7657), tensor(860.0634), tensor(635.5562), tensor(1074.3468)], tensor(0.5046), 0)]\n","Class person : ID 2: Confidence 0.5046173334121704 : Bounding Box [     507.19      859.75      1142.9      1934.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.6      1017.5      3764.9      2455.8]\n","Class boat : ID 31: Confidence 0.5110467672348022 : Bounding Box [     1760.8      669.96      3524.9      1458.7]\n","Logic frame count : 247\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 boats, 157.3ms\n","Speed: 10.1ms preprocess, 157.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1722.6014), tensor(663.1632), tensor(1850.9105), tensor(788.4586)], tensor(0.5088), 8), ([tensor(510.6661), tensor(859.2449), tensor(634.4257), tensor(1075.5961)], tensor(0.4274), 0), ([tensor(1572.2317), tensor(733.0004), tensor(1709.8098), tensor(807.4702)], tensor(0.3008), 8)]\n","Class person : ID 2: Confidence 0.4273500442504883 : Bounding Box [     508.93      859.42      1144.8      1934.6]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.7      1017.6      3764.9      2455.9]\n","Class boat : ID 31: Confidence 0.5088291168212891 : Bounding Box [     1763.3      665.02      3533.3      1452.7]\n","Logic frame count : 248\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 boats, 179.1ms\n","Speed: 4.2ms preprocess, 179.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1725.5369), tensor(659.1470), tensor(1857.1394), tensor(786.6462)], tensor(0.5515), 8), ([tensor(511.2355), tensor(860.3810), tensor(634.0739), tensor(1076.0038)], tensor(0.4106), 0), ([tensor(1572.2900), tensor(733.7568), tensor(1709.8760), tensor(808.0767)], tensor(0.2542), 8)]\n","Class person : ID 2: Confidence 0.4105851352214813 : Bounding Box [     509.87      860.05      1145.7      1935.7]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.8      1017.6        3765      2455.9]\n","Class boat : ID 31: Confidence 0.5514683127403259 : Bounding Box [     1766.6      660.55      3541.7      1446.7]\n","Logic frame count : 249\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 168.4ms\n","Speed: 3.9ms preprocess, 168.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1731.0382), tensor(658.7249), tensor(1859.0741), tensor(780.8792)], tensor(0.4866), 8), ([tensor(512.9246), tensor(861.7147), tensor(633.5109), tensor(1076.4237)], tensor(0.4142), 0), ([tensor(576.7604), tensor(888.6114), tensor(633.2105), tensor(1069.8892)], tensor(0.2772), 0)]\n","Class person : ID 2: Confidence 0.414172887802124 : Bounding Box [     511.19      861.17      1146.9      1937.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.8      1017.7      3765.1        2456]\n","Class boat : ID 31: Confidence 0.48664289712905884 : Bounding Box [     1773.1      658.66      3547.7      1440.5]\n","Logic frame count : 250\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 159.9ms\n","Speed: 9.7ms preprocess, 159.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(510.6667), tensor(859.7622), tensor(633.3080), tensor(1074.9819)], tensor(0.4399), 0), ([tensor(569.9368), tensor(880.1376), tensor(633.0309), tensor(1069.7833)], tensor(0.3181), 0)]\n","Class person : ID 2: Confidence 0.43985515832901 : Bounding Box [     510.46      860.29      1145.5      1935.7]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1730.9      1017.8      3765.1        2456]\n","Class boat : ID 31: Confidence None : Bounding Box [       1782      656.67      3550.6      1435.8]\n","Logic frame count : 251\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 178.7ms\n","Speed: 13.4ms preprocess, 178.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(507.0845), tensor(857.1572), tensor(633.7288), tensor(1077.4203)], tensor(0.4762), 0), ([tensor(548.2948), tensor(880.3912), tensor(633.5416), tensor(1070.8141)], tensor(0.3201), 0), ([tensor(577.7034), tensor(888.3999), tensor(633.4856), tensor(1070.7603)], tensor(0.2796), 0)]\n","Class person : ID 2: Confidence 0.4762158989906311 : Bounding Box [      507.6      858.24      1143.1      1934.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1731      1017.8      3765.2      2456.1]\n","Class boat : ID 31: Confidence None : Bounding Box [     1790.9      654.67      3553.5      1431.2]\n","Logic frame count : 252\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 162.7ms\n","Speed: 8.0ms preprocess, 162.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(509.1970), tensor(862.3833), tensor(633.8269), tensor(1077.6460)], tensor(0.4581), 0)]\n","Class person : ID 2: Confidence 0.45812350511550903 : Bounding Box [     507.98      860.91      1143.6      1938.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.1      1017.9      3765.3      2456.1]\n","Class boat : ID 31: Confidence None : Bounding Box [     1799.8      652.67      3556.4      1426.5]\n","Logic frame count : 253\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 148.9ms\n","Speed: 12.0ms preprocess, 148.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(509.9447), tensor(865.2228), tensor(633.7670), tensor(1078.8634)], tensor(0.4659), 0)]\n","Class person : ID 2: Confidence 0.46594810485839844 : Bounding Box [     508.49      863.81      1144.3      1942.2]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.1      1017.9      3765.3      2456.2]\n","Class boat : ID 31: Confidence None : Bounding Box [     1808.7      650.68      3559.3      1421.9]\n","Logic frame count : 254\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 179.6ms\n","Speed: 4.5ms preprocess, 179.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(509.7909), tensor(861.8038), tensor(634.3544), tensor(1078.0763)], tensor(0.4722), 0), ([tensor(574.1927), tensor(887.1367), tensor(634.3656), tensor(1072.2678)], tensor(0.2755), 0)]\n","Class person : ID 2: Confidence 0.4721556305885315 : Bounding Box [     508.98      862.63      1144.5      1940.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.2        1018      3765.4      2456.2]\n","Class boat : ID 31: Confidence None : Bounding Box [     1817.6      648.68      3562.2      1417.2]\n","Logic frame count : 255\n","Analyzing image for objects...\n","\n","0: 384x640 3 persons, 1 boat, 163.9ms\n","Speed: 4.6ms preprocess, 163.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1756.6160), tensor(641.1796), tensor(1874.3679), tensor(767.6327)], tensor(0.4855), 8), ([tensor(510.1688), tensor(860.9968), tensor(635.4827), tensor(1077.6287)], tensor(0.4072), 0), ([tensor(576.2714), tensor(889.1372), tensor(634.5753), tensor(1072.1724)], tensor(0.3678), 0), ([tensor(535.2290), tensor(886.4140), tensor(634.6311), tensor(1073.0396)], tensor(0.3538), 0)]\n","Class person : ID 2: Confidence 0.4072224497795105 : Bounding Box [     509.87      861.64      1145.2      1939.5]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.3      1018.1      3765.5      2456.3]\n","Class boat : ID 31: Confidence 0.4855072498321533 : Bounding Box [     1813.9      641.54        3574      1409.1]\n","Logic frame count : 256\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 162.8ms\n","Speed: 4.3ms preprocess, 162.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(510.5159), tensor(857.4965), tensor(635.5596), tensor(1079.2410)], tensor(0.4717), 0), ([tensor(1758.8699), tensor(635.2186), tensor(1879.2136), tensor(763.3417)], tensor(0.3748), 8)]\n","Class person : ID 2: Confidence 0.47171276807785034 : Bounding Box [      510.2      858.94      1145.9      1937.8]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.4      1018.1      3765.5      2456.3]\n","Class boat : ID 31: Confidence 0.37475407123565674 : Bounding Box [     1814.9      636.41      3582.8      1400.3]\n","Logic frame count : 257\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 241.0ms\n","Speed: 6.4ms preprocess, 241.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(513.5537), tensor(857.8696), tensor(636.9060), tensor(1076.6829)], tensor(0.4005), 0), ([tensor(1760.7051), tensor(641.2794), tensor(1877.8828), tensor(758.6068)], tensor(0.3061), 8), ([tensor(576.2402), tensor(891.1882), tensor(635.3914), tensor(1071.2502)], tensor(0.2680), 0)]\n","Class person : ID 2: Confidence 0.40048062801361084 : Bounding Box [     513.18      858.17      1148.2      1935.7]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.4      1018.2      3765.6      2456.4]\n","Class boat : ID 31: Confidence 0.30605918169021606 : Bounding Box [       1815      638.75      3587.5      1398.2]\n","Logic frame count : 258\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 227.5ms\n","Speed: 5.6ms preprocess, 227.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(516.9580), tensor(858.4154), tensor(635.9607), tensor(1078.9308)], tensor(0.4292), 0), ([tensor(1763.9573), tensor(655.3497), tensor(1882.0540), tensor(756.6334)], tensor(0.2861), 8)]\n","Class person : ID 2: Confidence 0.42920541763305664 : Bounding Box [     515.86      858.25      1151.4      1936.7]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.5      1018.3      3765.6      2456.4]\n","Class boat : ID 31: Confidence 0.28614217042922974 : Bounding Box [     1815.9      649.06      3595.2      1405.7]\n","Logic frame count : 259\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 227.0ms\n","Speed: 4.4ms preprocess, 227.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(513.1567), tensor(859.8035), tensor(636.7180), tensor(1082.0437)], tensor(0.4963), 0), ([tensor(1763.9233), tensor(633.8120), tensor(1882.5322), tensor(753.9099)], tensor(0.3919), 8), ([tensor(574.4168), tensor(873.5837), tensor(635.5521), tensor(1073.2061)], tensor(0.2629), 0)]\n","Class person : ID 2: Confidence 0.49629661440849304 : Bounding Box [     514.07       859.2      1150.9      1940.1]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.6      1018.3      3765.7      2456.5]\n","Class boat : ID 31: Confidence 0.39188238978385925 : Bounding Box [     1814.5      638.68      3599.6      1392.6]\n","Logic frame count : 260\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 220.7ms\n","Speed: 4.5ms preprocess, 220.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(514.1260), tensor(860.5763), tensor(634.9612), tensor(1082.6801)], tensor(0.5043), 0), ([tensor(1765.3810), tensor(642.7861), tensor(1884.3470), tensor(753.3889)], tensor(0.3519), 8), ([tensor(576.7598), tensor(868.8431), tensor(635.1033), tensor(1075.0515)], tensor(0.3340), 0)]\n","Class person : ID 2: Confidence 0.5043375492095947 : Bounding Box [     513.45      860.08      1150.7      1942.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.7      1018.4      3765.8      2456.5]\n","Class boat : ID 31: Confidence 0.35194268822669983 : Bounding Box [     1812.5      640.79      3605.4      1393.4]\n","Logic frame count : 261\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 232.8ms\n","Speed: 4.5ms preprocess, 232.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(520.6304), tensor(865.3079), tensor(637.8906), tensor(1089.9895)], tensor(0.4537), 0), ([tensor(1766.6564), tensor(640.4583), tensor(1887.0704), tensor(751.7417)], tensor(0.3293), 8), ([tensor(572.3730), tensor(868.4552), tensor(637.2361), tensor(1082.9523)], tensor(0.2658), 0)]\n","Class person : ID 2: Confidence 0.45370540022850037 : Bounding Box [     517.21      863.54      1157.1        1951]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.7      1018.4      3765.8      2456.5]\n","Class boat : ID 31: Confidence 0.3293026089668274 : Bounding Box [     1811.5       640.1      3611.1      1391.2]\n","Logic frame count : 262\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 223.1ms\n","Speed: 4.4ms preprocess, 223.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(517.9271), tensor(858.3253), tensor(743.7765), tensor(1088.8640)], tensor(0.4277), 0), ([tensor(520.0325), tensor(866.9976), tensor(640.5721), tensor(1089.9453)], tensor(0.4044), 0), ([tensor(1770.8765), tensor(640.4941), tensor(1887.3447), tensor(749.6179)], tensor(0.3969), 8)]\n","Class person : ID 2: Confidence 0.4044371545314789 : Bounding Box [     519.11      865.95        1160      1955.4]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.8      1018.5      3765.9      2456.6]\n","Class boat : ID 31: Confidence 0.39689651131629944 : Bounding Box [     1812.5      639.92      3617.2      1389.2]\n","Logic frame count : 263\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 256.2ms\n","Speed: 5.9ms preprocess, 256.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(1778.4617), tensor(633.1207), tensor(1894.2930), tensor(747.0707)], tensor(0.4494), 8), ([tensor(524.2670), tensor(863.2378), tensor(639.3595), tensor(1085.9048)], tensor(0.4416), 0), ([tensor(521.3284), tensor(860.0586), tensor(693.0549), tensor(1091.4653)], tensor(0.3789), 0)]\n","Class person : ID 2: Confidence 0.4415898621082306 : Bounding Box [     522.97      864.37      1162.6      1951.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1731.9      1018.6        3766      2456.6]\n","Class boat : ID 31: Confidence 0.44944289326667786 : Bounding Box [     1818.9      635.01      3627.8        1382]\n","Class person : ID 49: Confidence 0.3788661062717438 : Bounding Box [     547.53      859.28      1199.5      1950.8]\n","Logic frame count : 264\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 223.4ms\n","Speed: 10.1ms preprocess, 223.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(518.5154), tensor(858.9450), tensor(684.9121), tensor(1094.3518)], tensor(0.4980), 0), ([tensor(522.3857), tensor(862.8984), tensor(640.5642), tensor(1089.2893)], tensor(0.4142), 0), ([tensor(1778.1698), tensor(633.2177), tensor(1895.8304), tensor(747.3700)], tensor(0.2625), 8)]\n","Class person : ID 2: Confidence 0.414235383272171 : Bounding Box [     522.94      863.53      1163.4      1952.5]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1732      1018.6        3766      2456.7]\n","Class boat : ID 31: Confidence 0.2624956965446472 : Bounding Box [     1818.6      633.28      3634.9      1379.7]\n","Class person : ID 49: Confidence 0.49799463152885437 : Bounding Box [     535.31      858.24      1190.8        1953]\n","Logic frame count : 265\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 234.7ms\n","Speed: 9.0ms preprocess, 234.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(514.2198), tensor(858.1600), tensor(682.7833), tensor(1092.3927)], tensor(0.5324), 0), ([tensor(1781.8218), tensor(629.0290), tensor(1896.2646), tensor(745.5904)], tensor(0.4275), 8)]\n","Class person : ID 2: Confidence 0.5323502421379089 : Bounding Box [     528.94      860.07      1174.6      1951.6]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [       1732      1018.7      3766.1      2456.7]\n","Class boat : ID 31: Confidence 0.4275435507297516 : Bounding Box [     1819.7      629.92        3641        1375]\n","Class person : ID 49: Confidence None : Bounding Box [     527.68      855.83      1185.4      1954.4]\n","Logic frame count : 266\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 242.3ms\n","Speed: 9.8ms preprocess, 242.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(507.8713), tensor(855.5709), tensor(702.2266), tensor(1092.2092)], tensor(0.5861), 0), ([tensor(511.6390), tensor(859.5409), tensor(641.6733), tensor(1090.5270)], tensor(0.3969), 0)]\n","Class person : ID 2: Confidence 0.5860915780067444 : Bounding Box [      531.5      857.05      1182.9      1949.4]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1732.1      1018.8      3766.2      2456.8]\n","Class boat : ID 31: Confidence None : Bounding Box [     1826.7       628.1      3642.9      1371.1]\n","Class person : ID 49: Confidence 0.39691755175590515 : Bounding Box [     508.95      858.42      1162.4      1951.1]\n","Logic frame count : 267\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 158.2ms\n","Speed: 11.0ms preprocess, 158.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(506.3854), tensor(854.2374), tensor(674.1274), tensor(1092.2701)], tensor(0.5745), 0), ([tensor(505.5512), tensor(851.8380), tensor(750.9543), tensor(1090.5443)], tensor(0.4875), 0), ([tensor(1788.3352), tensor(634.9646), tensor(1901.6663), tensor(737.7788)], tensor(0.3945), 8)]\n","Class person : ID 2: Confidence 0.4874905049800873 : Bounding Box [     543.51      853.45        1204      1944.9]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1732.2      1018.8      3766.2      2456.8]\n","Class boat : ID 31: Confidence 0.39446449279785156 : Bounding Box [     1831.3      633.07        3647      1371.5]\n","Class person : ID 49: Confidence 0.5745245218276978 : Bounding Box [     510.83      855.02        1166      1948.1]\n","Logic frame count : 268\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 182.6ms\n","Speed: 4.3ms preprocess, 182.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(508.5963), tensor(853.7659), tensor(671.1322), tensor(1093.0288)], tensor(0.5739), 0), ([tensor(507.7278), tensor(851.4984), tensor(751.4211), tensor(1091.2478)], tensor(0.4508), 0), ([tensor(1790.4336), tensor(632.8059), tensor(1902.4409), tensor(737.8997)], tensor(0.4350), 8)]\n","Class person : ID 2: Confidence 0.4507538676261902 : Bounding Box [     546.91      851.88        1216      1943.5]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1732.3      1018.9      3766.3      2456.9]\n","Class boat : ID 31: Confidence 0.4350169599056244 : Bounding Box [     1831.2      632.54      3653.3      1369.9]\n","Class person : ID 49: Confidence 0.5738988518714905 : Bounding Box [     511.64      853.57      1168.4      1947.2]\n","Logic frame count : 269\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 167.7ms\n","Speed: 4.1ms preprocess, 167.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(498.0864), tensor(853.9240), tensor(688.0887), tensor(1092.1910)], tensor(0.5664), 0), ([tensor(1794.4556), tensor(625.8895), tensor(1902.4478), tensor(732.6907)], tensor(0.4139), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     551.07      850.99      1220.6      1943.3]\n","Class motorcycle : ID 20: Confidence None : Bounding Box [     1732.3      1018.9      3766.3      2456.9]\n","Class boat : ID 31: Confidence 0.41388219594955444 : Bounding Box [     1834.9      627.77        3657      1361.3]\n","Class person : ID 49: Confidence 0.5663629770278931 : Bounding Box [     509.99      853.22      1169.1      1946.5]\n","Logic frame count : 270\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 166.8ms\n","Speed: 4.1ms preprocess, 166.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(499.1369), tensor(854.6615), tensor(661.5230), tensor(1092.4252)], tensor(0.5990), 0), ([tensor(498.3929), tensor(852.8813), tensor(733.1031), tensor(1087.8706)], tensor(0.4644), 0), ([tensor(1798.6833), tensor(628.3149), tensor(1905.1218), tensor(730.8538)], tensor(0.2918), 8)]\n","Class person : ID 2: Confidence 0.5990166664123535 : Bounding Box [     508.86      853.65      1177.6      1946.2]\n","Class boat : ID 31: Confidence 0.29184064269065857 : Bounding Box [     1838.4      627.65      3663.3      1358.6]\n","Class person : ID 49: Confidence 0.4644232988357544 : Bounding Box [     523.08      852.46      1186.6      1942.6]\n","Logic frame count : 271\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 185.8ms\n","Speed: 3.9ms preprocess, 185.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(496.9034), tensor(853.7454), tensor(665.8438), tensor(1093.0491)], tensor(0.5885), 0), ([tensor(1799.2474), tensor(613.9858), tensor(1908.1735), tensor(731.1128)], tensor(0.5870), 8), ([tensor(496.1389), tensor(851.8276), tensor(753.4958), tensor(1091.4309)], tensor(0.4147), 0)]\n","Class person : ID 2: Confidence 0.41469326615333557 : Bounding Box [     524.56      852.25      1201.6      1944.3]\n","Class boat : ID 31: Confidence 0.5869585275650024 : Bounding Box [     1838.2      618.14      3669.7      1348.4]\n","Class person : ID 49: Confidence 0.5884525775909424 : Bounding Box [     504.85      852.82      1169.8      1945.2]\n","Logic frame count : 272\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 167.8ms\n","Speed: 7.4ms preprocess, 167.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(493.8493), tensor(850.4677), tensor(745.2898), tensor(1089.5927)], tensor(0.6384), 0), ([tensor(1798.0583), tensor(626.5411), tensor(1908.0979), tensor(730.5604)], tensor(0.5637), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     526.05      851.63      1203.3      1944.2]\n","Class boat : ID 31: Confidence 0.5636763572692871 : Bounding Box [     1835.2      622.96        3673      1352.7]\n","Class person : ID 49: Confidence 0.6383669376373291 : Bounding Box [     519.74      850.83      1190.8      1941.7]\n","Logic frame count : 273\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 172.0ms\n","Speed: 5.4ms preprocess, 172.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(498.7347), tensor(850.9062), tensor(742.3064), tensor(1088.2751)], tensor(0.6812), 0), ([tensor(1801.6028), tensor(633.6754), tensor(1911.8823), tensor(727.2777)], tensor(0.5194), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     527.54      851.01      1205.1      1944.1]\n","Class boat : ID 31: Confidence 0.5194416046142578 : Bounding Box [     1837.7      629.58      3677.6        1357]\n","Class person : ID 49: Confidence 0.681165337562561 : Bounding Box [     526.17      850.43      1202.7      1939.8]\n","Logic frame count : 274\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 184.1ms\n","Speed: 4.8ms preprocess, 184.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(497.8029), tensor(851.7166), tensor(737.6483), tensor(1087.2559)], tensor(0.7149), 0), ([tensor(1801.4270), tensor(621.3315), tensor(1911.7039), tensor(725.1695)], tensor(0.4301), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     529.03       850.4      1206.8      1944.1]\n","Class boat : ID 31: Confidence 0.43006250262260437 : Bounding Box [     1837.6      623.93      3679.7      1349.2]\n","Class person : ID 49: Confidence 0.714911699295044 : Bounding Box [     525.12      850.87      1206.4        1939]\n","Logic frame count : 275\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 175.0ms\n","Speed: 18.8ms preprocess, 175.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(499.1499), tensor(852.5001), tensor(734.3147), tensor(1090.5621)], tensor(0.7133), 0), ([tensor(1806.4834), tensor(609.1188), tensor(1912.8794), tensor(722.9860)], tensor(0.4382), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     530.51      849.78      1208.6        1944]\n","Class boat : ID 31: Confidence 0.4382030963897705 : Bounding Box [     1840.6      613.74      3684.6      1336.8]\n","Class person : ID 49: Confidence 0.7132713794708252 : Bounding Box [     522.58       851.6      1209.6      1941.4]\n","Logic frame count : 276\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 2 boats, 164.6ms\n","Speed: 12.5ms preprocess, 164.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(497.3533), tensor(852.6614), tensor(725.5037), tensor(1090.3153)], tensor(0.5872), 0), ([tensor(1811.5305), tensor(593.0518), tensor(1917.0544), tensor(723.9705)], tensor(0.4322), 8), ([tensor(1812.5132), tensor(629.1747), tensor(1916.5811), tensor(726.7432)], tensor(0.2898), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [        532      849.16      1210.3        1944]\n","Class boat : ID 31: Confidence 0.2897895872592926 : Bounding Box [     1841.9      623.26      3696.5      1348.1]\n","Class person : ID 49: Confidence 0.5872499942779541 : Bounding Box [      516.7      852.02      1207.5      1942.3]\n","Logic frame count : 277\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 2 boats, 177.9ms\n","Speed: 6.1ms preprocess, 177.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(496.2480), tensor(852.7375), tensor(725.5303), tensor(1089.8380)], tensor(0.5811), 0), ([tensor(1816.2769), tensor(632.6783), tensor(1919.3757), tensor(721.5319)], tensor(0.4054), 8), ([tensor(1810.3066), tensor(1055.3191), tensor(1908.2588), tensor(1173.2136)], tensor(0.3579), 0), ([tensor(1814.2695), tensor(578.9749), tensor(1921.7080), tensor(720.7984)], tensor(0.2781), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     533.49      848.54        1212      1943.9]\n","Class boat : ID 31: Confidence 0.4054482877254486 : Bounding Box [     1847.5      629.22      3702.4      1351.3]\n","Class person : ID 49: Confidence 0.5811453461647034 : Bounding Box [     512.93      852.26      1206.8      1942.4]\n","Logic frame count : 278\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 2 boats, 159.6ms\n","Speed: 4.3ms preprocess, 159.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(499.0520), tensor(852.8267), tensor(727.2202), tensor(1090.3870)], tensor(0.6138), 0), ([tensor(1815.7278), tensor(637.9324), tensor(1927.9197), tensor(718.7207)], tensor(0.3812), 8), ([tensor(1803.8710), tensor(1053.9156), tensor(1914.5067), tensor(1172.4794)], tensor(0.2901), 0), ([tensor(1813.6936), tensor(578.9323), tensor(1931.6677), tensor(718.2667)], tensor(0.2724), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     534.98      847.93      1213.8      1943.8]\n","Class boat : ID 31: Confidence 0.38124746084213257 : Bounding Box [     1851.7      634.98      3707.2      1354.2]\n","Class person : ID 49: Confidence 0.6137621998786926 : Bounding Box [     512.91      852.44        1210      1942.9]\n","Logic frame count : 279\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 168.7ms\n","Speed: 4.5ms preprocess, 168.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(496.4898), tensor(853.1287), tensor(719.1250), tensor(1090.2687)], tensor(0.5746), 0)]\n","Class person : ID 2: Confidence None : Bounding Box [     536.47      847.31      1215.5      1943.8]\n","Class boat : ID 31: Confidence None : Bounding Box [     1858.2      635.57      3708.9      1352.9]\n","Class person : ID 49: Confidence 0.5745507478713989 : Bounding Box [     508.13      852.73      1207.3      1943.2]\n","Logic frame count : 280\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 surfboard, 148.3ms\n","Speed: 4.2ms preprocess, 148.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(484.5484), tensor(857.1344), tensor(715.6062), tensor(1092.3406)], tensor(0.5860), 0), ([tensor(574.4242), tensor(1081.1831), tensor(673.4913), tensor(1142.7393)], tensor(0.3328), 37), ([tensor(491.3305), tensor(862.3309), tensor(634.7484), tensor(1084.9076)], tensor(0.2533), 0)]\n","Class person : ID 2: Confidence 0.2532986104488373 : Bounding Box [     478.26      861.74      1144.4      1947.1]\n","Class boat : ID 31: Confidence None : Bounding Box [     1864.6      636.17      3710.5      1351.6]\n","Class person : ID 49: Confidence 0.5859593152999878 : Bounding Box [     496.54       855.5        1198      1947.3]\n","Logic frame count : 281\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 surfboard, 173.7ms\n","Speed: 5.9ms preprocess, 173.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(505.2803), tensor(858.0750), tensor(713.5608), tensor(1092.3237)], tensor(0.5999), 0), ([tensor(574.3031), tensor(1081.2805), tensor(673.4744), tensor(1142.5757)], tensor(0.3539), 37), ([tensor(512.4391), tensor(862.6924), tensor(635.7784), tensor(1090.1089)], tensor(0.3159), 0)]\n","Class person : ID 2: Confidence 0.31588929891586304 : Bounding Box [     491.95      862.55      1155.1      1951.1]\n","Class boat : ID 31: Confidence None : Bounding Box [     1871.1      636.76      3712.1      1350.3]\n","Class person : ID 49: Confidence 0.5998861193656921 : Bounding Box [     504.95      857.18      1207.8      1949.5]\n","Logic frame count : 282\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 1 surfboard, 249.9ms\n","Speed: 16.5ms preprocess, 249.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(504.4970), tensor(858.0244), tensor(712.8987), tensor(1092.6014)], tensor(0.6055), 0), ([tensor(574.4019), tensor(1081.3605), tensor(673.2413), tensor(1142.5643)], tensor(0.3627), 37), ([tensor(1816.0171), tensor(607.7222), tensor(1934.9946), tensor(713.3133)], tensor(0.3164), 8), ([tensor(511.6326), tensor(862.5470), tensor(635.7275), tensor(1090.1627)], tensor(0.3076), 0)]\n","Class person : ID 2: Confidence 0.3076007068157196 : Bounding Box [     496.94      862.73      1156.8      1952.4]\n","Class boat : ID 31: Confidence 0.3163752555847168 : Bounding Box [     1859.2      611.04      3710.5      1324.2]\n","Class person : ID 49: Confidence 0.6054991483688354 : Bounding Box [     507.15      857.79      1211.2      1950.4]\n","Logic frame count : 283\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 1 surfboard, 161.4ms\n","Speed: 10.1ms preprocess, 161.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(504.8071), tensor(857.7065), tensor(713.3820), tensor(1093.1614)], tensor(0.6236), 0), ([tensor(574.3673), tensor(1081.3610), tensor(673.3203), tensor(1142.6283)], tensor(0.3560), 37), ([tensor(1819.8693), tensor(612.1757), tensor(1931.2550), tensor(712.4891)], tensor(0.3305), 8), ([tensor(511.9613), tensor(862.2589), tensor(635.9550), tensor(1090.4175)], tensor(0.2981), 0)]\n","Class person : ID 2: Confidence 0.2981235682964325 : Bounding Box [     499.96      862.58        1157      1952.8]\n","Class boat : ID 31: Confidence 0.3304823935031891 : Bounding Box [     1857.1      611.22      3715.4      1323.4]\n","Class person : ID 49: Confidence 0.6235523223876953 : Bounding Box [     508.01       857.8      1213.2      1950.9]\n","Logic frame count : 284\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 1 surfboard, 174.0ms\n","Speed: 10.0ms preprocess, 174.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(491.7166), tensor(857.1184), tensor(713.1674), tensor(1092.6926)], tensor(0.6035), 0), ([tensor(574.5422), tensor(1081.6516), tensor(673.3103), tensor(1142.6333)], tensor(0.3880), 37), ([tensor(495.8758), tensor(861.5322), tensor(635.5995), tensor(1085.8507)], tensor(0.2845), 0), ([tensor(1824.0264), tensor(629.4542), tensor(1937.6147), tensor(710.5914)], tensor(0.2704), 8)]\n","Class person : ID 2: Confidence 0.2845466732978821 : Bounding Box [     491.88      862.03      1144.9      1949.4]\n","Class boat : ID 31: Confidence 0.27040383219718933 : Bounding Box [     1859.9      622.77      3723.3      1333.3]\n","Class person : ID 49: Confidence 0.603508472442627 : Bounding Box [      499.5      857.41      1205.4      1950.4]\n","Class surfboard : ID 52: Confidence 0.3879663348197937 : Bounding Box [     574.44      1081.6      1247.9      2224.2]\n","Logic frame count : 285\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 1 surfboard, 154.9ms\n","Speed: 13.6ms preprocess, 154.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(490.1631), tensor(857.5330), tensor(687.2412), tensor(1092.6237)], tensor(0.5766), 0), ([tensor(574.5090), tensor(1081.6586), tensor(673.4116), tensor(1142.6298)], tensor(0.3557), 37), ([tensor(500.8794), tensor(862.0378), tensor(635.9806), tensor(1086.2375)], tensor(0.3109), 0), ([tensor(1813.9009), tensor(613.3322), tensor(1942.4014), tensor(711.5571)], tensor(0.3045), 8)]\n","Class person : ID 2: Confidence 0.3109307587146759 : Bounding Box [     492.77      862.14      1143.3      1948.7]\n","Class boat : ID 31: Confidence 0.3045322597026825 : Bounding Box [     1852.3      616.43      3724.3      1327.1]\n","Class person : ID 49: Confidence 0.5765532851219177 : Bounding Box [     487.74      857.53      1191.8      1950.4]\n","Class surfboard : ID 52: Confidence 0.3556861877441406 : Bounding Box [     574.49      1081.7      1247.9      2224.3]\n","Logic frame count : 286\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 surfboard, 160.6ms\n","Speed: 3.9ms preprocess, 160.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(493.4668), tensor(856.9159), tensor(697.5171), tensor(1092.9072)], tensor(0.5681), 0), ([tensor(574.5605), tensor(1081.6528), tensor(673.4334), tensor(1142.5679)], tensor(0.3537), 37), ([tensor(509.0597), tensor(862.0803), tensor(635.9949), tensor(1090.0334)], tensor(0.2954), 0)]\n","Class person : ID 2: Confidence 0.2953667640686035 : Bounding Box [     498.45       862.2      1148.4        1951]\n","Class boat : ID 31: Confidence None : Bounding Box [     1856.7      615.49      3724.8      1324.7]\n","Class person : ID 49: Confidence 0.5681250095367432 : Bounding Box [     488.85      857.16      1192.4      1950.2]\n","Class surfboard : ID 52: Confidence 0.3536856174468994 : Bounding Box [     574.57      1081.7        1248      2224.3]\n","Logic frame count : 287\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 surfboard, 162.4ms\n","Speed: 12.4ms preprocess, 162.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(484.8302), tensor(856.4109), tensor(708.4058), tensor(1093.0499)], tensor(0.5712), 0), ([tensor(574.6656), tensor(1081.7151), tensor(673.3594), tensor(1142.4968)], tensor(0.3627), 37), ([tensor(503.3753), tensor(860.5520), tensor(633.9916), tensor(1086.4341)], tensor(0.2570), 0)]\n","Class person : ID 2: Confidence 0.2570391893386841 : Bounding Box [      497.4       861.2      1144.7      1948.4]\n","Class boat : ID 31: Confidence None : Bounding Box [       1861      614.55      3725.4      1322.3]\n","Class person : ID 49: Confidence 0.5711985230445862 : Bounding Box [     486.87      856.68      1190.9      1949.8]\n","Class surfboard : ID 52: Confidence 0.36265280842781067 : Bounding Box [     574.65      1081.7        1248      2224.2]\n","Logic frame count : 288\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 167.3ms\n","Speed: 4.7ms preprocess, 167.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(499.3367), tensor(856.7577), tensor(676.5848), tensor(1092.2367)], tensor(0.6055), 0), ([tensor(509.3123), tensor(860.4469), tensor(636.6107), tensor(1087.3800)], tensor(0.2995), 0)]\n","Class person : ID 2: Confidence 0.2995237112045288 : Bounding Box [     501.95      860.74      1148.1        1948]\n","Class boat : ID 31: Confidence None : Bounding Box [     1865.3      613.61      3725.9      1319.9]\n","Class person : ID 49: Confidence 0.6055111289024353 : Bounding Box [     486.81      856.73        1188      1949.4]\n","Class surfboard : ID 52: Confidence None : Bounding Box [      574.7      1081.8        1248      2224.3]\n","Logic frame count : 289\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 158.9ms\n","Speed: 11.0ms preprocess, 158.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(501.8249), tensor(855.8638), tensor(662.1725), tensor(1091.7095)], tensor(0.6285), 0)]\n","Class person : ID 2: Confidence None : Bounding Box [     501.47       860.8      1147.4        1948]\n","Class boat : ID 31: Confidence None : Bounding Box [     1869.6      612.67      3726.4      1317.5]\n","Class person : ID 49: Confidence 0.6285009384155273 : Bounding Box [      485.2      856.16      1182.4      1948.2]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     574.74      1081.9      1248.1      2224.3]\n","Logic frame count : 290\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 176.4ms\n","Speed: 5.7ms preprocess, 176.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(498.7803), tensor(855.8628), tensor(683.0754), tensor(1092.1039)], tensor(0.6304), 0), ([tensor(506.2603), tensor(859.2124), tensor(636.7819), tensor(1089.7026)], tensor(0.3558), 0)]\n","Class person : ID 2: Confidence 0.35584497451782227 : Bounding Box [     501.56      859.57      1147.4      1948.7]\n","Class boat : ID 31: Confidence None : Bounding Box [     1873.9      611.73      3726.9        1315]\n","Class person : ID 49: Confidence 0.6304194331169128 : Bounding Box [     489.46      855.94      1185.4      1948.1]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     574.79      1081.9      1248.1      2224.4]\n","Logic frame count : 291\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 163.1ms\n","Speed: 5.6ms preprocess, 163.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(498.6013), tensor(855.9983), tensor(676.3894), tensor(1092.0437)], tensor(0.6314), 0)]\n","Class person : ID 2: Confidence None : Bounding Box [     501.08      859.49      1146.9      1948.7]\n","Class boat : ID 31: Confidence None : Bounding Box [     1878.2      610.79      3727.4      1312.6]\n","Class person : ID 49: Confidence 0.6314172148704529 : Bounding Box [      489.5      855.95      1183.5      1948.1]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     574.83        1082      1248.1      2224.4]\n","Logic frame count : 292\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 166.0ms\n","Speed: 3.9ms preprocess, 166.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(496.4837), tensor(856.5261), tensor(644.9628), tensor(1092.6201)], tensor(0.6370), 0), ([tensor(493.8279), tensor(857.8971), tensor(710.2610), tensor(1090.6571)], tensor(0.4751), 0), ([tensor(1836.6594), tensor(605.0760), tensor(1953.3306), tensor(692.6516)], tensor(0.2527), 8)]\n","Class person : ID 2: Confidence 0.6370468735694885 : Bounding Box [     496.43      857.14      1143.4      1949.1]\n","Class boat : ID 31: Confidence 0.2526935040950775 : Bounding Box [     1890.1      605.33      3735.7      1298.4]\n","Class person : ID 49: Confidence 0.4751380681991577 : Bounding Box [     496.74      857.21      1191.8      1948.4]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     574.88      1082.1      1248.2      2224.5]\n","Logic frame count : 293\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 215.5ms\n","Speed: 4.3ms preprocess, 215.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(493.9864), tensor(856.9380), tensor(638.1974), tensor(1094.5465)], tensor(0.6466), 0), ([tensor(490.8356), tensor(858.2166), tensor(706.6261), tensor(1092.2595)], tensor(0.3793), 0), ([tensor(1843.0171), tensor(601.4397), tensor(1957.1851), tensor(688.9883)], tensor(0.2730), 8)]\n","Class person : ID 2: Confidence 0.6466185450553894 : Bounding Box [     491.59       856.9      1138.5      1950.7]\n","Class boat : ID 31: Confidence 0.27303874492645264 : Bounding Box [     1893.2      602.22      3746.6      1291.8]\n","Class person : ID 49: Confidence 0.37932485342025757 : Bounding Box [     495.79       857.9      1192.3      1949.8]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     574.93      1082.1      1248.2      2224.5]\n","Logic frame count : 294\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 246.3ms\n","Speed: 10.5ms preprocess, 246.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(498.4562), tensor(857.3114), tensor(641.0583), tensor(1092.8926)], tensor(0.6241), 0), ([tensor(495.6233), tensor(858.4598), tensor(709.8925), tensor(1090.9625)], tensor(0.4292), 0)]\n","Class person : ID 2: Confidence 0.4291936159133911 : Bounding Box [     511.69      857.83      1164.5      1949.9]\n","Class boat : ID 31: Confidence None : Bounding Box [     1899.6      600.73      3747.1      1288.1]\n","Class person : ID 49: Confidence 0.6241391897201538 : Bounding Box [     481.55      857.56      1173.2      1950.1]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     574.97      1082.2      1248.3      2224.6]\n","Logic frame count : 295\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 240.9ms\n","Speed: 4.4ms preprocess, 240.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(504.6119), tensor(857.2238), tensor(641.4943), tensor(1092.9232)], tensor(0.6294), 0), ([tensor(503.2939), tensor(858.2300), tensor(710.7597), tensor(1091.1251)], tensor(0.4328), 0), ([tensor(1857.5015), tensor(600.0510), tensor(1959.1230), tensor(686.2185)], tensor(0.3505), 8)]\n","Class person : ID 2: Confidence 0.43279868364334106 : Bounding Box [     522.83      858.04      1181.5      1949.6]\n","Class boat : ID 31: Confidence 0.35052168369293213 : Bounding Box [       1905      599.88      3764.6      1285.9]\n","Class person : ID 49: Confidence 0.6293994188308716 : Bounding Box [     481.79      857.37      1168.8      1950.2]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.02      1082.3      1248.3      2224.6]\n","Logic frame count : 296\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 210.5ms\n","Speed: 9.2ms preprocess, 210.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(504.8515), tensor(857.2251), tensor(647.1892), tensor(1093.9159)], tensor(0.6040), 0), ([tensor(503.4997), tensor(858.2662), tensor(715.0731), tensor(1092.1216)], tensor(0.4292), 0), ([tensor(1856.3071), tensor(592.8165), tensor(1959.8162), tensor(682.8057)], tensor(0.3809), 8)]\n","Class person : ID 2: Confidence 0.4292353391647339 : Bounding Box [     526.61      858.14      1191.3      1950.1]\n","Class boat : ID 31: Confidence 0.3808659613132477 : Bounding Box [     1904.2      594.63      3770.1      1277.8]\n","Class person : ID 49: Confidence 0.603973925113678 : Bounding Box [     484.82      857.29      1168.5      1950.9]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.06      1082.3      1248.3      2224.7]\n","Logic frame count : 297\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 247.3ms\n","Speed: 4.0ms preprocess, 247.3ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(505.3663), tensor(857.6062), tensor(642.3476), tensor(1093.5552)], tensor(0.6125), 0), ([tensor(1861.1531), tensor(590.3916), tensor(1961.1611), tensor(682.7739)], tensor(0.4921), 8), ([tensor(503.8601), tensor(858.4977), tensor(711.1848), tensor(1091.7535)], tensor(0.3964), 0)]\n","Class person : ID 2: Confidence 0.39644885063171387 : Bounding Box [     525.69      858.34        1195      1950.3]\n","Class boat : ID 31: Confidence 0.49208948016166687 : Bounding Box [     1904.1      591.19      3779.1      1273.3]\n","Class person : ID 49: Confidence 0.6125326156616211 : Bounding Box [      486.1      857.52      1165.9      1951.2]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.11      1082.4      1248.4      2224.7]\n","Logic frame count : 298\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 229.0ms\n","Speed: 4.3ms preprocess, 229.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(504.5101), tensor(857.2051), tensor(639.2466), tensor(1093.3224)], tensor(0.6454), 0), ([tensor(1858.5967), tensor(590.7332), tensor(1961.9873), tensor(677.7236)], tensor(0.5184), 8), ([tensor(502.5193), tensor(857.8395), tensor(696.6641), tensor(1091.4635)], tensor(0.3640), 0)]\n","Class person : ID 2: Confidence 0.36395159363746643 : Bounding Box [     519.13      857.99      1191.1      1949.7]\n","Class boat : ID 31: Confidence 0.5184340476989746 : Bounding Box [     1903.3       590.2      3779.9      1268.7]\n","Class person : ID 49: Confidence 0.6454090476036072 : Bounding Box [     486.28      857.33      1162.1      1950.8]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.15      1082.5      1248.4      2224.8]\n","Logic frame count : 299\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 242.0ms\n","Speed: 9.5ms preprocess, 242.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(505.3289), tensor(857.9800), tensor(637.7922), tensor(1093.4340)], tensor(0.6308), 0), ([tensor(1858.1045), tensor(591.5861), tensor(1964.3960), tensor(676.3047)], tensor(0.4230), 8), ([tensor(503.0047), tensor(858.5114), tensor(696.2540), tensor(1091.4994)], tensor(0.3388), 0)]\n","Class person : ID 2: Confidence 0.33876755833625793 : Bounding Box [     516.06       858.3      1190.3      1949.9]\n","Class boat : ID 31: Confidence 0.4230494499206543 : Bounding Box [     1901.5      590.48      3782.5      1266.8]\n","Class person : ID 49: Confidence 0.6307554244995117 : Bounding Box [     487.54      857.77      1159.8      1951.3]\n","Class surfboard : ID 52: Confidence None : Bounding Box [      575.2      1082.5      1248.4      2224.8]\n","Logic frame count : 300\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 boat, 251.0ms\n","Speed: 7.8ms preprocess, 251.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(505.2792), tensor(857.6605), tensor(641.6222), tensor(1092.8998)], tensor(0.6359), 0), ([tensor(1858.0804), tensor(586.4764), tensor(1970.9764), tensor(678.7582)], tensor(0.4872), 8), ([tensor(503.1661), tensor(858.5076), tensor(708.1385), tensor(1091.1025)], tensor(0.3537), 0)]\n","Class person : ID 2: Confidence 0.3536752462387085 : Bounding Box [     517.72      858.42      1195.1      1949.7]\n","Class boat : ID 31: Confidence 0.4871509075164795 : Bounding Box [     1897.8      587.26      3790.4      1264.4]\n","Class person : ID 49: Confidence 0.6358591914176941 : Bounding Box [     490.18      857.73      1159.3      1950.9]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.24      1082.6      1248.5      2224.9]\n","Logic frame count : 301\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 2 boats, 255.1ms\n","Speed: 5.4ms preprocess, 255.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(505.5424), tensor(858.5111), tensor(638.4356), tensor(1092.3379)], tensor(0.6767), 0), ([tensor(1858.0183), tensor(583.8170), tensor(1974.6577), tensor(672.6444)], tensor(0.5690), 8), ([tensor(503.6448), tensor(859.3326), tensor(694.9373), tensor(1090.2394)], tensor(0.3575), 0), ([tensor(1733.1619), tensor(1016.0322), tensor(2033.6511), tensor(1438.8616)], tensor(0.2501), 8)]\n","Class person : ID 2: Confidence 0.3575187921524048 : Bounding Box [     514.19      859.01      1192.9      1949.6]\n","Class boat : ID 31: Confidence 0.5690445899963379 : Bounding Box [     1899.7      584.36      3792.3      1257.9]\n","Class person : ID 49: Confidence 0.6766996383666992 : Bounding Box [     491.36      858.27      1157.3      1950.9]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.29      1082.7      1248.5      2224.9]\n","Logic frame count : 302\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 boat, 202.6ms\n","Speed: 8.4ms preprocess, 202.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(500.9644), tensor(855.5809), tensor(638.4154), tensor(1092.2814)], tensor(0.6787), 0), ([tensor(1860.3506), tensor(583.4948), tensor(1973.6189), tensor(671.2430)], tensor(0.3668), 8)]\n","Class person : ID 2: Confidence None : Bounding Box [     515.54      859.04      1194.1      1949.7]\n","Class boat : ID 31: Confidence 0.36679282784461975 : Bounding Box [       1900      583.13      3795.5      1254.4]\n","Class person : ID 49: Confidence 0.6786884069442749 : Bounding Box [     489.59      856.53      1152.8      1948.9]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.34      1082.7      1248.5        2225]\n","Logic frame count : 303\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 167.7ms\n","Speed: 4.5ms preprocess, 167.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(505.6146), tensor(854.4548), tensor(638.7219), tensor(1091.6311)], tensor(0.6568), 0)]\n","Class person : ID 2: Confidence None : Bounding Box [     516.88      859.07      1195.4      1949.7]\n","Class boat : ID 31: Confidence None : Bounding Box [     1905.8      581.25      3795.5      1250.5]\n","Class person : ID 49: Confidence 0.656795084476471 : Bounding Box [     492.92      855.13      1153.5        1947]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.38      1082.8      1248.6        2225]\n","Logic frame count : 304\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 motorcycle, 1 boat, 160.1ms\n","Speed: 4.3ms preprocess, 160.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(507.0739), tensor(855.1847), tensor(637.4509), tensor(1091.6799)], tensor(0.6412), 0), ([tensor(1726.9727), tensor(1013.3740), tensor(2035.3130), tensor(1438.0496)], tensor(0.3239), 3), ([tensor(505.0241), tensor(854.8280), tensor(695.9205), tensor(1089.7714)], tensor(0.3135), 0), ([tensor(1844.8362), tensor(587.3762), tensor(1977.9377), tensor(672.0630)], tensor(0.2715), 8)]\n","Class person : ID 2: Confidence 0.31350064277648926 : Bounding Box [     513.66      855.49      1193.7      1945.4]\n","Class boat : ID 31: Confidence 0.2714766561985016 : Bounding Box [     1886.3      585.64      3789.9      1256.6]\n","Class person : ID 49: Confidence 0.6412322521209717 : Bounding Box [     495.43      855.09      1153.7      1946.8]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.43      1082.9      1248.6      2225.1]\n","Logic frame count : 305\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 154.2ms\n","Speed: 6.7ms preprocess, 154.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(504.0588), tensor(856.4071), tensor(636.5134), tensor(1092.0140)], tensor(0.6665), 0), ([tensor(1727.0161), tensor(1013.8362), tensor(2035.1772), tensor(1438.0667)], tensor(0.3030), 3)]\n","Class person : ID 2: Confidence None : Bounding Box [     514.65      855.16      1194.5        1945]\n","Class boat : ID 31: Confidence None : Bounding Box [     1889.8      584.44      3788.7      1253.7]\n","Class person : ID 49: Confidence 0.6665230393409729 : Bounding Box [     494.67      855.89        1151      1947.8]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.47      1082.9      1248.6      2225.1]\n","Logic frame count : 306\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 motorcycle, 165.8ms\n","Speed: 7.6ms preprocess, 165.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(508.7240), tensor(856.3477), tensor(637.9862), tensor(1092.2808)], tensor(0.6612), 0), ([tensor(506.9784), tensor(856.6431), tensor(697.1714), tensor(1090.9727)], tensor(0.3423), 0), ([tensor(1726.6445), tensor(1013.8889), tensor(2035.0715), tensor(1438.0200)], tensor(0.3233), 3)]\n","Class person : ID 2: Confidence 0.3423146903514862 : Bounding Box [     514.39      856.26      1196.7        1947]\n","Class boat : ID 31: Confidence None : Bounding Box [     1893.4      583.24      3787.6      1250.9]\n","Class person : ID 49: Confidence 0.6611803770065308 : Bounding Box [     498.42      856.16      1153.1      1948.3]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.52        1083      1248.7      2225.2]\n","Logic frame count : 307\n","Analyzing image for objects...\n","\n","0: 384x640 2 persons, 1 motorcycle, 2 boats, 160.4ms\n","Speed: 8.3ms preprocess, 160.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(507.4977), tensor(856.4955), tensor(638.6550), tensor(1092.2671)], tensor(0.6602), 0), ([tensor(1728.1855), tensor(1015.9302), tensor(2034.5156), tensor(1438.9136)], tensor(0.4026), 3), ([tensor(505.7471), tensor(856.7194), tensor(692.6163), tensor(1090.7949)], tensor(0.3595), 0), ([tensor(1881.2603), tensor(584.5128), tensor(1980.7371), tensor(671.2304)], tensor(0.3285), 8), ([tensor(1844.5471), tensor(576.8434), tensor(1984.8909), tensor(669.8055)], tensor(0.2545), 8)]\n","Class person : ID 2: Confidence 0.35947734117507935 : Bounding Box [     511.76      856.51      1195.2      1947.3]\n","Class boat : ID 31: Confidence 0.25448092818260193 : Bounding Box [     1883.1      577.61      3792.3      1246.8]\n","Class person : ID 49: Confidence 0.6602259874343872 : Bounding Box [     499.71      856.36      1152.9      1948.6]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.56      1083.1      1248.7      2225.2]\n","Logic frame count : 308\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 161.3ms\n","Speed: 4.9ms preprocess, 161.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(508.3160), tensor(855.9315), tensor(638.0283), tensor(1092.7621)], tensor(0.6641), 0), ([tensor(1727.8394), tensor(1016.2539), tensor(2034.2251), tensor(1438.5474)], tensor(0.4661), 3)]\n","Class person : ID 2: Confidence None : Bounding Box [      512.4      856.38      1195.8      1947.2]\n","Class boat : ID 31: Confidence None : Bounding Box [     1885.8      575.97      3791.2      1243.8]\n","Class person : ID 49: Confidence 0.664148211479187 : Bounding Box [      500.9      856.07      1152.9      1948.6]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.61      1083.1      1248.8      2225.3]\n","Class motorcycle : ID 54: Confidence 0.4660547375679016 : Bounding Box [     1727.1      1016.2      3763.1      2454.9]\n","Logic frame count : 309\n","Analyzing image for objects...\n","\n","0: 384x640 1 person, 1 motorcycle, 169.5ms\n","Speed: 5.4ms preprocess, 169.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","Processing detections...\n","[([tensor(512.8374), tensor(855.5178), tensor(636.0740), tensor(1093.9963)], tensor(0.6216), 0), ([tensor(1727.4778), tensor(1016.5291), tensor(2034.5398), tensor(1438.5413)], tensor(0.5445), 3)]\n","Class person : ID 2: Confidence None : Bounding Box [     513.04      856.24      1196.4      1947.1]\n","Class boat : ID 31: Confidence None : Bounding Box [     1888.4      574.32      3790.1      1240.9]\n","Class person : ID 49: Confidence 0.6216455101966858 : Bounding Box [     503.98      855.68      1154.9      1949.2]\n","Class surfboard : ID 52: Confidence None : Bounding Box [     575.65      1083.2      1248.8      2225.3]\n","Class motorcycle : ID 54: Confidence 0.5445199608802795 : Bounding Box [     1726.9      1016.6      3762.8      2455.2]\n"]}]},{"cell_type":"code","source":["print(tracked_summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upIVwddwzlm0","executionInfo":{"status":"ok","timestamp":1749971244283,"user_tz":-330,"elapsed":1588,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"757d2695-0e5e-4e28-b05e-fd31d1ab6991"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["{5: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7236363291740417, 'xyxy': array([     1722.3,      938.46,      3585.7,      2247.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2673659324645996, 'xyxy': array([     1859.1,       972.4,      3775.7,        2045])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3377344608306885, 'xyxy': array([     3.6177,      559.11,      761.45,      1500.2])}}, 6: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.6964578628540039, 'xyxy': array([     1723.5,      937.35,        3587,      2246.2])}, 'object2': {'class': 'bicycle', 'track_id': '2', 'confidence_score': 0.28645315766334534, 'xyxy': array([     1584.1,        1048,      3946.3,      2384.6])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4098939597606659, 'xyxy': array([    -1.3403,      557.18,      755.49,      1497.6])}}, 7: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7199354767799377, 'xyxy': array([     1724.4,      937.23,      3587.8,      2246.1])}, 'object2': {'class': 'bicycle', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     1520.3,      1065.2,      3987.5,      2461.2])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.38860443234443665, 'xyxy': array([    -4.8588,      556.91,      751.07,      1497.3])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.27088820934295654, 'xyxy': array([     1569.8,      733.66,      3266.9,        1544])}}, 8: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7413342595100403, 'xyxy': array([     1727.5,      938.49,      3591.1,      2246.9])}, 'object2': {'class': 'bicycle', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     1456.4,      1082.4,      4028.7,      2537.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3803648352622986, 'xyxy': array([    -4.2716,      557.63,      751.66,      1498.7])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.2823488414287567, 'xyxy': array([     1570.4,      734.38,      3267.2,      1544.7])}}, 9: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.6900205612182617, 'xyxy': array([       1726,      938.62,      3589.3,      2246.1])}, 'object2': {'class': 'bicycle', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     1392.5,      1099.5,      4069.9,      2614.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.39008209109306335, 'xyxy': array([  -0.073595,      557.59,      755.43,      1497.9])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1570.6,      734.59,      3267.4,      1544.8])}}, 10: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7544078826904297, 'xyxy': array([     1715.9,      938.74,      3581.8,      2247.4])}, 'object2': {'class': 'bicycle', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     1328.6,      1116.7,      4111.1,      2691.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.45028337836265564, 'xyxy': array([   -0.47398,      556.08,       754.3,      1495.7])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.2575945258140564, 'xyxy': array([     1570.7,      734.67,      3267.9,      1545.1])}}, 11: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.673297107219696, 'xyxy': array([     1713.6,      938.69,      3568.7,      2238.4])}, 'object2': {'class': 'bicycle', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     1264.7,      1133.8,      4152.3,      2767.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4799640476703644, 'xyxy': array([    -3.2664,      556.17,      750.03,        1495])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.27060800790786743, 'xyxy': array([     1570.7,      734.59,        3268,      1545.1])}}, 12: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7289037108421326, 'xyxy': array([       1705,      938.51,      3556.1,      2234.5])}, 'object2': {'class': 'bicycle', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     1200.8,        1151,      4193.5,      2844.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4585087299346924, 'xyxy': array([    -2.4093,      556.85,       750.8,      1496.1])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.2651703655719757, 'xyxy': array([     1570.8,      734.61,      3268.1,      1545.1])}}, 13: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7277493476867676, 'xyxy': array([     1699.6,      938.65,      3549.6,      2233.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2687870264053345, 'xyxy': array([       1801,      987.61,        3812,      2125.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.45909008383750916, 'xyxy': array([     -5.732,      558.28,      747.23,      1498.9])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1570.9,      734.73,      3268.3,      1545.2])}}, 14: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.702575147151947, 'xyxy': array([     1699.2,      938.52,      3550.9,      2234.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2598242461681366, 'xyxy': array([     1762.7,      977.27,        3818,      2149.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.44628849625587463, 'xyxy': array([    -6.4424,      556.84,      746.16,      1498.6])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.1,      734.86,      3268.5,      1545.4])}}, 15: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.715709388256073, 'xyxy': array([     1701.2,       939.1,      3555.8,        2237])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2779090106487274, 'xyxy': array([     1757.7,      974.72,      3811.6,      2154.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.44194725155830383, 'xyxy': array([    -6.4988,      560.64,      744.14,      1501.5])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.2,      734.98,      3268.7,      1545.5])}}, 16: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7407607436180115, 'xyxy': array([     1704.2,      940.55,      3561.5,      2240.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.28363633155822754, 'xyxy': array([     1756.3,      973.67,      3805.2,      2158.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4067404568195343, 'xyxy': array([    -5.4017,      560.87,      744.98,      1502.5])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.4,      735.11,      3268.9,      1545.7])}}, 17: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7744251489639282, 'xyxy': array([     1700.3,      940.92,      3557.8,      2242.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.27085062861442566, 'xyxy': array([     1771.1,      973.36,      3799.5,      2151.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.39761194586753845, 'xyxy': array([    -4.8705,      561.47,       744.6,      1503.1])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.5,      735.24,        3269,      1545.8])}}, 18: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7101247310638428, 'xyxy': array([     1699.1,      942.74,      3555.6,      2243.7])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.29845961928367615, 'xyxy': array([     1781.9,      972.93,      3793.6,        2146])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.38086867332458496, 'xyxy': array([     -4.174,      562.12,      744.94,      1504.2])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.7,      735.36,      3269.2,      1545.9])}}, 19: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7453113198280334, 'xyxy': array([     1700.8,      942.85,      3556.8,      2243.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.3342159688472748, 'xyxy': array([       1788,      972.87,      3789.7,      2144.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.35470372438430786, 'xyxy': array([    -3.2887,      562.95,       745.1,      1504.9])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.8,      735.49,      3269.4,      1546.1])}}, 20: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7080864310264587, 'xyxy': array([     1701.9,      943.86,      3558.7,      2245.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.31611499190330505, 'xyxy': array([     1788.3,      972.44,      3786.1,      2145.9])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.36187463998794556, 'xyxy': array([    -3.3346,      564.81,      744.48,      1506.9])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.9,      735.61,      3269.6,      1546.2])}}, 21: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7310882806777954, 'xyxy': array([     1701.4,      944.08,        3558,      2245.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.26040175557136536, 'xyxy': array([     1785.1,      972.31,        3779,      2147.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3701050877571106, 'xyxy': array([    -3.4839,       561.2,      743.74,      1503.5])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1572.1,      735.74,      3269.8,      1546.4])}}, 22: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.7772294282913208, 'xyxy': array([     1704.1,         945,      3558.7,      2244.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2560904622077942, 'xyxy': array([     1843.7,      971.75,      3751.5,      2091.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.37515994906425476, 'xyxy': array([    -3.3591,      560.12,      743.16,      1502.4])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1572.2,      735.86,      3269.9,      1546.5])}}, 23: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.710101842880249, 'xyxy': array([     1704.4,      945.11,      3557.8,      2242.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2557491660118103, 'xyxy': array([     1731.6,       952.8,      3696.9,      2114.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.40658262372016907, 'xyxy': array([    -1.0768,      557.44,      746.37,      1500.8])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1572.4,      735.99,      3270.1,      1546.7])}}, 24: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.2688494622707367, 'xyxy': array([     1683.5,      992.92,      3530.8,      2289.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6308862566947937, 'xyxy': array([     1608.8,      949.78,      3697.7,      2202.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.48115023970603943, 'xyxy': array([    -2.4494,      559.31,      743.56,        1502])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1572.5,      736.11,      3270.3,      1546.8])}}, 25: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1680.7,      998.57,      3527.2,      2294.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7588576674461365, 'xyxy': array([     1562.3,      947.69,      3687.4,      2240.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4808133840560913, 'xyxy': array([    -1.9157,      559.93,      744.72,        1504])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.3343971073627472, 'xyxy': array([     1571.3,      731.88,      3269.3,      1542.4])}}, 26: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.26386159658432007, 'xyxy': array([       1878,      980.23,      3616.6,      2181.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7616433501243591, 'xyxy': array([     1551.4,      949.49,      3676.6,      2259.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5362550616264343, 'xyxy': array([    -4.0313,      559.91,       740.9,      1503.7])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.3321009576320648, 'xyxy': array([     1571.1,      731.03,      3269.2,      1541.4])}}, 27: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.25503188371658325, 'xyxy': array([       1933,      977.02,      3637.5,      2135.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7554883360862732, 'xyxy': array([     1548.5,       951.5,      3661.7,      2270.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5499435067176819, 'xyxy': array([    -4.0801,      561.34,      738.77,      1503.8])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.3197128474712372, 'xyxy': array([     1571.1,      730.85,      3269.3,        1541])}}, 28: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.30027326941490173, 'xyxy': array([     1947.2,      975.88,      3642.6,      2109.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7932303547859192, 'xyxy': array([     1555.9,      949.55,      3646.8,      2270.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5108982920646667, 'xyxy': array([    -3.4877,      566.55,      739.09,      1509.8])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.2,      730.64,      3269.3,      1540.8])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3554593026638031, 'xyxy': array([     1305.8,      791.41,      2834.5,      1823.5])}}, 29: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3353298306465149, 'xyxy': array([     1954.9,      975.29,      3643.4,      2086.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7522002458572388, 'xyxy': array([     1568.7,      947.29,      3639.9,      2270.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.517519474029541, 'xyxy': array([    -2.6995,      568.51,      739.14,      1511.9])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1571.2,      730.42,      3269.3,      1540.5])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3028715252876282, 'xyxy': array([     1303.5,      788.75,      2832.9,      1821.3])}}, 30: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.31225213408470154, 'xyxy': array([     1949.6,      975.12,      3651.6,      2080.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7785013318061829, 'xyxy': array([       1580,      950.47,      3628.3,      2273.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.49160200357437134, 'xyxy': array([    -2.4905,      568.26,      738.64,      1511.7])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.3409014046192169, 'xyxy': array([     1569.6,      731.55,      3267.4,      1541.6])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.28029438853263855, 'xyxy': array([     1302.4,      793.12,      2832.4,      1826.1])}}, 31: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.328205943107605, 'xyxy': array([     1942.2,      975.62,      3662.4,      2078.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8181535601615906, 'xyxy': array([     1589.7,      948.57,      3615.4,      2269.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5348625779151917, 'xyxy': array([    -1.4343,       567.5,      738.56,      1510.1])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.3505733609199524, 'xyxy': array([     1568.3,      731.74,      3265.4,      1541.8])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3134942650794983, 'xyxy': array([     1302.8,      793.61,      2832.8,      1826.5])}}, 32: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.34419605135917664, 'xyxy': array([     1927.6,      974.53,      3676.5,      2085.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8255194425582886, 'xyxy': array([     1594.9,      949.55,      3600.4,      2269.9])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5833113789558411, 'xyxy': array([   -0.99241,      566.94,      737.95,      1508.9])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.39914247393608093, 'xyxy': array([     1567.7,       731.8,      3263.8,      1541.7])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3169173300266266, 'xyxy': array([     1303.8,      794.51,      2832.9,      1826.7])}}, 33: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.32342901825904846, 'xyxy': array([       1890,      974.57,      3701.9,      2119.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8334298133850098, 'xyxy': array([     1597.4,      950.37,      3585.5,        2271])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5497671365737915, 'xyxy': array([   -0.75899,      566.43,       737.7,      1508.4])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.39090853929519653, 'xyxy': array([     1567.5,      731.72,        3263,      1541.7])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1303.2,      792.61,      2833.5,      1825.6])}}, 34: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3127368986606598, 'xyxy': array([     1859.5,      975.41,      3712.1,        2143])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8434321880340576, 'xyxy': array([     1597.4,      952.44,      3571.7,      2275.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5391820073127747, 'xyxy': array([   -0.49618,      566.96,      737.52,        1509])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.34299758076667786, 'xyxy': array([     1567.3,      731.32,      3262.3,      1541.3])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.4575153887271881, 'xyxy': array([     1305.4,       798.6,      2834.5,      1830.7])}}, 35: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.28864750266075134, 'xyxy': array([     1845.5,      975.89,      3719.9,      2154.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8140861988067627, 'xyxy': array([     1599.2,      954.58,      3555.2,      2275.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5231857895851135, 'xyxy': array([   -0.72704,      567.14,      737.14,      1509.5])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.2597847282886505, 'xyxy': array([     1567.3,      730.87,      3261.8,        1541])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.4537612497806549, 'xyxy': array([     1307.4,      798.63,      2836.6,      1830.4])}}, 36: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3056165874004364, 'xyxy': array([     1841.6,       976.1,      3724.2,        2157])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8447884917259216, 'xyxy': array([     1597.8,      957.85,      3539.2,      2278.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5066836476325989, 'xyxy': array([   -0.57584,      567.37,      737.01,      1509.9])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.2915691137313843, 'xyxy': array([     1567.6,      730.71,      3261.8,      1540.9])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.5013687610626221, 'xyxy': array([     1309.9,      801.35,      2838.9,      1832.7])}}, 37: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.27814388275146484, 'xyxy': array([     1872.9,      975.74,      3704.4,      2115.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8416656255722046, 'xyxy': array([     1598.5,      959.36,      3525.7,        2280])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4872233271598816, 'xyxy': array([    0.18713,      566.84,      737.47,      1509.1])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1567.3,      730.58,      3261.5,      1540.8])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.41947516798973083, 'xyxy': array([     1308.1,      801.13,      2838.5,      1833.3])}}, 38: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3046521544456482, 'xyxy': array([     1882.7,      975.29,      3698.2,      2097.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8366967439651489, 'xyxy': array([     1601.5,       961.3,      3516.4,      2282.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.48303160071372986, 'xyxy': array([    0.34067,      567.29,      737.29,      1509.4])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1566.9,      730.46,      3261.2,      1540.7])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.39851754903793335, 'xyxy': array([     1307.4,      799.17,      2838.2,      1831.5])}}, 39: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.254201740026474, 'xyxy': array([     1860.5,      975.92,        3712,      2116.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8471227288246155, 'xyxy': array([     1599.5,      961.64,      3505.5,      2285.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4901758134365082, 'xyxy': array([    0.37121,      567.59,      737.11,      1509.7])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1566.6,      730.33,      3260.8,      1540.6])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.37519413232803345, 'xyxy': array([     1306.9,      795.78,      2838.2,      1828.2])}}, 40: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.33938711881637573, 'xyxy': array([       1878,      975.29,      3707.7,      2095.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8391422629356384, 'xyxy': array([     1587.1,      963.45,      3493.2,      2296.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.49988487362861633, 'xyxy': array([   0.058535,      567.71,      737.16,      1510.6])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.2581162750720978, 'xyxy': array([     1569.6,      731.33,      3263.4,      1541.3])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.4562067687511444, 'xyxy': array([     1307.9,      795.96,      2838.1,      1827.5])}}, 41: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.39488646388053894, 'xyxy': array([     1875.4,         976,        3708,      2091.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8244759440422058, 'xyxy': array([     1560.8,      964.21,        3465,      2305.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4949183166027069, 'xyxy': array([   0.079892,      567.33,      737.14,      1510.5])}, 'object4': {'class': 'umbrella', 'track_id': '7', 'confidence_score': 0.25395333766937256, 'xyxy': array([     1569.8,      731.44,      3263.8,      1541.5])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3618858754634857, 'xyxy': array([     1307.6,      797.35,      2837.9,      1828.8])}}, 42: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.25248536467552185, 'xyxy': array([     1845.1,      976.28,      3724.3,      2119.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8568687438964844, 'xyxy': array([     1543.7,      967.15,      3441.5,      2312.9])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.502098560333252, 'xyxy': array([  -0.053962,      567.38,      737.18,      1511.1])}, 'object4': {'class': 'person', 'track_id': '7', 'confidence_score': 0.33149391412734985, 'xyxy': array([     1632.7,      892.74,      3701.6,      1902.2])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.4271995425224304, 'xyxy': array([     1307.2,      793.26,      2837.9,      1824.8])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.254808634519577, 'xyxy': array([     1904.9,      974.59,      3726.9,      2064.9])}}, 43: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.25973495841026306, 'xyxy': array([     1825.9,      976.27,      3730.1,      2133.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8729144334793091, 'xyxy': array([     1533.4,      968.85,      3425.4,        2319])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5122024416923523, 'xyxy': array([    0.23565,      566.92,      737.13,      1510.6])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': 0.31200817227363586, 'xyxy': array([       1423,      1063.3,      3939.3,      2342.6])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.40387195348739624, 'xyxy': array([       1308,      796.35,      2839.3,      1828.2])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.333429217338562, 'xyxy': array([     1870.5,      975.21,      3725.1,      2083.3])}}, 44: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3323740065097809, 'xyxy': array([     1857.5,      975.09,      3722.9,      2103.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8728076815605164, 'xyxy': array([     1537.2,      969.72,      3420.8,      2322.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4949354827404022, 'xyxy': array([  -0.037782,      567.35,      736.88,      1511.4])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': 0.2839510440826416, 'xyxy': array([     1365.6,      1135.4,      4002.1,      2526.9])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.2606371343135834, 'xyxy': array([     1305.7,      800.35,      2838.2,      1833.2])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.25846195220947266, 'xyxy': array([     1823.7,      975.58,      3744.3,      2123.9])}, 'object7': {'class': 'boat', 'track_id': '20', 'confidence_score': 0.26721876859664917, 'xyxy': array([     1726.6,        1019,      3760.1,      2458.7])}}, 45: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.35346755385398865, 'xyxy': array([     1868.9,      974.59,      3723.1,        2091])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8683372735977173, 'xyxy': array([     1550.2,      970.72,      3424.1,        2324])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.48105525970458984, 'xyxy': array([    0.60441,      567.86,      737.21,      1511.6])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1345.4,      1168.6,      4071.5,      2607.4])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1305.6,      800.41,      2838.5,      1833.5])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1816.3,      975.66,      3746.5,      2129.7])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3395865261554718, 'xyxy': array([     1676.2,      1124.6,        3702,      2564.6])}}, 46: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3460899889469147, 'xyxy': array([     1874.7,      974.26,      3727.8,      2085.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8331116437911987, 'xyxy': array([     1562.9,      972.04,        3428,      2326.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.47622233629226685, 'xyxy': array([    0.48482,      568.14,      737.32,      1512.4])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1325.1,      1201.9,      4140.8,      2687.9])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1305.5,      800.47,      2838.8,      1833.8])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.3130451738834381, 'xyxy': array([     1882.3,      974.92,        3737,      2079.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.38283541798591614, 'xyxy': array([     1666.5,      1162.1,      3684.8,      2602.2])}}, 47: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.36950400471687317, 'xyxy': array([     1868.4,      973.95,      3730.4,      2086.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8549137711524963, 'xyxy': array([     1564.9,      973.57,      3419.2,      2328.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4868454337120056, 'xyxy': array([    0.33926,      568.37,      736.86,      1512.4])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1304.8,      1235.1,      4210.2,      2768.5])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1305.4,      800.52,      2839.1,      1834.1])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.27244117856025696, 'xyxy': array([     1817.4,      974.95,      3759.5,      2134.3])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.35711461305618286, 'xyxy': array([     1667.5,      1174.4,      3678.2,      2614.5])}}, 48: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3853906989097595, 'xyxy': array([     1867.4,      974.08,      3731.7,      2084.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8456022143363953, 'xyxy': array([     1555.7,      975.71,        3397,      2328.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.48408424854278564, 'xyxy': array([    0.40376,      568.52,      736.72,      1512.4])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1284.6,      1268.4,      4279.5,        2849])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1305.3,      800.58,      2839.3,      1834.5])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1811.1,      974.91,      3762.9,        2140])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.30216318368911743, 'xyxy': array([     1658.4,      1232.9,      3658.4,        2673])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3061201274394989, 'xyxy': array([       1569,       731.2,      3261.6,      1541.2])}}, 49: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3737972378730774, 'xyxy': array([     1860.8,      973.92,      3736.6,      2088.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8434794545173645, 'xyxy': array([     1526.2,      976.76,      3355.9,      2329.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.467349112033844, 'xyxy': array([   0.096993,      567.67,       736.4,      1511.7])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1264.3,      1301.6,      4348.9,      2929.6])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1305.2,      800.63,      2839.6,      1834.8])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.29097574949264526, 'xyxy': array([     1788.6,      974.94,      3757.9,      2153.1])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3645859956741333, 'xyxy': array([     1668.5,        1201,      3660.3,        2641])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.438703715801239, 'xyxy': array([     1568.9,      731.88,      3261.4,      1541.9])}}, 50: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.4105038046836853, 'xyxy': array([     1846.5,      973.97,      3747.2,      2102.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.843880295753479, 'xyxy': array([     1505.3,      978.87,      3322.7,      2329.6])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4637952446937561, 'xyxy': array([   0.092375,      567.42,      736.26,      1511.3])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1244.1,      1334.9,      4418.2,      3010.1])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1305,      800.69,      2839.9,      1835.1])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.27583789825439453, 'xyxy': array([     1852.8,      974.29,      3734.7,      2094.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3474932014942169, 'xyxy': array([     1676.5,      1187.8,      3660.6,      2627.8])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.35178956389427185, 'xyxy': array([     1569.2,      731.74,      3261.4,      1541.7])}}, 51: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.4514791965484619, 'xyxy': array([     1842.6,      973.75,      3747.7,      2103.7])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8094537258148193, 'xyxy': array([     1492.5,         981,      3300.6,      2331.6])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4599408805370331, 'xyxy': array([   0.013943,      567.38,      736.17,      1511.4])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1223.8,      1368.1,      4487.6,      3090.6])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.26552993059158325, 'xyxy': array([     1275.7,      724.62,      2603.4,      1617.4])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.42470088601112366, 'xyxy': array([     1804.5,      974.71,      3748.8,      2135.1])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.348660945892334, 'xyxy': array([     1682.4,      1183.5,      3659.3,      2623.5])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3093559443950653, 'xyxy': array([     1569.1,      731.47,      3261.3,      1541.5])}}, 52: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.4651404917240143, 'xyxy': array([     1839.6,      974.05,      3751.7,      2107.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8359587788581848, 'xyxy': array([     1483.5,      983.65,      3284.5,      2335.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.46255412697792053, 'xyxy': array([    0.27131,      567.14,      735.55,        1510])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1203.6,      1401.3,      4556.9,      3171.2])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.2548975348472595, 'xyxy': array([     1283.3,       717.5,      2586.9,      1589.8])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.4379892349243164, 'xyxy': array([     1793.7,      975.24,      3758.8,      2150.7])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.47556155920028687, 'xyxy': array([     1689.8,      1178.5,      3660.8,      2618.6])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3503054678440094, 'xyxy': array([     1569.4,      731.37,      3261.5,      1541.3])}}, 53: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3689929246902466, 'xyxy': array([     1854.1,      975.18,        3739,      2088.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8462425470352173, 'xyxy': array([     1480.2,      982.87,      3273.2,      2334.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4630093574523926, 'xyxy': array([    0.31212,      567.09,      735.32,      1509.6])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1183.3,      1434.6,      4626.3,      3251.7])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.33823496103286743, 'xyxy': array([     1263.9,      713.73,        2601,      1610.2])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.46097898483276367, 'xyxy': array([     1815.8,      975.56,        3760,      2138.6])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.43191996216773987, 'xyxy': array([     1694.6,      1176.7,      3660.3,      2616.7])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3546218276023865, 'xyxy': array([     1569.6,      731.51,        3262,      1541.6])}}, 54: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.28899893164634705, 'xyxy': array([     1855.1,      977.46,      3737.8,      2087.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7928577661514282, 'xyxy': array([     1483.4,      979.18,      3270.6,      2332.2])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4653533399105072, 'xyxy': array([   -0.15243,      566.73,      734.69,      1509.2])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1163.1,      1467.8,      4695.6,      3332.3])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.2976849377155304, 'xyxy': array([       1265,      714.88,      2592.4,      1603.9])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.41315385699272156, 'xyxy': array([     1813.4,      977.78,      3767.4,      2148.6])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3478790819644928, 'xyxy': array([     1698.8,      1175.6,      3659.5,      2615.6])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.34863603115081787, 'xyxy': array([     1569.7,      731.79,      3262.5,      1542.1])}}, 55: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.27561622858047485, 'xyxy': array([     1860.4,       979.3,      3731.4,      2078.6])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7847111821174622, 'xyxy': array([     1486.2,      974.82,        3262,      2325.2])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.46080851554870605, 'xyxy': array([     -0.117,      566.22,       734.8,      1508.9])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1142.8,      1501.1,        4765,      3412.8])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.3206033706665039, 'xyxy': array([     1240.3,      713.13,        2608,      1634.7])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.327463299036026, 'xyxy': array([     1829.2,      979.86,        3745,        2126])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.4142040014266968, 'xyxy': array([     1701.2,      1174.7,      3657.2,      2614.8])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.37736114859580994, 'xyxy': array([     1569.7,       731.8,      3262.6,      1542.2])}}, 56: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3328222632408142, 'xyxy': array([     1842.9,      981.48,        3738,      2094.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4531099796295166, 'xyxy': array([     1471.8,      974.67,        3229,      2321.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.46425989270210266, 'xyxy': array([   0.023563,         566,      734.79,      1508.5])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1122.6,      1534.3,      4834.3,      3493.3])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.25383806228637695, 'xyxy': array([     1236.5,      711.27,      2615.2,      1644.3])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.40020838379859924, 'xyxy': array([     1803.5,      980.76,      3754.3,      2150.7])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.5195767879486084, 'xyxy': array([     1704.5,      1172.9,      3656.3,      2612.9])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.32778680324554443, 'xyxy': array([     1569.6,      731.71,      3262.6,      1542.2])}}, 57: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.2730559706687927, 'xyxy': array([     1820.4,      984.27,      3745.2,      2116.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7829081416130066, 'xyxy': array([       1459,      973.04,      3203.6,        2320])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4650840163230896, 'xyxy': array([   -0.24819,      565.68,      734.67,      1508.5])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1102.3,      1567.6,      4903.7,      3573.9])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1230.3,      706.98,      2607.8,      1639.3])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.3973832130432129, 'xyxy': array([     1792.8,      984.51,        3754,      2163.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.5445166230201721, 'xyxy': array([     1706.3,      1170.5,      3654.3,      2610.5])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3632515072822571, 'xyxy': array([     1569.7,      731.86,      3262.7,      1542.4])}}, 58: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1818.6,      985.23,      3745.1,      2118.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8039460182189941, 'xyxy': array([     1440.4,      972.07,      3178.7,      2325.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4638656675815582, 'xyxy': array([   -0.31632,      565.84,      734.74,      1508.9])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1082.1,      1600.8,        4973,      3654.4])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.44857677817344666, 'xyxy': array([     1221.4,      713.42,      2613.4,      1661.1])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.3296588659286499, 'xyxy': array([     1789.8,      989.54,      3749.8,      2170.4])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.400033175945282, 'xyxy': array([     1708.5,      1168.2,      3653.2,      2608.2])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2925380766391754, 'xyxy': array([     1570.1,      731.94,      3263.1,      1542.5])}}, 59: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1816.9,      986.19,        3745,      2120.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7434568405151367, 'xyxy': array([     1419.3,       973.1,      3140.6,      2323.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4647732079029083, 'xyxy': array([    -0.1916,      566.06,         735,      1509.4])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1061.8,      1634.1,      5042.4,        3735])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.3783107101917267, 'xyxy': array([     1223.6,      714.21,      2612.3,      1663.7])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1784.8,       990.9,      3750.9,      2175.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3117942810058594, 'xyxy': array([     1748.3,      1058.7,      3699.5,      2497.3])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31114205718040466, 'xyxy': array([       1570,      732.17,      3262.6,      1542.5])}}, 60: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1815.2,      987.15,        3745,      2122.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7907202243804932, 'xyxy': array([     1384.1,      972.84,        3088,      2319.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.46684321761131287, 'xyxy': array([   -0.20495,      566.06,      735.12,      1509.6])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1041.5,      1667.3,      5111.7,      3815.5])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.2558874189853668, 'xyxy': array([     1220.2,      774.43,      2602.8,      1724.3])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1779.7,      992.26,        3752,      2180.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.2810838520526886, 'xyxy': array([     1722.2,      1126.7,      3670.7,      2566.1])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.40415099263191223, 'xyxy': array([     1570.4,       732.6,      3262.8,      1542.8])}}, 61: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1813.5,      988.11,      3744.9,      2124.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.837352991104126, 'xyxy': array([       1356,      975.78,      3047.4,      2320.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4905957281589508, 'xyxy': array([    0.20134,      566.14,      735.74,      1510.1])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     1021.3,      1700.5,        5181,        3896])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.25091835856437683, 'xyxy': array([     1218.7,      741.83,      2598.5,      1694.5])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1774.6,      993.62,      3753.1,      2185.6])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.4834195375442505, 'xyxy': array([     1712.8,      1151.5,        3658,      2591.3])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3495294749736786, 'xyxy': array([     1570.4,      732.52,      3263.1,      1542.8])}}, 62: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1811.8,      989.07,      3744.8,        2126])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8588495850563049, 'xyxy': array([     1347.3,      978.93,      3028.4,        2323])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.49323052167892456, 'xyxy': array([    0.29096,      566.33,      735.81,      1510.4])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([       1001,      1733.8,      5250.4,      3976.6])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1212.5,      741.96,      2594.1,      1695.8])}, 'object6': {'class': 'bicycle', 'track_id': '19', 'confidence_score': 0.2592153549194336, 'xyxy': array([     1593.2,        1006,      3894.7,      2417.4])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.34321337938308716, 'xyxy': array([     1710.7,      1162.2,      3652.7,      2602.2])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3270798921585083, 'xyxy': array([     1569.8,      732.05,      3262.7,      1542.4])}}, 63: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1810.1,      990.03,      3744.7,      2127.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8415799736976624, 'xyxy': array([     1346.4,      981.54,      3019.2,      2325.9])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4882650375366211, 'xyxy': array([    0.15537,      566.84,      736.16,      1511.7])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     980.79,        1767,      5319.7,      4057.1])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.3614797592163086, 'xyxy': array([     1223.2,      727.57,      2593.5,      1677.9])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.27021536231040955, 'xyxy': array([     1718.6,      1003.4,        3791,      2274.2])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.43168315291404724, 'xyxy': array([     1711.3,      1165.8,      3650.2,      2605.8])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31571105122566223, 'xyxy': array([       1570,      732.15,      3263.3,      1542.7])}}, 64: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1808.3,      990.99,      3744.7,      2129.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8380433917045593, 'xyxy': array([     1346.4,         980,      3009.9,      2323.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4777207374572754, 'xyxy': array([  -0.066768,       567.1,      736.24,      1512.6])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     960.54,      1800.3,      5389.1,      4137.7])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.5356395244598389, 'xyxy': array([     1224.1,      723.62,      2589.5,      1674.3])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1708.2,      1005.3,      3796.6,      2285.9])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.45860037207603455, 'xyxy': array([     1712.3,      1166.5,      3648.3,      2606.5])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.36481356620788574, 'xyxy': array([     1569.7,      731.97,      3262.1,      1542.1])}}, 65: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1806.6,      991.95,      3744.6,      2131.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8153027296066284, 'xyxy': array([     1339.3,      976.39,      2993.6,      2320.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.48452821373939514, 'xyxy': array([   -0.40421,      567.37,      735.73,        1513])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     940.29,      1833.5,      5458.4,      4218.2])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.6511183381080627, 'xyxy': array([     1218.4,         721,        2579,      1671.8])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1697.8,      1007.3,      3802.1,      2297.7])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3801576495170593, 'xyxy': array([     1713.2,        1168,      3646.7,      2608.1])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3085256516933441, 'xyxy': array([     1569.7,      731.99,        3262,        1542])}}, 66: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1804.9,      992.91,      3744.5,      2133.7])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8322356939315796, 'xyxy': array([     1321.6,      973.93,      2965.3,      2319.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.49494096636772156, 'xyxy': array([    0.26075,      566.65,      735.36,      1511.1])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     920.04,      1866.8,      5527.8,      4298.7])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.6344496011734009, 'xyxy': array([     1219.9,      721.72,      2576.3,      1672.5])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1687.4,      1009.3,      3807.7,      2309.4])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.32993441820144653, 'xyxy': array([     1713.6,      1169.5,      3644.6,      2609.5])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.40405750274658203, 'xyxy': array([     1569.6,       732.1,      3261.7,      1542.1])}}, 67: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1803.2,      993.87,      3744.5,      2135.6])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.834327220916748, 'xyxy': array([     1307.1,      972.49,      2937.3,      2317.2])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4996373951435089, 'xyxy': array([    0.33115,      566.39,      735.28,      1510.9])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     899.79,        1900,      5597.1,      4379.3])}, 'object5': {'class': 'person', 'track_id': '16', 'confidence_score': 0.37357503175735474, 'xyxy': array([     1225.6,      721.51,        2576,      1670.2])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.2668918967247009, 'xyxy': array([     1767.9,      1007.1,      3714.8,      2202.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.44087356328964233, 'xyxy': array([     1715.4,      1168.6,      3644.5,      2608.7])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.33785948157310486, 'xyxy': array([     1569.7,      731.96,      3262.2,      1542.2])}}, 68: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1801.5,      994.83,      3744.4,      2137.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8714941740036011, 'xyxy': array([     1295.8,      971.86,      2912.7,      2314.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4918988049030304, 'xyxy': array([    0.41078,      566.53,      735.12,        1511])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     879.54,      1933.3,      5666.5,      4459.8])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.25947731733322144, 'xyxy': array([     1299.5,      777.41,      2720.9,      1770.4])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1765.4,      1008.7,      3711.7,      2203.7])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.403953492641449, 'xyxy': array([     1716.9,      1168.7,      3644.2,      2608.7])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.40073835849761963, 'xyxy': array([     1570.5,      732.45,      3263.6,      1542.8])}}, 69: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1799.7,      995.79,      3744.3,      2139.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8717107772827148, 'xyxy': array([       1281,      969.19,      2884.4,      2311.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4960978627204895, 'xyxy': array([     0.1264,       567.5,      733.86,      1510.9])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     859.29,      1966.5,      5735.8,      4540.4])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1305,      782.37,      2733.7,      1780.4])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1762.9,      1010.3,      3708.6,        2205])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.4496040344238281, 'xyxy': array([     1718.6,      1168.8,      3644.3,      2608.8])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2511257827281952, 'xyxy': array([     1570.3,      732.06,      3263.3,      1542.4])}}, 70: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([       1798,      996.75,      3744.2,      2141.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8320175409317017, 'xyxy': array([     1265.5,      967.81,      2858.1,      2312.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5081787109375, 'xyxy': array([    0.14882,      567.89,      733.49,      1510.9])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     839.04,      1999.7,      5805.1,      4620.9])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1310.6,      787.33,      2746.6,      1790.5])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.29826048016548157, 'xyxy': array([     1800.3,      1019.5,      3710.3,      2190.2])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.41009560227394104, 'xyxy': array([     1718.4,      1168.8,      3642.6,      2608.9])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.4,      732.07,      3263.4,      1542.4])}}, 71: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1796.3,      997.71,      3744.2,      2143.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8530373573303223, 'xyxy': array([     1252.8,       967.3,      2829.7,      2308.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.5044475793838501, 'xyxy': array([    0.17052,      568.23,      733.32,      1511.1])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     818.79,        2033,      5874.5,      4701.4])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1316.1,      792.29,      2759.4,      1800.5])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([       1802,      1021.8,      3707.6,      2189.8])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.42047640681266785, 'xyxy': array([     1718.5,        1168,      3641.3,        2608])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.4,      732.07,      3263.4,      1542.4])}}, 72: {'object1': {'class': 'motorcycle', 'track_id': '1', 'confidence_score': 0.2861579358577728, 'xyxy': array([     1565.5,      1019.6,      3923.4,      2454.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8572903275489807, 'xyxy': array([     1244.8,      965.02,      2807.9,      2302.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4940668046474457, 'xyxy': array([    0.22005,      568.16,       733.9,      1511.9])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     798.54,      2066.2,      5943.8,        4782])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.2528970539569855, 'xyxy': array([     1325.8,      808.64,      2797.1,      1830.9])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.272535502910614, 'xyxy': array([     1782.5,      1030.3,        3698,      2205.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3722386658191681, 'xyxy': array([     1719.2,      1166.6,      3640.9,      2606.6])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.5,      732.08,      3263.5,      1542.4])}}, 73: {'object1': {'class': 'motorcycle', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1548.5,      1021.8,      3935.7,      2474.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8355449438095093, 'xyxy': array([     1232.6,      956.97,      2777.7,      2288.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.49243173003196716, 'xyxy': array([   -0.25428,      568.27,      733.68,      1512.6])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     778.29,      2099.5,      6013.2,      4862.5])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1332.1,      814.63,      2811.8,      1842.8])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': 0.3018592298030853, 'xyxy': array([     1773.4,      1039.1,      3689.7,      2217.5])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3569600284099579, 'xyxy': array([     1722.3,      1164.9,      3643.7,      2604.8])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.5,      732.08,      3263.6,      1542.4])}}, 74: {'object1': {'class': 'motorcycle', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1531.5,      1023.9,        3948,      2494.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8633086681365967, 'xyxy': array([     1189.6,      949.01,      2712.4,      2273.6])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.48268768191337585, 'xyxy': array([    -0.3217,      568.01,      733.67,      1512.7])}, 'object4': {'class': 'bicycle', 'track_id': '7', 'confidence_score': None, 'xyxy': array([     758.04,      2132.7,      6082.5,      4943.1])}, 'object5': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1338.3,      820.62,      2826.6,      1854.7])}, 'object6': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1771.6,      1042.8,        3686,        2220])}, 'object7': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.46204131841659546, 'xyxy': array([     1724.4,      1162.4,      3645.7,      2602.4])}, 'object8': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.6,      732.09,      3263.7,      1542.4])}}, 75: {'object1': {'class': 'motorcycle', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1514.6,      1026.1,      3960.3,      2514.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8421517610549927, 'xyxy': array([     1164.4,      947.61,      2671.2,      2270.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.48951637744903564, 'xyxy': array([   0.085689,      567.68,      733.75,      1512.1])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': 0.44061970710754395, 'xyxy': array([     1264.6,      744.25,        2662,      1718.1])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1769.7,      1046.5,      3682.3,      2222.6])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.44094786047935486, 'xyxy': array([     1725.3,      1162.9,      3646.7,      2602.9])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.6,      732.09,      3263.7,      1542.4])}}, 76: {'object1': {'class': 'motorcycle', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1497.6,      1028.3,      3972.6,      2534.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8728433847427368, 'xyxy': array([     1144.1,      945.84,      2633.4,      2263.9])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4718244671821594, 'xyxy': array([    0.20923,      567.75,      733.91,      1512.3])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1263.2,      742.65,      2660.4,      1716.4])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1767.8,      1050.1,      3678.6,      2225.1])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.4734817445278168, 'xyxy': array([     1726.3,      1162.9,      3647.8,      2602.9])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.7,       732.1,      3263.8,      1542.4])}}, 77: {'object1': {'class': 'motorcycle', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1480.6,      1030.5,      3984.8,      2554.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.871126651763916, 'xyxy': array([     1131.4,      944.86,      2605.5,        2260])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4618804454803467, 'xyxy': array([    0.48029,      567.85,      733.96,      1512.1])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1261.9,      741.05,      2658.9,      1714.6])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': 0.3088656961917877, 'xyxy': array([     1776.3,      1056.4,      3678.6,      2229.9])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.47691982984542847, 'xyxy': array([     1726.2,      1164.2,      3647.6,      2604.2])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.8,       732.1,      3263.9,      1542.4])}}, 78: {'object1': {'class': 'motorcycle', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1463.6,      1032.7,      3997.1,      2574.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8893305063247681, 'xyxy': array([     1131.8,      946.67,      2592.3,      2259.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.46029412746429443, 'xyxy': array([     0.6498,      568.16,      734.28,      1512.6])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1260.6,      739.45,      2657.3,      1712.8])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': 0.2792949974536896, 'xyxy': array([     1777.8,      1058.9,      3676.7,      2233.9])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.5345380306243896, 'xyxy': array([     1726.1,      1165.8,      3647.4,      2605.8])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2507622539997101, 'xyxy': array([     1570.6,      732.14,      3263.7,      1542.3])}}, 79: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.29044440388679504, 'xyxy': array([     1726.3,      1056.1,      3712.6,      2275.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.884996771812439, 'xyxy': array([       1133,      944.96,      2583.9,      2257.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.45619773864746094, 'xyxy': array([    0.65734,      568.43,      734.17,      1512.7])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1259.3,      737.84,      2655.7,        1711])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': 0.2531464993953705, 'xyxy': array([       1778,      1059.6,      3671.3,      2233.4])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.4555476903915405, 'xyxy': array([     1726.1,      1166.9,      3647.4,      2606.9])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.7,      732.15,      3263.8,      1542.3])}}, 80: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': 0.3178545832633972, 'xyxy': array([       1759,      1058.6,      3680.2,      2242.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8864638209342957, 'xyxy': array([     1130.5,      945.05,        2570,      2255.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.46722519397735596, 'xyxy': array([    0.60249,      568.07,      733.99,      1512.2])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1257.9,      736.24,      2654.1,      1709.2])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1776.7,      1063.1,      3668.5,      2235.9])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3455739915370941, 'xyxy': array([     1726.1,      1166.2,      3647.5,      2606.2])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.8,      732.15,      3263.8,      1542.3])}}, 81: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1764.1,      1062.2,      3669.9,      2236.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8723835349082947, 'xyxy': array([     1131.6,      941.25,        2562,      2250.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.470181405544281, 'xyxy': array([    0.63386,      567.77,      733.86,      1511.8])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1256.6,      734.64,      2652.5,      1707.4])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1775.5,      1066.5,      3665.8,      2238.4])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3072735667228699, 'xyxy': array([     1722.3,      1168.4,      3642.5,      2608.4])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.8,      732.16,      3263.9,      1542.3])}}, 82: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1769.2,      1065.7,      3659.6,      2230.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8735778331756592, 'xyxy': array([     1124.7,      940.34,      2547.4,      2249.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4657690227031708, 'xyxy': array([    0.61458,      567.88,      733.83,      1511.9])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1255.3,      733.03,      2650.9,      1705.6])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1774.2,        1070,      3663.1,        2241])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3271424472332001, 'xyxy': array([     1724.1,      1165.5,      3644.2,      2605.5])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.9,      732.17,      3263.9,      1542.3])}}, 83: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1774.3,      1069.3,      3649.2,      2224.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8518145084381104, 'xyxy': array([     1118.3,      939.28,      2531.5,      2247.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.45730146765708923, 'xyxy': array([     0.6119,      567.68,      733.79,      1511.7])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1254,      731.43,      2649.3,      1703.8])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([       1773,      1073.4,      3660.3,      2243.5])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.25655269622802734, 'xyxy': array([     1725.2,      1163.5,      3645.4,      2603.5])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.9,      732.18,      3263.9,      1542.3])}}, 84: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1779.4,      1072.9,      3638.9,      2218.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8357917666435242, 'xyxy': array([     1120.6,      938.89,      2524.2,      2245.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4493110179901123, 'xyxy': array([    0.66873,      567.58,      733.76,      1511.5])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1252.6,      729.83,      2647.7,        1702])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1771.8,      1076.9,      3657.6,        2246])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1725.2,      1163.5,      3645.4,      2603.5])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1571,      732.18,        3264,      1542.3])}}, 85: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1784.5,      1076.4,      3628.5,      2212.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8212548494338989, 'xyxy': array([     1121.4,       939.2,      2517.3,      2245.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.43998658657073975, 'xyxy': array([     1.0338,      568.85,      733.58,      1511.7])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1251.3,      728.23,      2646.1,      1700.2])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1770.5,      1080.3,      3654.9,      2248.5])}, 'object6': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.260670930147171, 'xyxy': array([     1765.9,      1049.1,      3696.9,      2488.4])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1571,      732.19,        3264,      1542.3])}}, 86: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1789.6,        1080,      3618.2,      2206.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.834891140460968, 'xyxy': array([     1124.8,       937.9,      2512.3,      2242.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.42882096767425537, 'xyxy': array([     1.0594,      569.41,      733.34,      1511.6])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.27676403522491455, 'xyxy': array([     1292.4,      763.25,      2784.1,      1796.1])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1769.3,      1083.8,      3652.2,      2251.1])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.2784058153629303, 'xyxy': array([     1737.2,      1119.1,      3667.8,      2558.9])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2808515131473541, 'xyxy': array([     1571.7,      732.59,      3265.4,      1542.6])}}, 87: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1794.6,      1083.6,      3607.8,      2200.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.840106189250946, 'xyxy': array([     1125.6,      937.21,      2506.6,      2241.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.41650065779685974, 'xyxy': array([    0.92011,      569.83,       733.1,      1511.7])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.45203155279159546, 'xyxy': array([     1311.1,      785.15,      2812.2,      1819.1])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([       1768,      1087.2,      3649.4,      2253.6])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.29167354106903076, 'xyxy': array([     1727.5,      1145.4,      3657.5,      2585.3])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.29504460096359253, 'xyxy': array([     1571.5,      732.72,        3266,      1542.8])}}, 88: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1799.7,      1087.1,      3597.5,      2194.6])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8582502007484436, 'xyxy': array([     1124.3,       938.3,      2499.7,      2242.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4109361171722412, 'xyxy': array([    0.89056,      569.94,      733.04,      1511.5])}, 'object4': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.26741042733192444, 'xyxy': array([     1315.4,      809.83,      2817.4,      1841.1])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1766.8,      1090.6,      3646.7,      2256.1])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.29445284605026245, 'xyxy': array([     1723.7,      1154.4,        3653,      2594.3])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.28525373339653015, 'xyxy': array([     1571.1,      732.79,      3266.2,      1542.8])}}, 89: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1804.8,      1090.7,      3587.1,      2188.6])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8595718145370483, 'xyxy': array([     1123.2,      937.95,      2492.9,      2241.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.40542852878570557, 'xyxy': array([    0.82331,      570.07,         733,      1511.4])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.2784174978733063, 'xyxy': array([     1315.4,      802.02,      2823.9,      1834.7])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1765.5,      1094.1,        3644,      2258.6])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.2633377015590668, 'xyxy': array([     1722.7,      1158.1,      3651.2,      2598.1])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.27503445744514465, 'xyxy': array([     1571.1,      732.73,      3266.6,      1542.8])}}, 90: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1809.9,      1094.3,      3576.8,      2182.7])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.864113450050354, 'xyxy': array([     1125.7,      938.07,      2490.3,      2241.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.40006113052368164, 'xyxy': array([    0.72865,      570.16,      732.94,      1511.4])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.29356545209884644, 'xyxy': array([     1313.9,       801.7,      2826.3,      1834.8])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1764.3,      1097.5,      3641.3,      2261.2])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1722.2,      1159.5,      3650.7,      2599.5])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2953231930732727, 'xyxy': array([     1570.7,      732.73,      3266.4,      1542.7])}}, 91: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([       1815,      1097.9,      3566.4,      2176.7])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8759588599205017, 'xyxy': array([     1122.9,      937.99,      2482.8,      2240.9])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4046110212802887, 'xyxy': array([     0.4953,      570.36,      732.84,      1511.7])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3785436749458313, 'xyxy': array([     1311.9,      804.15,      2826.8,        1837])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1763.1,        1101,      3638.5,      2263.7])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1721.6,      1160.8,      3650.1,      2600.8])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2506610155105591, 'xyxy': array([     1570.6,      732.62,      3266.5,      1542.6])}}, 92: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1820.1,      1101.4,      3556.1,      2170.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8799974918365479, 'xyxy': array([       1122,      937.12,      2478.3,      2240.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.4023171067237854, 'xyxy': array([     0.5037,      570.39,      732.96,      1511.7])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1314.9,      807.29,      2833.2,      1842.5])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1761.8,      1104.4,      3635.8,      2266.2])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1721,      1162.2,      3649.6,      2602.2])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.279476135969162, 'xyxy': array([     1570.5,      732.63,      3266.2,      1542.4])}}, 93: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1825.2,        1105,      3545.7,      2164.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8872366547584534, 'xyxy': array([     1120.1,       936.5,      2472.5,      2240.2])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.40325403213500977, 'xyxy': array([   -0.21691,      570.06,      732.09,      1511.3])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1317.9,      810.44,      2839.6,        1848])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1760.6,      1107.9,      3633.1,      2268.7])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1720.5,      1163.5,        3649,      2603.6])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.32637476921081543, 'xyxy': array([     1570.3,       732.9,        3266,      1542.6])}}, 94: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1830.3,      1108.6,      3535.4,      2158.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.886360228061676, 'xyxy': array([     1117.6,      936.47,      2466.4,      2240.6])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.41000860929489136, 'xyxy': array([   -0.42709,      569.68,      731.84,      1511.1])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1320.9,      813.59,        2846,      1853.4])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1759.3,      1111.3,      3630.4,      2271.3])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1719.9,      1164.9,      3648.5,      2604.9])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31828129291534424, 'xyxy': array([       1570,         733,      3265.8,      1542.7])}}, 95: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1835.4,      1112.1,        3525,      2152.9])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.888171911239624, 'xyxy': array([     1112.9,      936.94,      2458.4,      2241.7])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.40000343322753906, 'xyxy': array([   -0.32196,      569.34,      731.76,      1510.5])}, 'object4': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.26920270919799805, 'xyxy': array([     1313.6,      821.88,      2824.2,      1850.5])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1758.1,      1114.8,      3627.6,      2273.8])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1719.3,      1166.2,      3647.9,      2606.3])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.29145348072052, 'xyxy': array([     1570.2,      732.94,      3266.1,      1542.7])}}, 96: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1840.5,      1115.7,      3514.7,        2147])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8814164400100708, 'xyxy': array([     1107.7,      937.19,      2449.8,      2242.6])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3912138044834137, 'xyxy': array([   -0.23519,      569.21,      731.94,      1510.6])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3106034994125366, 'xyxy': array([     1312.3,      805.92,      2829.1,      1837.3])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1756.8,      1118.2,      3624.9,      2276.3])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1718.8,      1167.6,      3647.4,      2607.6])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.2,      732.98,      3266.1,      1542.7])}}, 97: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1845.6,      1119.3,      3504.3,        2141])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8525545597076416, 'xyxy': array([     1100.6,      937.01,      2439.8,      2244.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.39228498935699463, 'xyxy': array([  -0.084217,      568.87,      731.96,      1510.1])}, 'object4': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1314.1,      807.53,      2832.8,      1840.1])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1755.6,      1121.6,      3622.2,      2278.8])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1718.2,      1168.9,      3646.8,        2609])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.33074915409088135, 'xyxy': array([     1569.7,       733.5,      3265.8,      1543.4])}}, 98: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1850.7,      1122.8,        3494,      2135.1])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8462130427360535, 'xyxy': array([       1097,      938.08,      2430.2,      2244.4])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.38178551197052, 'xyxy': array([   -0.10592,      568.75,      731.92,      1509.9])}, 'object4': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.2886374294757843, 'xyxy': array([     1312.2,      837.65,      2826.5,      1866.5])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1754.4,      1125.1,      3619.5,      2281.4])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1717.6,      1170.3,      3646.3,      2610.4])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3132687509059906, 'xyxy': array([     1569.8,      733.63,        3266,      1543.6])}}, 99: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1855.8,      1126.4,      3483.6,      2129.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8580297827720642, 'xyxy': array([     1094.5,       936.7,      2422.1,      2242.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.36490485072135925, 'xyxy': array([    0.02999,      568.38,      732.05,      1509.5])}, 'object4': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1313.8,      842.24,      2829.1,      1871.7])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1753.1,      1128.5,      3616.7,      2283.9])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1717.1,      1171.6,      3645.7,      2611.7])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.32635927200317383, 'xyxy': array([     1569.4,       733.5,      3265.8,      1543.6])}}, 100: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1860.8,        1130,      3473.3,      2123.2])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8591225147247314, 'xyxy': array([     1092.9,      935.92,      2414.7,      2241.5])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.36141955852508545, 'xyxy': array([  -0.011539,      568.32,      731.98,      1509.4])}, 'object4': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1315.3,      846.83,      2831.7,        1877])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1751.9,        1132,        3614,      2286.4])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1716.5,        1173,      3645.2,      2613.1])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3254322111606598, 'xyxy': array([     1569.4,      733.45,      3265.9,      1543.6])}}, 101: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1865.9,      1133.5,      3462.9,      2117.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8726344704627991, 'xyxy': array([     1094.2,      932.88,        2411,      2238.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.34849312901496887, 'xyxy': array([ -0.0033831,      568.17,      731.99,      1509.3])}, 'object4': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.26726415753364563, 'xyxy': array([     1311.6,      854.78,      2826.7,      1883.4])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1750.6,      1135.4,      3611.3,      2288.9])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1715.9,      1174.3,      3644.6,      2614.4])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3167817294597626, 'xyxy': array([     1569.8,      733.51,      3266.2,      1543.6])}}, 102: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([       1871,      1137.1,      3452.6,      2111.3])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8767626285552979, 'xyxy': array([     1094.6,      929.89,      2407.6,      2235.6])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.340548574924469, 'xyxy': array([  -0.018892,      568.24,      731.89,      1509.3])}, 'object4': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1312.7,       859.7,      2828.4,      1888.8])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1749.4,      1138.9,      3608.6,      2291.5])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1715.4,      1175.6,      3644.1,      2615.8])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2798087000846863, 'xyxy': array([     1570.1,      733.42,      3266.7,      1543.7])}}, 103: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1876.1,      1140.7,      3442.2,      2105.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8854039907455444, 'xyxy': array([     1095.1,      926.82,      2401.9,      2229.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.33999088406562805, 'xyxy': array([   -0.12479,      568.06,      731.87,      1509.2])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': 0.30118557810783386, 'xyxy': array([     1355.1,      768.54,        2734,      1692.9])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1748.2,      1142.3,      3605.8,        2294])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1714.8,        1177,      3643.5,      2617.2])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.27691200375556946, 'xyxy': array([     1570.1,      733.35,      3266.9,      1543.6])}}, 104: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1881.2,      1144.2,      3431.9,      2099.4])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8768921494483948, 'xyxy': array([     1096.8,      923.63,      2396.6,      2223.1])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3463614284992218, 'xyxy': array([   -0.35845,      568.12,      731.77,      1509.6])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1361.4,      763.61,      2724.9,      1677.6])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1746.9,      1145.7,      3603.1,      2296.5])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.2507299780845642, 'xyxy': array([     1721.4,      1162.7,        3648,      2602.7])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2873729467391968, 'xyxy': array([     1570.4,      733.54,      3267.3,      1543.8])}}, 105: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1886.3,      1147.8,      3421.5,      2093.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8773452043533325, 'xyxy': array([       1093,      915.87,      2382.4,      2207.9])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.33099398016929626, 'xyxy': array([   -0.41312,      567.85,      731.74,      1509.5])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1367.7,      758.68,      2715.8,      1662.3])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1745.7,      1149.2,      3600.4,        2299])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1721.2,      1163.1,      3647.8,      2603.1])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3333410620689392, 'xyxy': array([     1570.5,      733.76,      3267.5,        1544])}}, 106: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1891.4,      1151.4,      3411.2,      2087.5])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8587027788162231, 'xyxy': array([     1090.1,       906.8,      2369.3,      2191.8])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.31373095512390137, 'xyxy': array([   -0.39213,       567.9,      731.79,      1509.7])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1374,      753.74,      2706.7,      1647.1])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1744.4,      1152.6,      3597.7,      2301.6])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1721,      1163.5,      3647.6,      2603.6])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3026750087738037, 'xyxy': array([     1570.4,      733.78,      3267.5,        1544])}}, 107: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1896.5,      1154.9,      3400.8,      2081.6])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.872454047203064, 'xyxy': array([     1086.1,      902.72,      2351.1,        2176])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.34929555654525757, 'xyxy': array([   -0.38613,      568.11,      731.71,      1509.9])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1380.3,      748.81,      2697.6,      1631.8])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1743.2,      1156.1,      3594.9,      2304.1])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1720.8,        1164,      3647.4,        2604])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31604963541030884, 'xyxy': array([     1570.2,      733.44,        3267,      1543.6])}}, 108: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1901.6,      1158.5,      3390.5,      2075.7])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8755906820297241, 'xyxy': array([       1078,      901.85,        2331,      2165.2])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3585565984249115, 'xyxy': array([   -0.19705,      568.15,      731.83,      1509.9])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1386.6,      743.87,      2688.5,      1616.6])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1741.9,      1159.5,      3592.2,      2306.6])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1720.6,      1164.4,      3647.2,      2604.4])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.33428651094436646, 'xyxy': array([       1570,      733.22,      3266.9,      1543.4])}}, 109: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1906.7,      1162.1,      3380.1,      2069.7])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8639832139015198, 'xyxy': array([       1072,      902.82,        2317,      2160.3])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.372185617685318, 'xyxy': array([   -0.34825,      568.93,      731.62,      1510.7])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1392.9,      738.94,      2679.4,      1601.3])}, 'object5': {'class': 'person', 'track_id': '19', 'confidence_score': None, 'xyxy': array([     1740.7,        1163,      3589.5,      2309.1])}, 'object6': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1720.4,      1164.8,        3647,      2604.9])}, 'object7': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31546467542648315, 'xyxy': array([     1569.3,      732.95,      3266.4,      1543.3])}}, 110: {'object1': {'class': 'person', 'track_id': '1', 'confidence_score': None, 'xyxy': array([     1911.8,      1165.7,      3369.8,      2063.8])}, 'object2': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8687649965286255, 'xyxy': array([     1063.1,         903,      2300.8,        2155])}, 'object3': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3371342420578003, 'xyxy': array([   -0.44298,      569.41,      731.27,        1511])}, 'object4': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1399.3,         734,      2670.3,        1586])}, 'object5': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1720.2,      1165.3,      3646.8,      2605.3])}, 'object6': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3173438012599945, 'xyxy': array([     1569.3,      732.82,      3266.3,      1543.2])}}, 111: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8520039916038513, 'xyxy': array([     1051.8,      903.27,      2285.7,      2153.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3389894366264343, 'xyxy': array([   -0.27143,      569.52,      731.07,      1510.6])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1405.6,      729.07,      2661.2,      1570.8])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.2582909166812897, 'xyxy': array([     1722.3,      1161.8,      3647.3,      2601.8])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3305268883705139, 'xyxy': array([     1569.2,      732.96,      3266.2,      1543.3])}}, 112: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8555947542190552, 'xyxy': array([     1047.4,      904.79,      2277.8,      2153.4])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.33708909153938293, 'xyxy': array([   -0.22212,      569.53,      730.96,      1510.5])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1411.9,      724.14,      2652.1,      1555.5])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3771086633205414, 'xyxy': array([     1722.9,      1160.9,      3646.8,      2600.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.308442622423172, 'xyxy': array([     1569.2,         733,      3266.3,      1543.5])}}, 113: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8629661202430725, 'xyxy': array([     1041.6,       901.2,      2267.2,      2146.8])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.29404282569885254, 'xyxy': array([   -0.21587,      569.81,      730.96,      1510.8])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1418.2,       719.2,        2643,      1540.3])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.40754130482673645, 'xyxy': array([     1723.3,      1160.7,      3646.5,      2600.7])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.33729690313339233, 'xyxy': array([     1569.1,      733.13,      3266.3,      1543.6])}}, 114: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8586195111274719, 'xyxy': array([       1031,      897.67,        2250,      2138.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.31017157435417175, 'xyxy': array([   -0.13391,      569.66,      730.91,      1510.5])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.2695835530757904, 'xyxy': array([     1305.1,      856.61,      2831.2,      1881.5])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.38218557834625244, 'xyxy': array([     1723.3,      1160.8,        3646,      2600.8])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3661642372608185, 'xyxy': array([     1569.3,      733.22,      3266.6,      1543.8])}}, 115: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8540780544281006, 'xyxy': array([     1019.7,       896.7,      2234.1,      2134.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.32557448744773865, 'xyxy': array([   -0.23479,      569.83,      730.69,      1510.6])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1302.9,      861.79,      2836.2,      1891.6])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.37666746973991394, 'xyxy': array([     1723.5,      1160.7,      3645.6,      2600.7])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3704257309436798, 'xyxy': array([     1569.6,      733.14,      3266.9,      1543.7])}}, 116: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.855402946472168, 'xyxy': array([       1012,      898.69,      2219.1,      2132.2])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.329935222864151, 'xyxy': array([   -0.24423,      569.93,      730.66,      1510.8])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1300.6,      866.97,      2841.2,      1901.6])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.35818225145339966, 'xyxy': array([     1723.8,      1160.7,      3645.6,      2600.7])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31729796528816223, 'xyxy': array([     1569.5,      732.89,      3266.9,      1543.5])}}, 117: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8509459495544434, 'xyxy': array([     996.79,      898.44,      2199.2,      2130.8])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.33909350633621216, 'xyxy': array([   -0.28865,      569.92,      730.64,      1510.9])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1298.3,      872.15,      2846.2,      1911.7])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.2968491017818451, 'xyxy': array([       1724,      1161.1,      3645.3,      2601.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3136543929576874, 'xyxy': array([     1569.9,      732.76,      3267.5,      1543.4])}}, 118: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8357191681861877, 'xyxy': array([     983.34,      895.83,      2182.6,      2129.3])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.34043657779693604, 'xyxy': array([   -0.55795,       570.3,      730.58,      1511.7])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1296,      877.32,      2851.2,      1921.8])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.29662877321243286, 'xyxy': array([     1724.2,      1161.5,      3645.2,      2601.5])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.34812337160110474, 'xyxy': array([       1570,      732.86,      3267.7,      1543.5])}}, 119: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8424013257026672, 'xyxy': array([      976.3,      894.74,      2171.9,      2128.3])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3428664803504944, 'xyxy': array([   -0.41528,      570.05,      730.27,        1511])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1293.8,       882.5,      2856.2,      1931.8])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.32302191853523254, 'xyxy': array([     1724.2,        1162,      3644.9,        2602])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31494489312171936, 'xyxy': array([     1569.4,      732.62,        3267,      1543.3])}}, 120: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8593382239341736, 'xyxy': array([     971.81,      894.39,      2163.1,      2127.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3184772729873657, 'xyxy': array([   -0.34444,      570.08,      730.44,      1511.2])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.4138942360877991, 'xyxy': array([     1303.2,      871.01,        2835,      1901.5])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3500719666481018, 'xyxy': array([     1723.8,      1162.2,      3644.1,      2602.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3525658845901489, 'xyxy': array([     1569.5,      732.86,      3266.8,      1543.4])}}, 121: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.839645504951477, 'xyxy': array([     972.29,      894.56,      2158.1,      2127.2])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3304018974304199, 'xyxy': array([   -0.56291,      570.42,      730.71,      1512.4])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.29567956924438477, 'xyxy': array([     1304.4,      861.48,      2833.4,      1891.4])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3560486137866974, 'xyxy': array([     1724.1,      1162.5,      3644.1,      2602.5])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.37490135431289673, 'xyxy': array([       1570,      732.77,      3266.8,        1543])}}, 122: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8336515426635742, 'xyxy': array([     975.42,      894.45,      2153.4,      2124.4])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3461051881313324, 'xyxy': array([   -0.72658,      570.67,      730.83,      1513.2])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1303,      864.12,      2836.2,      1896.8])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.3688087463378906, 'xyxy': array([     1724.5,      1162.9,      3644.4,      2602.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.33431708812713623, 'xyxy': array([       1570,      732.57,      3266.7,      1542.7])}}, 123: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8131893277168274, 'xyxy': array([     971.22,      893.51,      2140.1,      2119.4])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3374704420566559, 'xyxy': array([   -0.62195,      570.68,      730.65,        1513])}, 'object3': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1301.7,      866.75,      2838.9,      1902.1])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.29879119992256165, 'xyxy': array([     1724.8,        1163,      3644.6,        2603])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.337890625, 'xyxy': array([     1570.3,      732.44,      3267.1,      1542.6])}}, 124: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8083390593528748, 'xyxy': array([     969.94,      891.87,      2127.6,        2111])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.34467649459838867, 'xyxy': array([   -0.60581,      570.61,      730.62,        1513])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.3204612731933594, 'xyxy': array([       1306,      862.23,      2834.6,        1893])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1724.8,      1163.2,      3644.6,      2603.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.36317428946495056, 'xyxy': array([     1570.7,      732.55,      3267.4,      1542.6])}}, 125: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.773517906665802, 'xyxy': array([     966.64,      889.72,      2115.7,      2104.7])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3558083474636078, 'xyxy': array([   -0.48277,      570.98,       730.4,      1513.1])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.27031034231185913, 'xyxy': array([     1306.9,      863.86,      2833.5,      1894.1])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1724.8,      1163.4,      3644.5,      2603.4])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3878409266471863, 'xyxy': array([     1570.6,      732.54,      3267.4,      1542.6])}}, 126: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7938801646232605, 'xyxy': array([     959.76,      887.77,        2105,      2103.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3740405738353729, 'xyxy': array([   -0.24456,      571.01,      730.32,      1512.7])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.31902214884757996, 'xyxy': array([     1343.2,      774.63,      2652.1,      1631.6])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1724.8,      1163.5,      3644.5,      2603.5])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.44298288226127625, 'xyxy': array([     1570.6,      732.76,      3267.6,      1542.8])}}, 127: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8116477727890015, 'xyxy': array([     953.76,      886.15,      2095.8,      2104.3])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.36951303482055664, 'xyxy': array([  -0.073731,      570.77,      730.35,      1512.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.2651948928833008, 'xyxy': array([     1342.9,      739.31,      2576.9,      1531.4])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1724.8,      1163.7,      3644.5,      2603.7])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.4259718060493469, 'xyxy': array([     1570.6,      732.78,      3267.4,      1542.7])}}, 128: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8456700444221497, 'xyxy': array([     948.38,       885.5,      2085.4,      2103.4])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.34412524104118347, 'xyxy': array([   -0.18433,      570.56,      730.63,      1512.7])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.2853614389896393, 'xyxy': array([     1337.9,      727.87,      2557.3,        1498])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1724.8,      1163.9,      3644.5,      2603.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.41898754239082336, 'xyxy': array([     1570.6,      732.88,      3267.5,      1542.7])}}, 129: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8491284847259521, 'xyxy': array([     949.37,      886.85,      2079.6,      2100.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3131488561630249, 'xyxy': array([   -0.11573,      571.05,      730.83,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1344.8,      716.35,      2527.4,      1463.3])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1724.8,        1164,      3644.5,        2604])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.35440489649772644, 'xyxy': array([     1570.2,      732.71,      3267.3,      1542.6])}}, 130: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8192925453186035, 'xyxy': array([     950.99,      887.64,      2076.5,      2099.7])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3168272376060486, 'xyxy': array([   -0.08792,      571.23,      731.16,      1513.9])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': 0.25579243898391724, 'xyxy': array([     1364.3,      731.51,      2664.4,      1546.6])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.25787073373794556, 'xyxy': array([     1720.5,      1170.3,      3638.6,      2610.3])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2981491982936859, 'xyxy': array([       1570,      732.45,      3266.9,      1542.3])}}, 131: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.793461799621582, 'xyxy': array([     946.27,      888.59,      2068.7,      2100.1])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.2955431342124939, 'xyxy': array([  -0.043878,      571.29,      731.32,      1514.1])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1373.7,      723.75,      2657.3,      1528.5])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.25299689173698425, 'xyxy': array([     1756.4,      1064.3,      3689.8,      2504.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1570,      732.41,      3266.9,      1542.2])}}, 132: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8242340087890625, 'xyxy': array([     937.92,      890.08,      2058.1,      2102.1])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.2813604772090912, 'xyxy': array([   -0.14468,      571.54,      731.03,      1514.2])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1383.1,         716,      2650.3,      1510.5])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2702689468860626, 'xyxy': array([     1764.2,      1029.2,      3710.4,      2469.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.1,      732.38,      3266.8,      1542.2])}}, 133: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8654396533966064, 'xyxy': array([     920.44,      893.14,        2036,      2105.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.3021390736103058, 'xyxy': array([   -0.41175,      571.95,      730.54,      1514.5])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1392.5,      708.25,      2643.3,      1492.4])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2596572935581207, 'xyxy': array([     1764.3,      1016.4,      3720.6,      2456.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.1,      732.34,      3266.8,      1542.1])}}, 134: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8604755401611328, 'xyxy': array([     896.09,      894.01,      2008.9,      2110.8])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.27620676159858704, 'xyxy': array([   -0.62402,      572.08,      730.37,      1514.9])}, 'object3': {'class': 'person', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1401.9,      700.49,      2636.2,      1474.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.257994145154953, 'xyxy': array([     1761.6,      1011.6,      3726.9,      2451.4])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2634454071521759, 'xyxy': array([     1565.8,      736.13,      3260.1,      1545.5])}}, 135: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8170142769813538, 'xyxy': array([     874.74,      892.07,      1981.5,      2111.1])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.2865629196166992, 'xyxy': array([   -0.73151,      572.33,         730,        1515])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.443534255027771, 'xyxy': array([     1271.4,       804.2,      2862.6,      1812.5])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2521030902862549, 'xyxy': array([     1758.2,      1010.4,      3731.1,      2450.3])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1565.3,      736.46,      3259.5,      1545.8])}}, 136: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7855839133262634, 'xyxy': array([     852.12,      893.52,      1949.8,      2111.7])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.81847,      572.48,      729.96,      1515.2])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.5052724480628967, 'xyxy': array([     1269.1,      810.98,      2875.2,      1837.2])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1762.4,      999.82,      3735.3,      2439.7])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1564.8,      736.78,      3258.9,      1546.1])}}, 137: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8301398754119873, 'xyxy': array([     839.34,      891.86,      1925.9,      2106.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.2571103274822235, 'xyxy': array([   -0.52141,      572.31,      729.94,      1514.8])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': 0.2702350914478302, 'xyxy': array([     1270.3,      819.74,      2874.6,      1852.1])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1766.6,      989.27,      3739.5,      2429.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2545357644557953, 'xyxy': array([     1566.5,      736.12,      3259.5,      1545.5])}}, 138: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.848052442073822, 'xyxy': array([     831.11,      891.16,      1909.5,      2105.1])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.26503950357437134, 'xyxy': array([   -0.23672,      571.85,      729.88,      1513.9])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1265.1,      823.66,      2890.3,      1869.4])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.26006782054901123, 'xyxy': array([     1753.8,      1010.3,      3733.9,      2450.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1566.2,      736.35,      3259.1,      1545.7])}}, 139: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8394785523414612, 'xyxy': array([     828.32,      893.02,      1899.8,      2106.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': 0.2644484043121338, 'xyxy': array([   -0.19234,      571.68,      729.74,      1513.5])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1260,      827.57,      2905.9,      1886.7])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1756.7,        1003,      3736.7,      2442.7])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1565.9,      736.58,      3258.7,      1545.9])}}, 140: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8285282254219055, 'xyxy': array([     828.77,      893.47,      1894.8,      2107.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.20426,      571.71,      729.69,      1513.5])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1254.8,      831.49,      2921.6,        1904])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1759.5,      995.59,      3739.6,      2435.3])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2857591211795807, 'xyxy': array([     1570.6,      733.07,      3265.2,      1543.1])}}, 141: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8339290022850037, 'xyxy': array([     834.35,      891.13,      1891.4,      2100.8])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.21618,      571.74,      729.64,      1513.5])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1249.7,       835.4,      2937.3,      1921.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1762.4,      988.22,      3742.4,      2427.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3001728951931, 'xyxy': array([     1576.2,      735.67,      3268.9,      1543.6])}}, 142: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.27287134528160095, 'xyxy': array([     822.05,      889.04,      1865.1,      2093.5])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([    -0.2281,      571.77,      729.59,      1513.5])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1244.5,      839.32,        2953,      1938.6])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': 0.2572811245918274, 'xyxy': array([     1702.7,      1149.5,      3672.4,      2589.4])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3122597932815552, 'xyxy': array([     1576.2,       735.9,      3268.9,      1543.2])}}, 143: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7596392631530762, 'xyxy': array([     814.57,      890.32,      1844.9,      2089.7])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.24002,      571.81,      729.54,      1513.5])}, 'object3': {'class': 'boat', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1239.4,      843.23,      2968.7,        1956])}, 'object4': {'class': 'bicycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1699.2,      1157.9,      3668.9,      2597.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3258487284183502, 'xyxy': array([     1576.1,      736.71,      3269.8,      1543.8])}}, 144: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7672507762908936, 'xyxy': array([     799.84,      885.92,      1815.3,      2078.3])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.25194,      571.84,       729.5,      1513.5])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.4041057527065277, 'xyxy': array([     1326.8,      734.88,      2579.1,        1523])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.4052540361881256, 'xyxy': array([     1742.8,      1047.4,      3720.5,      2486.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1576.9,      736.95,      3270.1,      1543.8])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2634052634239197, 'xyxy': array([     1191.3,      750.47,      2873.4,      1712.7])}}, 145: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7547175884246826, 'xyxy': array([     786.52,      882.43,      1788.8,      2068.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.26386,      571.87,      729.45,      1513.4])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.34706756472587585, 'xyxy': array([     1325.3,      727.27,      2562.7,        1493])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.29626160860061646, 'xyxy': array([     1749.3,      1025.7,      3733.9,      2465.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2523226737976074, 'xyxy': array([     1573.8,      739.85,      3264.5,      1545.4])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([       1168,      757.22,        2947,      1774.9])}}, 146: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7845410704612732, 'xyxy': array([      777.9,      879.85,      1773.7,      2067.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.27577,       571.9,       729.4,      1513.4])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.4047594964504242, 'xyxy': array([     1318.8,      725.04,      2562.5,      1484.8])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3244205713272095, 'xyxy': array([     1749.7,      1018.1,      3740.3,      2457.8])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1574.2,      740.37,      3264.2,      1545.6])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1144.6,      763.97,      3020.7,      1837.1])}}, 147: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8087498545646667, 'xyxy': array([     772.25,      878.94,      1761.7,      2067.8])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.28769,      571.93,      729.35,      1513.4])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.419063001871109, 'xyxy': array([     1312.5,      724.38,      2568.3,      1484.1])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2638412415981293, 'xyxy': array([     1749.7,      1015.8,      3745.2,      2455.6])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.30391010642051697, 'xyxy': array([       1575,      737.33,      3268.8,      1543.7])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1121.3,      770.72,      3094.3,      1899.4])}}, 148: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8248971104621887, 'xyxy': array([     760.95,      880.02,      1742.6,      2066.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.29961,      571.97,       729.3,      1513.4])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.47105881571769714, 'xyxy': array([     1306.8,      724.02,      2572.5,      1483.5])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.31539386510849, 'xyxy': array([     1746.8,      1015.1,      3746.3,      2454.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3166867792606354, 'xyxy': array([     1574.6,      736.39,      3269.9,        1543])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1097.9,      777.47,        3168,      1961.6])}}, 149: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.844961404800415, 'xyxy': array([      752.7,      884.05,      1727.1,      2068.8])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.31153,         572,      729.25,      1513.4])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.42389580607414246, 'xyxy': array([     1304.6,      723.93,      2577.3,      1482.2])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.4194157123565674, 'xyxy': array([       1744,      1014.7,      3746.6,      2454.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.27404752373695374, 'xyxy': array([     1572.7,      739.41,      3266.1,      1545.1])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1074.6,      784.22,      3241.6,      2023.8])}}, 150: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8171380162239075, 'xyxy': array([     744.08,      884.07,      1711.6,      2067.5])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.32345,      572.03,      729.21,      1513.4])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.44031625986099243, 'xyxy': array([     1300.5,      724.15,      2580.5,      1482.2])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.31800904870033264, 'xyxy': array([     1742.5,      1015.4,      3748.5,      2455.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2706321179866791, 'xyxy': array([       1572,      740.59,      3264.7,        1546])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1051.2,      790.96,      3315.3,      2086.1])}}, 151: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8060168027877808, 'xyxy': array([     737.66,      881.76,      1699.1,      2064.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.33537,      572.06,      729.16,      1513.4])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.4116895794868469, 'xyxy': array([     1295.1,      724.38,      2584.9,      1484.6])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.38576164841651917, 'xyxy': array([     1740.6,      1015.6,      3749.3,      2455.5])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2732471823692322, 'xyxy': array([       1573,      737.63,      3268.3,      1543.9])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1027.9,      797.71,      3388.9,      2148.3])}}, 152: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8009248375892639, 'xyxy': array([     731.88,      880.33,      1685.1,        2061])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.34729,       572.1,      729.11,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.3846827745437622, 'xyxy': array([     1291.7,      724.76,      2588.8,      1486.1])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3591492176055908, 'xyxy': array([     1739.2,      1015.9,      3750.3,      2455.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.26558423042297363, 'xyxy': array([     1571.5,      740.11,      3265.4,      1545.8])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1004.6,      804.46,      3462.6,      2210.5])}}, 153: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8267976641654968, 'xyxy': array([      725.5,      876.64,      1670.2,      2054.5])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.35921,      572.13,      729.06,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.40169191360473633, 'xyxy': array([     1289.1,      724.81,      2591.8,      1486.7])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3471285402774811, 'xyxy': array([     1738.2,      1016.2,      3751.4,      2456.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2590295374393463, 'xyxy': array([     1571.1,      740.93,      3264.4,      1546.4])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([      981.2,      811.21,      3536.2,      2272.8])}}, 154: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7613665461540222, 'xyxy': array([     711.47,      874.55,      1648.2,      2053.5])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.37113,      572.16,      729.01,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.43459323048591614, 'xyxy': array([     1287.5,      725.47,      2593.1,      1486.5])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3632343113422394, 'xyxy': array([       1737,      1015.9,      3751.7,      2455.7])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2581256031990051, 'xyxy': array([     1570.5,      741.22,      3263.7,      1546.8])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     957.86,      817.96,      3609.9,        2335])}}, 155: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7777445912361145, 'xyxy': array([     713.14,      873.42,      1643.9,      2052.7])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.38305,      572.19,      728.96,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.3950578272342682, 'xyxy': array([     1285.8,      725.79,      2595.2,      1486.7])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3387277126312256, 'xyxy': array([     1736.3,      1015.6,      3752.5,      2455.2])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.26100024580955505, 'xyxy': array([     1572.2,       737.9,      3267.9,      1544.4])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     934.51,      824.71,      3683.5,      2397.2])}}, 156: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7531852126121521, 'xyxy': array([     708.73,      871.51,      1632.7,      2051.5])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.39497,      572.22,      728.92,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.3596871495246887, 'xyxy': array([     1282.4,      725.22,        2597,      1487.5])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.40800032019615173, 'xyxy': array([     1734.9,      1015.7,      3752.3,      2455.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1572.3,      737.87,        3268,      1544.4])}, 'object6': {'class': 'person', 'track_id': '31', 'confidence_score': 0.3039812445640564, 'xyxy': array([     1288.3,      735.63,      2798.5,        1605])}}, 157: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8227233290672302, 'xyxy': array([     702.81,      866.74,      1619.6,        2048])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.40689,      572.26,      728.87,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.33666789531707764, 'xyxy': array([     1282.5,      725.03,        2598,        1486])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.33173149824142456, 'xyxy': array([     1734.5,      1015.9,      3752.9,        2455])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1572.4,      737.84,      3268.1,      1544.3])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2523561418056488, 'xyxy': array([     1212.1,      804.04,      2924.2,      1809.7])}}, 158: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8248351216316223, 'xyxy': array([     702.26,      864.54,      1610.5,      2044.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.41881,      572.29,      728.82,      1513.3])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.36510026454925537, 'xyxy': array([     1281.3,      725.37,      2600.3,      1486.8])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.36266955733299255, 'xyxy': array([     1733.9,      1015.7,      3752.7,      2454.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2573946714401245, 'xyxy': array([     1569.9,      740.99,      3263.7,      1546.8])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1202.6,      812.21,      2950.8,      1839.1])}}, 159: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8211461901664734, 'xyxy': array([     703.48,      862.31,      1601.7,      2038.3])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.43073,      572.32,      728.77,      1513.2])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.2985379099845886, 'xyxy': array([     1280.2,         725,      2600.6,      1485.7])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.4117530882358551, 'xyxy': array([     1733.3,      1015.5,      3753.1,      2453.6])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.26065605878829956, 'xyxy': array([     1569.7,      741.39,      3262.9,      1547.2])}, 'object6': {'class': 'person', 'track_id': '31', 'confidence_score': 0.35197997093200684, 'xyxy': array([     1298.6,      743.86,      2800.3,      1627.9])}}, 160: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7907639145851135, 'xyxy': array([     704.94,      859.47,      1594.8,      2032.5])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.44265,      572.35,      728.72,      1513.2])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.35557931661605835, 'xyxy': array([     1279.5,      724.54,      2601.5,      1484.8])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.35335779190063477, 'xyxy': array([     1732.8,      1015.5,        3754,      2453.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2824108600616455, 'xyxy': array([     1571.6,      737.22,      3267.2,      1543.8])}, 'object6': {'class': 'person', 'track_id': '31', 'confidence_score': None, 'xyxy': array([       1304,         741,      2801.6,      1622.7])}}, 161: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7872939705848694, 'xyxy': array([     706.63,      847.16,      1588.1,      2016.6])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.45457,      572.39,      728.67,      1513.2])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.28520530462265015, 'xyxy': array([     1278.2,      723.65,      2604.8,      1485.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.34786394238471985, 'xyxy': array([     1732.9,      1015.6,      3755.8,      2454.6])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1571.7,      737.04,      3267.3,      1543.7])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2947995066642761, 'xyxy': array([     1234.4,      794.75,      2911.3,      1798.7])}}, 162: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8005589246749878, 'xyxy': array([     705.16,      842.95,      1573.9,      2001.8])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.46649,      572.42,      728.63,      1513.2])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1278.7,      722.94,      2602.9,      1483.2])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3173593282699585, 'xyxy': array([     1732.5,        1016,      3756.1,        2455])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1571.8,      736.85,      3267.4,      1543.5])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2920730710029602, 'xyxy': array([     1229.7,      800.29,      2921.4,      1827.5])}}, 163: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8189666867256165, 'xyxy': array([     701.94,       847.2,      1561.5,      1999.4])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.47841,      572.45,      728.58,      1513.2])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.3067178428173065, 'xyxy': array([     1279.1,      723.93,      2602.5,      1482.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3457355499267578, 'xyxy': array([     1731.6,      1016.3,      3755.6,      2455.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1571.9,      736.67,      3267.5,      1543.3])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.28931012749671936, 'xyxy': array([     1233.1,      804.48,      2917.5,      1839.9])}}, 164: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8055402040481567, 'xyxy': array([     700.68,      857.09,      1553.3,      2005.3])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.49033,      572.48,      728.53,      1513.2])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1279.7,       723.4,      2600.6,      1480.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3153928220272064, 'xyxy': array([       1732,      1016.6,      3757.2,      2455.8])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1571.9,      736.48,      3267.6,      1543.2])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2664823532104492, 'xyxy': array([     1239.8,      806.86,        2911,      1844.9])}}, 165: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.793129563331604, 'xyxy': array([     699.05,      856.05,      1543.7,      1997.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.50225,      572.51,      728.48,      1513.2])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1280.4,      722.86,      2598.7,      1478.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3352658152580261, 'xyxy': array([     1731.7,      1016.7,      3757.5,      2455.9])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1572,       736.3,      3267.7,        1543])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.27806416153907776, 'xyxy': array([     1247.2,      810.91,        2904,      1849.3])}}, 166: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8059884309768677, 'xyxy': array([     698.08,      856.19,      1538.2,      1995.9])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.51417,      572.55,      728.43,      1513.1])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.327623575925827, 'xyxy': array([       1278,      724.69,      2604.1,      1483.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3075689971446991, 'xyxy': array([     1731.9,      1016.6,      3758.4,        2456])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.31072908639907837, 'xyxy': array([     1572.7,      737.61,        3270,      1544.8])}, 'object6': {'class': 'umbrella', 'track_id': '31', 'confidence_score': 0.2595607042312622, 'xyxy': array([     1305.3,      752.65,      2699.5,      1613.5])}}, 167: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8229321241378784, 'xyxy': array([     694.23,      854.25,      1531.6,      1993.4])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.52609,      572.58,      728.38,      1513.1])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.30147528648376465, 'xyxy': array([       1276,       725.3,        2606,      1485.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3325226604938507, 'xyxy': array([     1731.4,      1016.6,      3758.4,        2456])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2856791317462921, 'xyxy': array([     1572.8,      737.44,      3270.8,      1544.7])}, 'object6': {'class': 'umbrella', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1307.2,      750.54,        2688,        1603])}}, 168: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8283445239067078, 'xyxy': array([     691.08,      849.48,      1526.3,      1988.7])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.53801,      572.61,      728.34,      1513.1])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.2579340636730194, 'xyxy': array([     1275.8,      725.66,      2606.8,      1485.3])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3597027659416199, 'xyxy': array([     1730.7,      1016.5,      3758.2,        2456])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.27350616455078125, 'xyxy': array([     1572.4,      737.11,        3271,      1544.4])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.28674644231796265, 'xyxy': array([     1259.3,      801.04,      2858.4,      1798.1])}}, 169: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8444233536720276, 'xyxy': array([     686.49,      847.67,      1519.3,      1986.1])}, 'object2': {'class': 'train', 'track_id': '3', 'confidence_score': None, 'xyxy': array([   -0.54993,      572.64,      728.29,      1513.1])}, 'object3': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1276.1,      725.52,      2605.9,      1484.5])}, 'object4': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.39859136939048767, 'xyxy': array([       1730,      1016.2,      3757.1,      2455.1])}, 'object5': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1572.6,      737.01,      3271.3,      1544.4])}, 'object6': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3482528030872345, 'xyxy': array([     1257.1,      808.65,      2884.9,      1831.9])}}, 170: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.8079754710197449, 'xyxy': array([     679.85,      848.47,      1510.7,      1986.4])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.284505158662796, 'xyxy': array([     1276.4,      726.34,      2607.4,      1484.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.37956544756889343, 'xyxy': array([     1729.7,      1015.9,      3757.7,      2455.1])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.28314217925071716, 'xyxy': array([       1572,      738.67,      3270.2,      1545.7])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.26337045431137085, 'xyxy': array([     1259.5,      816.06,      2890.9,      1849.2])}}, 171: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7460402846336365, 'xyxy': array([     670.52,      850.39,      1498.5,      1987.1])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.30073195695877075, 'xyxy': array([     1276.5,      726.25,      2607.8,        1484])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.34842947125434875, 'xyxy': array([     1730.2,      1015.9,      3758.4,      2455.1])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.2535127103328705, 'xyxy': array([     1568.8,      740.88,      3263.8,      1546.9])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1254.2,      821.25,      2905.3,      1866.9])}}, 172: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7837527394294739, 'xyxy': array([      658.8,      855.61,      1480.5,      1987.9])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.2854200005531311, 'xyxy': array([     1275.3,      726.65,      2608.3,      1484.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3155137002468109, 'xyxy': array([     1730.1,      1016.1,      3758.4,      2455.1])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.30675414204597473, 'xyxy': array([       1571,      739.81,      3267.1,      1546.3])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2908262610435486, 'xyxy': array([     1264.5,      820.35,      2888.4,      1856.1])}}, 173: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7581015825271606, 'xyxy': array([     645.94,      857.58,      1462.2,      1987.9])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.3026335537433624, 'xyxy': array([     1274.5,      726.81,      2609.8,      1485.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.307197242975235, 'xyxy': array([     1729.8,      1016.2,      3758.5,      2455.2])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.28412771224975586, 'xyxy': array([     1572.2,      739.01,      3268.8,      1545.5])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1260.2,      824.94,      2900.2,        1871])}}, 174: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7576361298561096, 'xyxy': array([      633.2,      861.44,      1445.1,      1993.3])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.31407520174980164, 'xyxy': array([     1274.1,      726.84,      2610.3,      1485.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2571253478527069, 'xyxy': array([     1731.5,      1016.2,        3761,      2455.7])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3413669764995575, 'xyxy': array([     1572.6,      739.13,      3269.2,      1545.6])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1255.9,      829.53,      2912.1,      1885.9])}}, 175: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6568236947059631, 'xyxy': array([     624.83,      860.11,      1431.2,      1991.6])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.29142042994499207, 'xyxy': array([       1273,      726.78,        2611,        1486])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.35181891918182373, 'xyxy': array([     1730.6,      1016.1,      3759.7,        2455])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.3032304644584656, 'xyxy': array([     1572.4,      737.86,      3269.4,      1544.3])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1251.6,      834.12,      2923.9,      1900.8])}}, 176: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5936446189880371, 'xyxy': array([     605.45,       856.3,      1406.5,      1987.8])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.27241888642311096, 'xyxy': array([     1271.9,      726.78,      2610.8,      1486.2])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3615896999835968, 'xyxy': array([     1732.5,        1016,      3760.6,      2453.8])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.33407050371170044, 'xyxy': array([     1571.7,         737,      3269.2,      1543.6])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.25640246272087097, 'xyxy': array([     1272.4,      822.43,      2886.9,      1859.4])}}, 177: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6337359547615051, 'xyxy': array([     569.54,       851.4,      1364.5,      1981.2])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.2631314992904663, 'xyxy': array([     1271.7,      726.96,        2611,      1486.3])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3047518730163574, 'xyxy': array([     1730.7,        1016,      3758.6,      2453.3])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.327850878238678, 'xyxy': array([     1572.1,      737.93,      3269.8,      1544.6])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1269.9,      825.56,      2894.9,      1869.2])}}, 178: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6370808482170105, 'xyxy': array([     569.93,      848.18,      1359.5,      1976.4])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.2843457758426666, 'xyxy': array([     1271.6,      726.91,      2611.7,      1486.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3096632957458496, 'xyxy': array([     1729.8,        1016,      3758.1,      2453.5])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.29703086614608765, 'xyxy': array([     1572.3,      739.07,      3269.9,      1545.5])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2618452310562134, 'xyxy': array([     1277.5,      833.04,      2880.1,      1868.9])}}, 179: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6363482475280762, 'xyxy': array([     563.88,      844.03,      1348.6,      1971.1])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': 0.28412771224975586, 'xyxy': array([     1270.9,      726.72,      2612.3,      1486.7])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.4067316949367523, 'xyxy': array([     1729.1,        1016,      3757.5,      2453.2])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1572.4,      739.15,        3270,      1545.6])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1275.6,       836.6,      2886.3,      1877.7])}}, 180: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6234100461006165, 'xyxy': array([     572.18,      848.16,      1352.9,      1975.2])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1270.9,      726.72,      2612.1,      1486.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.36852067708969116, 'xyxy': array([     1728.8,      1016.1,      3757.9,      2453.8])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1572.6,      739.23,      3270.1,      1545.6])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1273.6,      840.16,      2892.4,      1886.6])}}, 181: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6639697551727295, 'xyxy': array([      604.5,      843.66,      1379.4,      1967.1])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1270.9,      726.72,      2611.9,      1486.5])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.5540333986282349, 'xyxy': array([     1728.4,      1015.9,      3756.8,      2452.7])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1572.7,      739.32,      3270.2,      1545.7])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1271.6,      843.72,      2898.6,      1895.4])}}, 182: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6460922956466675, 'xyxy': array([     616.26,      833.68,      1380.9,      1947.3])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1270.9,      726.72,      2611.8,      1486.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.40176549553871155, 'xyxy': array([     1728.4,      1016.2,      3756.9,      2452.9])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.25831595063209534, 'xyxy': array([     1570.5,      742.11,      3265.4,      1547.5])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.31561028957366943, 'xyxy': array([     1313.6,      809.08,      2894.9,      1830.9])}}, 183: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6687704920768738, 'xyxy': array([     616.82,      826.74,      1374.3,      1935.6])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1270.9,      726.72,      2611.6,      1486.3])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3587570786476135, 'xyxy': array([     1729.4,      1016.3,      3758.4,      2453.1])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.4,      742.45,        3265,      1547.7])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.4100978970527649, 'xyxy': array([     1322.8,      805.45,      2898.3,      1822.3])}}, 184: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6473991870880127, 'xyxy': array([     618.39,      824.79,      1366.4,      1924.3])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1270.9,      726.71,      2611.5,      1486.2])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.31650376319885254, 'xyxy': array([       1730,      1016.6,      3759.1,      2453.3])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.3,      742.79,      3264.7,      1547.9])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.29260241985321045, 'xyxy': array([     1318.1,       817.8,      2881.1,      1828.8])}}, 185: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6950753927230835, 'xyxy': array([     617.08,      825.28,      1361.1,      1922.9])}, 'object2': {'class': 'umbrella', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1270.9,      726.71,      2611.3,      1486.1])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.2,      1016.6,      3759.1,      2453.2])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.2,      743.13,      3264.3,      1548.2])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.4158335328102112, 'xyxy': array([     1324.4,       817.2,      2882.2,      1825.8])}}, 186: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7469438314437866, 'xyxy': array([     615.67,      824.73,      1355.9,      1920.5])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.2689063549041748, 'xyxy': array([     1501.5,      804.26,      3103.4,      1712.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.4,      1016.5,      3759.1,        2453])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1570.1,      743.47,        3264,      1548.4])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.45743662118911743, 'xyxy': array([     1323.8,         811,      2878.4,        1818])}}, 187: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7447863817214966, 'xyxy': array([     614.05,      825.03,      1350.8,      1918.6])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1520.2,       810.6,      3143.5,      1731.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2565937936306, 'xyxy': array([     1730.6,      1016.7,      3759.9,      2453.4])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1570,      743.81,      3263.6,      1548.6])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.2730858623981476, 'xyxy': array([     1334.4,      801.81,      2885.3,      1805.3])}}, 188: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7017284631729126, 'xyxy': array([     611.13,      824.74,      1347.4,      1920.7])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1539,      816.94,      3183.6,      1749.8])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.8,      1016.7,        3760,      2453.3])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.28125810623168945, 'xyxy': array([     1565.5,      741.73,      3262.7,        1549])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1337.4,       800.8,      2887.7,      1803.9])}}, 189: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6654963493347168, 'xyxy': array([     608.04,      830.47,      1340.3,      1924.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1557.8,      823.27,      3223.8,      1768.3])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1731,      1016.7,      3760.1,      2453.2])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1564.9,      741.87,      3262.3,      1549.2])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1340.5,      799.79,        2890,      1802.4])}}, 190: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6729214787483215, 'xyxy': array([     606.57,      831.78,        1337,      1925.4])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1576.5,      829.61,      3263.9,      1786.7])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.32293665409088135, 'xyxy': array([     1732.2,      1017.1,      3762.3,        2454])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1564.4,         742,        3262,      1549.5])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.4006898105144501, 'xyxy': array([     1383.2,      799.72,      2924.9,      1791.7])}}, 191: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6363751888275146, 'xyxy': array([     606.57,      830.75,      1333.5,      1921.9])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1595.3,      835.94,        3304,      1805.2])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1732.5,      1017.1,      3762.5,        2454])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': 0.32298019528388977, 'xyxy': array([     1566.8,      741.01,      3261.1,      1547.6])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1390.8,       798.8,      2930.1,      1789.3])}}, 192: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6766626834869385, 'xyxy': array([     598.45,      830.64,      1324.6,      1923.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1614.1,      842.28,      3344.2,      1823.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.39315858483314514, 'xyxy': array([     1732.1,      1017.3,      3762.9,      2454.4])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1566.4,      741.04,      3260.7,      1547.6])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.32419735193252563, 'xyxy': array([     1415.3,      785.88,      2956.4,      1769.9])}}, 193: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7074602246284485, 'xyxy': array([     593.86,      828.13,      1321.8,      1925.6])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1632.8,      848.62,      3384.3,        1842])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.28972914814949036, 'xyxy': array([     1730.9,      1016.5,      3762.9,      2454.4])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1566.1,      741.06,      3260.4,      1547.6])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1425.3,       783.7,      2963.2,      1765.7])}}, 194: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6933348178863525, 'xyxy': array([     588.72,      828.97,      1316.6,      1928.5])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1651.6,      854.95,      3424.5,      1860.5])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.295774906873703, 'xyxy': array([     1730.1,      1015.9,      3762.7,      2454.3])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1565.8,      741.09,      3260.1,      1547.7])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1435.3,      781.52,      2970.1,      1761.5])}}, 195: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7314469814300537, 'xyxy': array([     585.95,      832.08,      1311.9,      1931.2])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1670.4,      861.29,      3464.6,      1878.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.410993367433548, 'xyxy': array([     1731.5,      1016.3,      3763.8,      2454.1])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1565.4,      741.12,      3259.7,      1547.7])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1445.3,      779.35,      2976.9,      1757.3])}}, 196: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6435344219207764, 'xyxy': array([     572.17,      823.39,      1296.3,        1923])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1689.1,      867.62,      3504.7,      1897.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2903355360031128, 'xyxy': array([     1730.7,      1015.8,        3763,      2453.7])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1565.1,      741.15,      3259.4,      1547.7])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.38677477836608887, 'xyxy': array([     1462.4,       774.7,      2989.5,      1739.8])}}, 197: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6194969415664673, 'xyxy': array([     568.05,      817.66,      1290.5,      1919.1])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1707.9,      873.96,      3544.9,      1915.8])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.7,      1015.7,      3763.1,      2453.6])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1564.7,      741.17,      3259.1,      1547.8])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1473.9,      772.29,      2996.1,      1734.3])}}, 198: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5839844942092896, 'xyxy': array([     558.62,      819.91,        1277,      1920.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': 0.28757941722869873, 'xyxy': array([     1425.3,      772.16,      3101.5,      1729.7])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2856343984603882, 'xyxy': array([     1730.3,      1015.5,      3762.2,        2453])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1564.4,       741.2,      3258.8,      1547.8])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.6122562289237976, 'xyxy': array([       1484,      812.53,      3009.6,      1766.6])}}, 199: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5734034180641174, 'xyxy': array([     562.16,      824.11,      1276.6,      1922.8])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1425.1,      771.83,      3112.2,      1735.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.3,      1015.3,      3762.2,      2452.9])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1564.1,      741.23,      3258.4,      1547.8])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1496.2,      814.57,      3016.1,      1765.1])}}, 200: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5197986364364624, 'xyxy': array([     536.35,      824.14,      1246.7,      1920.6])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1425,       771.5,      3122.9,      1741.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.3,      1015.2,      3762.3,      2452.8])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1563.7,      741.26,      3258.1,      1547.8])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1508.3,      816.61,      3022.6,      1763.6])}}, 201: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5901668071746826, 'xyxy': array([     519.96,      822.15,      1226.7,      1917.2])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1424.8,      771.17,      3133.5,      1747.2])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.4,      1015.1,      3762.3,      2452.7])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1563.4,      741.28,      3257.8,      1547.9])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2915206551551819, 'xyxy': array([     1511.2,      816.71,      3039.6,      1760.5])}}, 202: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5067878365516663, 'xyxy': array([     514.14,      827.83,      1205.9,      1904.7])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1424.7,      770.84,      3144.2,      1753.1])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.4,        1015,      3762.3,      2452.5])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1563.1,      741.31,      3257.5,      1547.9])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.261312335729599, 'xyxy': array([       1515,      819.71,      3049.2,      1756.1])}}, 203: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5847830176353455, 'xyxy': array([     512.47,      826.63,        1202,      1906.2])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1424.5,      770.51,      3154.9,      1758.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.4,      1014.9,      3762.3,      2452.4])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1562.7,      741.34,      3257.1,      1547.9])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1527.3,      821.67,        3055,      1754.1])}}, 204: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.43558648228645325, 'xyxy': array([     509.45,         823,      1191.8,      1897.5])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1424.4,      770.18,      3165.6,      1764.7])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2594124376773834, 'xyxy': array([       1730,      1015.9,      3762.4,      2453.6])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1562.4,      741.36,      3256.8,        1548])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1539.7,      823.64,      3060.9,      1752.1])}}, 205: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.470182865858078, 'xyxy': array([     518.37,      813.13,      1204.1,      1894.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1424.2,      769.85,      3176.2,      1770.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1730,      1015.8,      3762.5,      2453.6])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1562.1,      741.39,      3256.5,        1548])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([       1552,      825.61,      3066.7,      1750.1])}}, 206: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6112223267555237, 'xyxy': array([     517.77,      813.67,      1199.8,      1892.5])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1424.1,      769.51,      3186.9,      1776.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.32045215368270874, 'xyxy': array([     1730.2,      1016.3,      3761.6,      2453.1])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1561.7,      741.42,      3256.1,        1548])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.4391687512397766, 'xyxy': array([     1566.8,      742.48,      3107.3,      1665.5])}}, 207: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6045993566513062, 'xyxy': array([     515.76,         811,      1193.7,      1887.4])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1423.9,      769.18,      3197.6,      1782.2])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.35263314843177795, 'xyxy': array([     1730.1,      1016.4,      3761.2,        2453])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1561.4,      741.45,      3255.8,      1548.1])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1580.8,      736.12,      3115.1,      1655.4])}}, 208: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6111091375350952, 'xyxy': array([     512.37,      814.66,      1184.1,      1885.6])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1423.8,      768.85,      3208.2,      1788.1])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.1,      1016.5,      3761.1,      2452.9])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1561,      741.47,      3255.5,      1548.1])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1594.8,      729.76,        3123,      1645.3])}}, 209: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5087568163871765, 'xyxy': array([     505.75,      812.27,      1172.4,      1880.9])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1423.6,      768.52,      3218.9,      1793.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.1,      1016.5,        3761,      2452.8])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1560.7,       741.5,      3255.2,      1548.1])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2516234517097473, 'xyxy': array([     1599.6,      716.57,      3148.6,      1626.9])}}, 210: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.7036237716674805, 'xyxy': array([     504.54,      807.64,      1167.4,      1875.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1423.5,      768.19,      3229.6,      1799.7])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.34097447991371155, 'xyxy': array([     1730.4,      1015.5,        3763,        2453])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1560.4,      741.53,      3254.8,      1548.2])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.4845687448978424, 'xyxy': array([       1595,      726.49,      3164.3,      1635.5])}}, 211: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5939533114433289, 'xyxy': array([     502.53,      809.04,      1160.9,        1875])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1423.3,      767.86,      3240.2,      1805.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.30414703488349915, 'xyxy': array([     1730.8,      1015.3,      3763.6,      2452.8])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1560,      741.56,      3254.5,      1548.2])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1608.8,      721.28,      3171.9,      1626.7])}}, 212: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5796123743057251, 'xyxy': array([     499.01,      809.68,      1157.2,      1881.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1423.2,      767.53,      3250.9,      1811.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.3148893713951111, 'xyxy': array([     1730.6,      1015.7,      3763.8,      2453.6])}, 'object4': {'class': 'umbrella', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1559.7,      741.58,      3254.2,      1548.2])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1622.6,      716.08,      3179.5,      1617.9])}}, 213: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5897049307823181, 'xyxy': array([     503.62,      822.44,      1162.3,      1899.8])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1423,       767.2,      3261.6,      1817.2])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.31521597504615784, 'xyxy': array([     1730.5,      1015.7,      3764.1,      2453.8])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': 0.27136123180389404, 'xyxy': array([       1677,      651.72,      3228.8,      1369.5])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.32877394556999207, 'xyxy': array([       1621,      721.53,      3202.4,        1623])}}, 214: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2513832449913025, 'xyxy': array([      531.1,      826.24,      1198.9,      1908.9])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1422.9,      766.87,      3272.3,      1823.1])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.39062315225601196, 'xyxy': array([     1730.1,      1015.5,      3763.7,      2453.6])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1683.3,      647.58,      3226.2,      1361.2])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.3742953836917877, 'xyxy': array([     1623.7,      717.71,      3221.5,        1616])}}, 215: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.47648152709007263, 'xyxy': array([     538.82,      836.57,      1211.9,      1923.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1422.7,      766.54,      3282.9,      1828.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.27958622574806213, 'xyxy': array([       1730,      1015.9,      3763.4,      2453.8])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1689.6,      643.43,      3223.5,      1352.9])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1637.3,      713.63,      3229.4,      1608.7])}}, 216: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5900638103485107, 'xyxy': array([     521.58,      831.54,      1192.5,      1920.1])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1422.6,      766.21,      3293.6,      1834.8])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1730,      1015.9,      3763.5,      2453.8])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1695.8,      639.29,      3220.9,      1344.7])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1650.9,      709.55,      3237.2,      1601.4])}}, 217: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5541254878044128, 'xyxy': array([     510.64,      845.21,      1178.8,      1934.6])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1422.4,      765.88,      3304.3,      1840.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.4315923750400543, 'xyxy': array([       1731,      1016.6,      3765.2,      2454.9])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1702.1,      635.15,      3218.3,      1336.4])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.519263505935669, 'xyxy': array([     1647.1,      708.37,      3248.5,        1595])}}, 218: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6088827252388, 'xyxy': array([      509.1,      851.31,        1174,        1940])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1422.3,      765.55,        3315,      1846.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.32817062735557556, 'xyxy': array([     1731.4,        1017,      3765.8,      2455.3])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1708.3,      631.01,      3215.6,      1328.2])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.3656878173351288, 'xyxy': array([     1647.5,      709.94,      3262.7,      1592.7])}}, 219: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6838274598121643, 'xyxy': array([     505.41,      853.57,        1166,      1939.8])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1422.1,      765.22,      3325.6,      1852.3])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.33516064286231995, 'xyxy': array([     1731.9,      1016.9,      3766.4,      2455.2])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1714.6,      626.87,        3213,      1319.9])}, 'object5': {'class': 'motorcycle', 'track_id': '31', 'confidence_score': 0.2706528306007385, 'xyxy': array([     1651.8,      727.86,      3276.2,      1605.5])}}, 220: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.580048680305481, 'xyxy': array([     500.28,      855.49,      1156.4,      1937.6])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1422,      764.89,      3336.3,      1858.1])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.26879990100860596, 'xyxy': array([     1729.8,      1015.9,      3764.6,      2454.5])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1720.9,      622.73,      3210.4,      1311.6])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4543960392475128, 'xyxy': array([       1649,       714.9,      3283.7,      1589.2])}}, 221: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.33182600140571594, 'xyxy': array([     500.23,      864.48,      1154.4,      1946.6])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1421.8,      764.56,        3347,      1863.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1729.8,      1015.8,      3764.7,      2454.6])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1727.1,      618.58,      3207.7,      1303.4])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.28038734197616577, 'xyxy': array([     1642.3,      710.61,      3287.7,      1582.7])}}, 222: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4113544821739197, 'xyxy': array([     499.34,      867.87,      1151.2,      1949.4])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1421.6,      764.23,      3357.6,      1869.8])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1729.7,      1015.8,      3764.7,      2454.6])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1733.4,      614.44,      3205.1,      1295.1])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.49891018867492676, 'xyxy': array([     1644.9,      710.26,      3298.2,        1579])}}, 223: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5182303190231323, 'xyxy': array([     503.43,      866.09,      1151.3,      1943.1])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1421.5,       763.9,      3368.3,      1875.6])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1729.6,      1015.8,      3764.8,      2454.7])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1739.7,       610.3,      3202.5,      1286.9])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.344377338886261, 'xyxy': array([     1653.1,      708.17,      3313.7,      1573.7])}}, 224: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6054401993751526, 'xyxy': array([     507.13,      863.29,      1152.9,        1939])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1421.3,      763.57,        3379,      1881.4])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1729.6,      1015.7,      3764.9,      2454.7])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1745.9,      606.16,      3199.8,      1278.6])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2701519727706909, 'xyxy': array([     1666.1,      710.02,      3331.9,      1571.7])}}, 225: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6190927624702454, 'xyxy': array([     508.52,      862.02,        1154,      1939.1])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1421.2,      763.24,      3389.7,      1887.3])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1729.5,      1015.7,        3765,      2454.8])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1752.2,      602.02,      3197.2,      1270.3])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5744413733482361, 'xyxy': array([     1675.2,      703.54,      3346.3,      1561.4])}}, 226: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6399936676025391, 'xyxy': array([     508.17,      860.21,      1153.3,      1938.2])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([       1421,      762.91,      3400.3,      1893.1])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.25940200686454773, 'xyxy': array([     1727.7,      1015.8,      3762.7,      2454.9])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1758.5,      597.88,      3194.6,      1262.1])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.47141218185424805, 'xyxy': array([     1682.7,      715.39,      3358.9,      1569.6])}}, 227: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6403214931488037, 'xyxy': array([     507.32,      859.31,        1152,      1937.7])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1420.9,      762.58,        3411,      1898.9])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1727.4,      1015.8,      3762.5,        2455])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1764.7,      593.73,      3191.9,      1253.8])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1695.4,      714.72,      3364.7,      1565.4])}}, 228: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5836523175239563, 'xyxy': array([     506.66,       859.1,        1150,      1936.3])}, 'object2': {'class': 'motorcycle', 'track_id': '16', 'confidence_score': None, 'xyxy': array([     1420.7,      762.25,      3421.7,      1904.8])}, 'object3': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1727.2,      1015.8,      3762.4,      2455.1])}, 'object4': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1771,      589.59,      3189.3,      1245.5])}, 'object5': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3590114116668701, 'xyxy': array([       1692,      703.47,      3372.7,      1553.3])}}, 229: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5740570425987244, 'xyxy': array([     510.87,      859.79,      1152.7,      1935.2])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1727,      1015.8,      3762.3,      2455.1])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1777.3,      585.45,      3186.7,      1237.3])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2919926047325134, 'xyxy': array([     1684.2,       694.1,      3372.7,      1541.4])}}, 230: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5817378759384155, 'xyxy': array([     512.68,      861.62,      1153.8,      1936.8])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1726.7,      1015.8,      3762.2,      2455.2])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1783.5,      581.31,        3184,        1229])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.33312639594078064, 'xyxy': array([     1693.4,      691.29,      3387.6,      1535.3])}}, 231: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.570093035697937, 'xyxy': array([     514.25,      861.28,      1155.4,      1937.2])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1726.5,      1015.8,      3762.1,      2455.3])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1789.8,      577.17,      3181.4,      1220.8])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3299991190433502, 'xyxy': array([     1704.6,      698.57,      3401.9,      1538.2])}}, 232: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6249285936355591, 'xyxy': array([     516.41,      861.53,      1156.8,        1937])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1726.2,      1015.7,        3762,      2455.3])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1796.1,      573.03,      3178.8,      1212.5])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.25579744577407837, 'xyxy': array([     1710.5,      699.78,        3414,      1536.9])}}, 233: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5756226181983948, 'xyxy': array([     517.05,      863.13,      1156.8,        1938])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1726,      1015.7,      3761.9,      2455.4])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1802.3,      568.88,      3176.1,      1204.2])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4054524302482605, 'xyxy': array([     1715.3,       695.7,      3420.1,      1527.6])}}, 234: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.37209466099739075, 'xyxy': array([     519.59,      860.87,      1158.6,      1934.5])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1725.8,      1015.7,      3761.7,      2455.5])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1808.6,      564.74,      3173.5,        1196])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.29984062910079956, 'xyxy': array([     1715.3,      691.76,      3426.8,      1521.7])}}, 235: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     519.97,      861.45,      1158.6,      1934.6])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1725.5,      1015.7,      3761.6,      2455.5])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1814.9,       560.6,      3170.9,      1187.7])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5036508440971375, 'xyxy': array([     1727.4,      687.44,      3432.1,      1507.9])}}, 236: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5181974768638611, 'xyxy': array([     517.41,      861.86,      1157.5,      1938.4])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1725.3,      1015.7,      3761.5,      2455.6])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1821.1,      556.46,      3168.2,      1179.5])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.42387649416923523, 'xyxy': array([     1729.7,      683.09,      3437.9,      1499.5])}}, 237: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5427436828613281, 'xyxy': array([     515.16,      863.95,      1154.8,      1940.4])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.25559234619140625, 'xyxy': array([     1730.1,      1016.9,      3764.1,      2455.3])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1827.4,      552.32,      3165.6,      1171.2])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.36765021085739136, 'xyxy': array([     1734.1,      683.74,      3447.6,      1497.1])}}, 238: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4904354512691498, 'xyxy': array([     504.91,      863.06,      1144.2,      1939.9])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.2,        1017,      3764.2,      2455.3])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1833.7,      548.18,        3163,      1162.9])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4001966416835785, 'xyxy': array([       1736,      679.89,      3451.5,      1488.7])}}, 239: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5453373789787292, 'xyxy': array([      500.6,      863.84,      1139.8,        1941])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2862592339515686, 'xyxy': array([     1730.4,        1017,      3764.8,      2455.5])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1839.9,      544.03,      3160.3,      1154.7])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3977232277393341, 'xyxy': array([     1745.6,      678.81,      3464.1,      1483.3])}}, 240: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.541325032711029, 'xyxy': array([     501.19,         865,        1140,      1942.1])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': 0.2913256883621216, 'xyxy': array([     1730.1,      1017.1,      3764.4,      2455.5])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1846.2,      539.89,      3157.7,      1146.4])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3197682499885559, 'xyxy': array([       1742,      675.23,      3475.5,        1482])}}, 241: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5045130252838135, 'xyxy': array([     499.96,      861.14,      1138.4,      1938.3])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.2,      1017.1,      3764.5,      2455.6])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1852.5,      535.75,      3155.1,      1138.2])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2884851098060608, 'xyxy': array([     1741.3,      671.89,      3482.8,      1477.9])}}, 242: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5605162978172302, 'xyxy': array([     503.77,      860.37,        1142,      1937.7])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.2,      1017.2,      3764.6,      2455.6])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([     1858.7,      531.61,      3152.4,      1129.9])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.292900949716568, 'xyxy': array([     1744.6,      671.44,      3482.7,      1470.8])}}, 243: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5309495329856873, 'xyxy': array([     505.37,      859.29,      1143.5,      1936.8])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.3,      1017.3,      3764.6,      2455.6])}, 'object3': {'class': 'person', 'track_id': '21', 'confidence_score': None, 'xyxy': array([       1865,      527.47,      3149.8,      1121.6])}, 'object4': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.27714061737060547, 'xyxy': array([     1748.3,      669.33,      3488.3,      1464.7])}}, 244: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5190362334251404, 'xyxy': array([     504.94,      858.94,      1142.7,      1936.5])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.4,      1017.3,      3764.7,      2455.7])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.46334585547447205, 'xyxy': array([       1755,      671.18,      3498.9,      1463.5])}}, 245: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5430151224136353, 'xyxy': array([     501.62,      858.51,      1138.7,        1935])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.5,      1017.4,      3764.8,      2455.7])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.38190028071403503, 'xyxy': array([     1751.4,      669.28,      3502.2,      1460.2])}}, 246: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.583009660243988, 'xyxy': array([     502.89,       859.2,      1139.1,      1934.3])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.5,      1017.4,      3764.8,      2455.8])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3372493386268616, 'xyxy': array([     1757.8,      669.63,      3516.3,      1459.9])}}, 247: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5046173334121704, 'xyxy': array([     507.19,      859.75,      1142.9,      1934.3])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.6,      1017.5,      3764.9,      2455.8])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5110467672348022, 'xyxy': array([     1760.8,      669.96,      3524.9,      1458.7])}}, 248: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4273500442504883, 'xyxy': array([     508.93,      859.42,      1144.8,      1934.6])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.7,      1017.6,      3764.9,      2455.9])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5088291168212891, 'xyxy': array([     1763.3,      665.02,      3533.3,      1452.7])}}, 249: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4105851352214813, 'xyxy': array([     509.87,      860.05,      1145.7,      1935.7])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.8,      1017.6,        3765,      2455.9])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5514683127403259, 'xyxy': array([     1766.6,      660.55,      3541.7,      1446.7])}}, 250: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.414172887802124, 'xyxy': array([     511.19,      861.17,      1146.9,      1937.3])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.8,      1017.7,      3765.1,        2456])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.48664289712905884, 'xyxy': array([     1773.1,      658.66,      3547.7,      1440.5])}}, 251: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.43985515832901, 'xyxy': array([     510.46,      860.29,      1145.5,      1935.7])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1730.9,      1017.8,      3765.1,        2456])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([       1782,      656.67,      3550.6,      1435.8])}}, 252: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4762158989906311, 'xyxy': array([      507.6,      858.24,      1143.1,      1934.9])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1731,      1017.8,      3765.2,      2456.1])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1790.9,      654.67,      3553.5,      1431.2])}}, 253: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.45812350511550903, 'xyxy': array([     507.98,      860.91,      1143.6,      1938.3])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.1,      1017.9,      3765.3,      2456.1])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1799.8,      652.67,      3556.4,      1426.5])}}, 254: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.46594810485839844, 'xyxy': array([     508.49,      863.81,      1144.3,      1942.2])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.1,      1017.9,      3765.3,      2456.2])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1808.7,      650.68,      3559.3,      1421.9])}}, 255: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4721556305885315, 'xyxy': array([     508.98,      862.63,      1144.5,      1940.9])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.2,        1018,      3765.4,      2456.2])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1817.6,      648.68,      3562.2,      1417.2])}}, 256: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4072224497795105, 'xyxy': array([     509.87,      861.64,      1145.2,      1939.5])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.3,      1018.1,      3765.5,      2456.3])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4855072498321533, 'xyxy': array([     1813.9,      641.54,        3574,      1409.1])}}, 257: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.47171276807785034, 'xyxy': array([      510.2,      858.94,      1145.9,      1937.8])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.4,      1018.1,      3765.5,      2456.3])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.37475407123565674, 'xyxy': array([     1814.9,      636.41,      3582.8,      1400.3])}}, 258: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.40048062801361084, 'xyxy': array([     513.18,      858.17,      1148.2,      1935.7])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.4,      1018.2,      3765.6,      2456.4])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.30605918169021606, 'xyxy': array([       1815,      638.75,      3587.5,      1398.2])}}, 259: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.42920541763305664, 'xyxy': array([     515.86,      858.25,      1151.4,      1936.7])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.5,      1018.3,      3765.6,      2456.4])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.28614217042922974, 'xyxy': array([     1815.9,      649.06,      3595.2,      1405.7])}}, 260: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.49629661440849304, 'xyxy': array([     514.07,       859.2,      1150.9,      1940.1])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.6,      1018.3,      3765.7,      2456.5])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.39188238978385925, 'xyxy': array([     1814.5,      638.68,      3599.6,      1392.6])}}, 261: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5043375492095947, 'xyxy': array([     513.45,      860.08,      1150.7,      1942.3])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.7,      1018.4,      3765.8,      2456.5])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.35194268822669983, 'xyxy': array([     1812.5,      640.79,      3605.4,      1393.4])}}, 262: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.45370540022850037, 'xyxy': array([     517.21,      863.54,      1157.1,        1951])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.7,      1018.4,      3765.8,      2456.5])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3293026089668274, 'xyxy': array([     1811.5,       640.1,      3611.1,      1391.2])}}, 263: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4044371545314789, 'xyxy': array([     519.11,      865.95,        1160,      1955.4])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.8,      1018.5,      3765.9,      2456.6])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.39689651131629944, 'xyxy': array([     1812.5,      639.92,      3617.2,      1389.2])}}, 264: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4415898621082306, 'xyxy': array([     522.97,      864.37,      1162.6,      1951.9])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1731.9,      1018.6,        3766,      2456.6])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.44944289326667786, 'xyxy': array([     1818.9,      635.01,      3627.8,        1382])}, 'object4': {'class': 'person', 'track_id': '49', 'confidence_score': 0.3788661062717438, 'xyxy': array([     547.53,      859.28,      1199.5,      1950.8])}}, 265: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.414235383272171, 'xyxy': array([     522.94,      863.53,      1163.4,      1952.5])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1732,      1018.6,        3766,      2456.7])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2624956965446472, 'xyxy': array([     1818.6,      633.28,      3634.9,      1379.7])}, 'object4': {'class': 'person', 'track_id': '49', 'confidence_score': 0.49799463152885437, 'xyxy': array([     535.31,      858.24,      1190.8,        1953])}}, 266: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5323502421379089, 'xyxy': array([     528.94,      860.07,      1174.6,      1951.6])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([       1732,      1018.7,      3766.1,      2456.7])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4275435507297516, 'xyxy': array([     1819.7,      629.92,        3641,        1375])}, 'object4': {'class': 'person', 'track_id': '49', 'confidence_score': None, 'xyxy': array([     527.68,      855.83,      1185.4,      1954.4])}}, 267: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5860915780067444, 'xyxy': array([      531.5,      857.05,      1182.9,      1949.4])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1732.1,      1018.8,      3766.2,      2456.8])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1826.7,       628.1,      3642.9,      1371.1])}, 'object4': {'class': 'person', 'track_id': '49', 'confidence_score': 0.39691755175590515, 'xyxy': array([     508.95,      858.42,      1162.4,      1951.1])}}, 268: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4874905049800873, 'xyxy': array([     543.51,      853.45,        1204,      1944.9])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1732.2,      1018.8,      3766.2,      2456.8])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.39446449279785156, 'xyxy': array([     1831.3,      633.07,        3647,      1371.5])}, 'object4': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5745245218276978, 'xyxy': array([     510.83,      855.02,        1166,      1948.1])}}, 269: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4507538676261902, 'xyxy': array([     546.91,      851.88,        1216,      1943.5])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1732.3,      1018.9,      3766.3,      2456.9])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4350169599056244, 'xyxy': array([     1831.2,      632.54,      3653.3,      1369.9])}, 'object4': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5738988518714905, 'xyxy': array([     511.64,      853.57,      1168.4,      1947.2])}}, 270: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     551.07,      850.99,      1220.6,      1943.3])}, 'object2': {'class': 'motorcycle', 'track_id': '20', 'confidence_score': None, 'xyxy': array([     1732.3,      1018.9,      3766.3,      2456.9])}, 'object3': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.41388219594955444, 'xyxy': array([     1834.9,      627.77,        3657,      1361.3])}, 'object4': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5663629770278931, 'xyxy': array([     509.99,      853.22,      1169.1,      1946.5])}}, 271: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.5990166664123535, 'xyxy': array([     508.86,      853.65,      1177.6,      1946.2])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.29184064269065857, 'xyxy': array([     1838.4,      627.65,      3663.3,      1358.6])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.4644232988357544, 'xyxy': array([     523.08,      852.46,      1186.6,      1942.6])}}, 272: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.41469326615333557, 'xyxy': array([     524.56,      852.25,      1201.6,      1944.3])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5869585275650024, 'xyxy': array([     1838.2,      618.14,      3669.7,      1348.4])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5884525775909424, 'xyxy': array([     504.85,      852.82,      1169.8,      1945.2])}}, 273: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     526.05,      851.63,      1203.3,      1944.2])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5636763572692871, 'xyxy': array([     1835.2,      622.96,        3673,      1352.7])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6383669376373291, 'xyxy': array([     519.74,      850.83,      1190.8,      1941.7])}}, 274: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     527.54,      851.01,      1205.1,      1944.1])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5194416046142578, 'xyxy': array([     1837.7,      629.58,      3677.6,        1357])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.681165337562561, 'xyxy': array([     526.17,      850.43,      1202.7,      1939.8])}}, 275: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     529.03,       850.4,      1206.8,      1944.1])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.43006250262260437, 'xyxy': array([     1837.6,      623.93,      3679.7,      1349.2])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.714911699295044, 'xyxy': array([     525.12,      850.87,      1206.4,        1939])}}, 276: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     530.51,      849.78,      1208.6,        1944])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4382030963897705, 'xyxy': array([     1840.6,      613.74,      3684.6,      1336.8])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.7132713794708252, 'xyxy': array([     522.58,       851.6,      1209.6,      1941.4])}}, 277: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([        532,      849.16,      1210.3,        1944])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2897895872592926, 'xyxy': array([     1841.9,      623.26,      3696.5,      1348.1])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5872499942779541, 'xyxy': array([      516.7,      852.02,      1207.5,      1942.3])}}, 278: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     533.49,      848.54,        1212,      1943.9])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4054482877254486, 'xyxy': array([     1847.5,      629.22,      3702.4,      1351.3])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5811453461647034, 'xyxy': array([     512.93,      852.26,      1206.8,      1942.4])}}, 279: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     534.98,      847.93,      1213.8,      1943.8])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.38124746084213257, 'xyxy': array([     1851.7,      634.98,      3707.2,      1354.2])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6137621998786926, 'xyxy': array([     512.91,      852.44,        1210,      1942.9])}}, 280: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     536.47,      847.31,      1215.5,      1943.8])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1858.2,      635.57,      3708.9,      1352.9])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5745507478713989, 'xyxy': array([     508.13,      852.73,      1207.3,      1943.2])}}, 281: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2532986104488373, 'xyxy': array([     478.26,      861.74,      1144.4,      1947.1])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1864.6,      636.17,      3710.5,      1351.6])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5859593152999878, 'xyxy': array([     496.54,       855.5,        1198,      1947.3])}}, 282: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.31588929891586304, 'xyxy': array([     491.95,      862.55,      1155.1,      1951.1])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1871.1,      636.76,      3712.1,      1350.3])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5998861193656921, 'xyxy': array([     504.95,      857.18,      1207.8,      1949.5])}}, 283: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.3076007068157196, 'xyxy': array([     496.94,      862.73,      1156.8,      1952.4])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3163752555847168, 'xyxy': array([     1859.2,      611.04,      3710.5,      1324.2])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6054991483688354, 'xyxy': array([     507.15,      857.79,      1211.2,      1950.4])}}, 284: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2981235682964325, 'xyxy': array([     499.96,      862.58,        1157,      1952.8])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3304823935031891, 'xyxy': array([     1857.1,      611.22,      3715.4,      1323.4])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6235523223876953, 'xyxy': array([     508.01,       857.8,      1213.2,      1950.9])}}, 285: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2845466732978821, 'xyxy': array([     491.88,      862.03,      1144.9,      1949.4])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.27040383219718933, 'xyxy': array([     1859.9,      622.77,      3723.3,      1333.3])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.603508472442627, 'xyxy': array([      499.5,      857.41,      1205.4,      1950.4])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': 0.3879663348197937, 'xyxy': array([     574.44,      1081.6,      1247.9,      2224.2])}}, 286: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.3109307587146759, 'xyxy': array([     492.77,      862.14,      1143.3,      1948.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3045322597026825, 'xyxy': array([     1852.3,      616.43,      3724.3,      1327.1])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5765532851219177, 'xyxy': array([     487.74,      857.53,      1191.8,      1950.4])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': 0.3556861877441406, 'xyxy': array([     574.49,      1081.7,      1247.9,      2224.3])}}, 287: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2953667640686035, 'xyxy': array([     498.45,       862.2,      1148.4,        1951])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1856.7,      615.49,      3724.8,      1324.7])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5681250095367432, 'xyxy': array([     488.85,      857.16,      1192.4,      1950.2])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': 0.3536856174468994, 'xyxy': array([     574.57,      1081.7,        1248,      2224.3])}}, 288: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2570391893386841, 'xyxy': array([      497.4,       861.2,      1144.7,      1948.4])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([       1861,      614.55,      3725.4,      1322.3])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.5711985230445862, 'xyxy': array([     486.87,      856.68,      1190.9,      1949.8])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': 0.36265280842781067, 'xyxy': array([     574.65,      1081.7,        1248,      2224.2])}}, 289: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.2995237112045288, 'xyxy': array([     501.95,      860.74,      1148.1,        1948])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1865.3,      613.61,      3725.9,      1319.9])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6055111289024353, 'xyxy': array([     486.81,      856.73,        1188,      1949.4])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([      574.7,      1081.8,        1248,      2224.3])}}, 290: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     501.47,       860.8,      1147.4,        1948])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1869.6,      612.67,      3726.4,      1317.5])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6285009384155273, 'xyxy': array([      485.2,      856.16,      1182.4,      1948.2])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     574.74,      1081.9,      1248.1,      2224.3])}}, 291: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.35584497451782227, 'xyxy': array([     501.56,      859.57,      1147.4,      1948.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1873.9,      611.73,      3726.9,        1315])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6304194331169128, 'xyxy': array([     489.46,      855.94,      1185.4,      1948.1])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     574.79,      1081.9,      1248.1,      2224.4])}}, 292: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     501.08,      859.49,      1146.9,      1948.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1878.2,      610.79,      3727.4,      1312.6])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6314172148704529, 'xyxy': array([      489.5,      855.95,      1183.5,      1948.1])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     574.83,        1082,      1248.1,      2224.4])}}, 293: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6370468735694885, 'xyxy': array([     496.43,      857.14,      1143.4,      1949.1])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2526935040950775, 'xyxy': array([     1890.1,      605.33,      3735.7,      1298.4])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.4751380681991577, 'xyxy': array([     496.74,      857.21,      1191.8,      1948.4])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     574.88,      1082.1,      1248.2,      2224.5])}}, 294: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.6466185450553894, 'xyxy': array([     491.59,       856.9,      1138.5,      1950.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.27303874492645264, 'xyxy': array([     1893.2,      602.22,      3746.6,      1291.8])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.37932485342025757, 'xyxy': array([     495.79,       857.9,      1192.3,      1949.8])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     574.93,      1082.1,      1248.2,      2224.5])}}, 295: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4291936159133911, 'xyxy': array([     511.69,      857.83,      1164.5,      1949.9])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1899.6,      600.73,      3747.1,      1288.1])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6241391897201538, 'xyxy': array([     481.55,      857.56,      1173.2,      1950.1])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     574.97,      1082.2,      1248.3,      2224.6])}}, 296: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.43279868364334106, 'xyxy': array([     522.83,      858.04,      1181.5,      1949.6])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.35052168369293213, 'xyxy': array([       1905,      599.88,      3764.6,      1285.9])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6293994188308716, 'xyxy': array([     481.79,      857.37,      1168.8,      1950.2])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.02,      1082.3,      1248.3,      2224.6])}}, 297: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.4292353391647339, 'xyxy': array([     526.61,      858.14,      1191.3,      1950.1])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.3808659613132477, 'xyxy': array([     1904.2,      594.63,      3770.1,      1277.8])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.603973925113678, 'xyxy': array([     484.82,      857.29,      1168.5,      1950.9])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.06,      1082.3,      1248.3,      2224.7])}}, 298: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.39644885063171387, 'xyxy': array([     525.69,      858.34,        1195,      1950.3])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.49208948016166687, 'xyxy': array([     1904.1,      591.19,      3779.1,      1273.3])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6125326156616211, 'xyxy': array([      486.1,      857.52,      1165.9,      1951.2])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.11,      1082.4,      1248.4,      2224.7])}}, 299: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.36395159363746643, 'xyxy': array([     519.13,      857.99,      1191.1,      1949.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5184340476989746, 'xyxy': array([     1903.3,       590.2,      3779.9,      1268.7])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6454090476036072, 'xyxy': array([     486.28,      857.33,      1162.1,      1950.8])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.15,      1082.5,      1248.4,      2224.8])}}, 300: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.33876755833625793, 'xyxy': array([     516.06,       858.3,      1190.3,      1949.9])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4230494499206543, 'xyxy': array([     1901.5,      590.48,      3782.5,      1266.8])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6307554244995117, 'xyxy': array([     487.54,      857.77,      1159.8,      1951.3])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([      575.2,      1082.5,      1248.4,      2224.8])}}, 301: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.3536752462387085, 'xyxy': array([     517.72,      858.42,      1195.1,      1949.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.4871509075164795, 'xyxy': array([     1897.8,      587.26,      3790.4,      1264.4])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6358591914176941, 'xyxy': array([     490.18,      857.73,      1159.3,      1950.9])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.24,      1082.6,      1248.5,      2224.9])}}, 302: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.3575187921524048, 'xyxy': array([     514.19,      859.01,      1192.9,      1949.6])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.5690445899963379, 'xyxy': array([     1899.7,      584.36,      3792.3,      1257.9])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6766996383666992, 'xyxy': array([     491.36,      858.27,      1157.3,      1950.9])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.29,      1082.7,      1248.5,      2224.9])}}, 303: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     515.54,      859.04,      1194.1,      1949.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.36679282784461975, 'xyxy': array([       1900,      583.13,      3795.5,      1254.4])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6786884069442749, 'xyxy': array([     489.59,      856.53,      1152.8,      1948.9])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.34,      1082.7,      1248.5,        2225])}}, 304: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     516.88,      859.07,      1195.4,      1949.7])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1905.8,      581.25,      3795.5,      1250.5])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.656795084476471, 'xyxy': array([     492.92,      855.13,      1153.5,        1947])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.38,      1082.8,      1248.6,        2225])}}, 305: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.31350064277648926, 'xyxy': array([     513.66,      855.49,      1193.7,      1945.4])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.2714766561985016, 'xyxy': array([     1886.3,      585.64,      3789.9,      1256.6])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6412322521209717, 'xyxy': array([     495.43,      855.09,      1153.7,      1946.8])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.43,      1082.9,      1248.6,      2225.1])}}, 306: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     514.65,      855.16,      1194.5,        1945])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1889.8,      584.44,      3788.7,      1253.7])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6665230393409729, 'xyxy': array([     494.67,      855.89,        1151,      1947.8])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.47,      1082.9,      1248.6,      2225.1])}}, 307: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.3423146903514862, 'xyxy': array([     514.39,      856.26,      1196.7,        1947])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1893.4,      583.24,      3787.6,      1250.9])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6611803770065308, 'xyxy': array([     498.42,      856.16,      1153.1,      1948.3])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.52,        1083,      1248.7,      2225.2])}}, 308: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': 0.35947734117507935, 'xyxy': array([     511.76,      856.51,      1195.2,      1947.3])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': 0.25448092818260193, 'xyxy': array([     1883.1,      577.61,      3792.3,      1246.8])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6602259874343872, 'xyxy': array([     499.71,      856.36,      1152.9,      1948.6])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.56,      1083.1,      1248.7,      2225.2])}}, 309: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([      512.4,      856.38,      1195.8,      1947.2])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1885.8,      575.97,      3791.2,      1243.8])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.664148211479187, 'xyxy': array([      500.9,      856.07,      1152.9,      1948.6])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.61,      1083.1,      1248.8,      2225.3])}, 'object5': {'class': 'motorcycle', 'track_id': '54', 'confidence_score': 0.4660547375679016, 'xyxy': array([     1727.1,      1016.2,      3763.1,      2454.9])}}, 310: {'object1': {'class': 'person', 'track_id': '2', 'confidence_score': None, 'xyxy': array([     513.04,      856.24,      1196.4,      1947.1])}, 'object2': {'class': 'boat', 'track_id': '31', 'confidence_score': None, 'xyxy': array([     1888.4,      574.32,      3790.1,      1240.9])}, 'object3': {'class': 'person', 'track_id': '49', 'confidence_score': 0.6216455101966858, 'xyxy': array([     503.98,      855.68,      1154.9,      1949.2])}, 'object4': {'class': 'surfboard', 'track_id': '52', 'confidence_score': None, 'xyxy': array([     575.65,      1083.2,      1248.8,      2225.3])}, 'object5': {'class': 'motorcycle', 'track_id': '54', 'confidence_score': 0.5445199608802795, 'xyxy': array([     1726.9,      1016.6,      3762.8,      2455.2])}}}\n"]}]},{"cell_type":"code","source":["# Cleanup\n","cctv_feed.release()\n","out_video.release()"],"metadata":{"id":"Q5lx98Xoo0jM","executionInfo":{"status":"ok","timestamp":1749971246931,"user_tz":-330,"elapsed":5,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import csv\n","import datetime"],"metadata":{"id":"iCCXdPrj48mJ","executionInfo":{"status":"ok","timestamp":1749971248011,"user_tz":-330,"elapsed":3,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["csv_filepath = 'samples/9th June/5am/tracked_objects.csv'\n","write_header = not os.path.exists(csv_filepath)"],"metadata":{"id":"2hfjEPT65BGx","executionInfo":{"status":"ok","timestamp":1749971249131,"user_tz":-330,"elapsed":16,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["with open(csv_filepath, mode='w', newline='') as csv_file:\n","  writer = csv.writer(csv_file)\n","\n","  # Write header if file is new\n","  if write_header:\n","    writer.writerow(['Frames', 'Class', 'Track Id','Confidence Score','XYXY'])\n","\n","  # Write data from tracked_summary\n","  for frame, objects in tracked_summary.items():\n","    for obj_name, obj_data in objects.items():\n","      writer.writerow([\n","          frame,\n","          obj_data['class'],\n","          obj_data['track_id'],\n","          obj_data['confidence_score'],\n","          str(obj_data['xyxy'])\n","      ])\n","\n","print(f\"Tracked summary written to {csv_filepath}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPpj0p-q5Ll6","executionInfo":{"status":"ok","timestamp":1749971250614,"user_tz":-330,"elapsed":84,"user":{"displayName":"Nukul Gupta","userId":"06727282945713710781"}},"outputId":"bbb4ba71-6439-45a2-c3fa-7ad44e0425f7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Tracked summary written to samples/9th June/5am/tracked_objects.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lJ31YjsQ-0CY"},"execution_count":null,"outputs":[]}]}